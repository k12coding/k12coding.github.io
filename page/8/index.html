<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>k12的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="2">
<meta property="og:type" content="website">
<meta property="og:title" content="k12的博客">
<meta property="og:url" content="https://k12coding.github.io/page/8/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="2">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="k12的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">k12的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">k12的笔记</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <!--
      <nav id="sub-nav">
        
          此处隐藏rss,注释掉 <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a> -->
        
        <!--此处隐藏,注释掉 <a id="nav-search-btn" class="nav-icon" title="搜索"></a> 
      </nav>
      -->
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://k12coding.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-IDEA中测试Hive源码" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/" class="article-date">
  <time class="dt-published" datetime="2022-01-07T12:54:53.000Z" itemprop="datePublished">2022-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/">IDEA中测试Hive源码</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>将添加随机数，去除随机数的UDF整合到源码中，并在IDEA的终端中完成测试：</p>
<p>ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAddRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">import java.util.Random;</span><br><span class="line"></span><br><span class="line">public class UDFAddRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		int num = new Random().nextInt(10);</span><br><span class="line">		return s+&quot;_&quot;+num;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFAddRandom input = new UDFAddRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRemoveRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">public class UDFRemoveRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		return s.split(&quot;_&quot;)[0];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFRemoveRandom input = new UDFRemoveRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK_91&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hive.ql.udf.UDFAddRandom; </span><br><span class="line">import org.apache.hadoop.hive.ql.udf.UDFRemoveRandom; </span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">static &#123;</span><br><span class="line">    system.registerUDF(&quot;add_random&quot;, UDFAddRandom.class, false);</span><br><span class="line">	system.registerUDF(&quot;remove_random&quot;, UDFRemoveRandom.class, false);</span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在WINDOWS上部署maven，编译hive</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220106155419571.png" alt="image-20220106155419571"></p>
<p>编译好的整个文件夹导入到idea中</p>
<p>使用快捷键Ctrl+Alt+shift+S打开项目的jdk配置，把内存改大点再rebuild</p>
<p>rebuild，期间如有报错，按提示修改，最终Build completed successfully</p>
<ul>
<li><p>设置hive-site.xml与服务器上的一样，CliDriver目录下的resources文件夹（需要自己手动创建）</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107143625518.png" alt="image-20220107143625518"></p>
<p>简单做法：只有一个hive-site.xml也可以了</p>
<p>hive-site.xml：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--</span><br><span class="line">   Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">   contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">   this work for additional information regarding copyright ownership.</span><br><span class="line">   The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">   (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line">   the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">   Unless required by applicable law or agreed to in writing, software</span><br><span class="line">   distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">   See the License for the specific language governing permissions and</span><br><span class="line">   limitations under the License.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;jdbc:mysql://hadoop001:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;ruozedata001&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop001&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;thrift://hadoop001:9083&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>复杂做法：5个都放进去</p>
</li>
<li><p>metastore</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107162612716.png" alt="metastore"></p>
<p>在hive-site.xml中添加<code>hive.metastore.uris：thrift://hadoop001:9083</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;thrift://hadoop001:9083&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>在服务器上执行<code>hive --service metastore</code>启动服务端metastore </p>
</li>
<li><p>修改运行环境，IDEA 的 VM 添加 ：</p>
<p>设置系统属性jline.WindowsTerminal.directConsole为false，控制台才能接受输入，否则输入命令后回车没有反应：</p>
<p><code>-Djline.WindowsTerminal.directConsole=false</code> </p>
<p>修改系统用户名称，否则无权限访问hdfs：</p>
<p><code>-DHADOOP_USER_NAME=hadoop</code></p>
</li>
</ul>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107164818544.png" alt="image-20220107164818544"></p>
<p>运行入口文件CliDriver.java</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107143910446.png" alt="image-20220107143910446"></p>
<p>验证函数是否存在；</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107143958077.png" alt="image-20220107143958077"></p>
<p>测试使用：</p>
<p><strong>add_ramdom</strong>:</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107144756925.png" alt="image-20220107144756925"></p>
<p><strong>remove_random</strong>:</p>
<p><img src="/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/image-20220107144730402.png" alt="image-20220107144730402"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2022/01/07/IDEA%E4%B8%AD%E6%B5%8B%E8%AF%95Hive%E6%BA%90%E7%A0%81/" data-id="cl3b9tuai000kakuddrx7cy0d" data-title="IDEA中测试Hive源码" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SQL-WHERE、ON、HAVING的区别" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/05/SQL-WHERE%E3%80%81ON%E3%80%81HAVING%E7%9A%84%E5%8C%BA%E5%88%AB/" class="article-date">
  <time class="dt-published" datetime="2022-01-04T18:19:13.000Z" itemprop="datePublished">2022-01-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/05/SQL-WHERE%E3%80%81ON%E3%80%81HAVING%E7%9A%84%E5%8C%BA%E5%88%AB/">SQL:WHERE、ON、HAVING的区别</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="WHERE-与-HAVING"><a href="#WHERE-与-HAVING" class="headerlink" title="WHERE 与 HAVING"></a>WHERE 与 HAVING</h2><p><code>WHERE</code>与<code>HAVING</code>的根本区别在于：</p>
<ul>
<li><code>WHERE</code>子句在<code>GROUP BY</code>分组和聚合函数<strong>之前</strong>对数据行进行过滤；</li>
<li><code>HAVING</code>子句对<code>GROUP BY</code>分组和聚合函数<strong>之后</strong>的数据行进行过滤。</li>
</ul>
<p>因此，<code>WHERE</code>子句中不能使用聚合函数。例如，以下语句将会返回错误：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 查找人数大于 <span class="number">5</span> 的部门</span><br><span class="line">select dept_id, count(*)</span><br><span class="line"><span class="function">from employee</span></span><br><span class="line"><span class="function">where <span class="title">count</span><span class="params">(*)</span> &gt; 5</span></span><br><span class="line"><span class="function">group by dept_id</span>;</span><br></pre></td></tr></table></figure>

<p>由于在执行<code>WHERE</code>子句时，还没有计算聚合函数 count(*)，所以无法使用。正确的方法是使用HAVING对聚合之后的结果进行过滤：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- 查找人数大于 <span class="number">5</span> 的部门</span><br><span class="line">select dept_id, count(*)</span><br><span class="line"><span class="function">from employee</span></span><br><span class="line"><span class="function">group by dept_id</span></span><br><span class="line"><span class="function">having <span class="title">count</span><span class="params">(*)</span> &gt; 5</span>;</span><br><span class="line">dept_id|count(*)|</span><br><span class="line">-------|--------|</span><br><span class="line">      <span class="number">4</span>|       <span class="number">9</span>|</span><br><span class="line">      <span class="number">5</span>|       <span class="number">8</span>|</span><br></pre></td></tr></table></figure>

<p>另一方面，<code>HAVING</code>子句中不能使用除了分组字段和聚合函数之外的其他字段。例如，以下语句将会返回错误：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 统计每个部门月薪大于等于 <span class="number">30000</span> 的员工人数</span><br><span class="line">select dept_id, count(*)</span><br><span class="line">from employee</span><br><span class="line">group by dept_id</span><br><span class="line">having salary &gt;= <span class="number">30000</span>;</span><br></pre></td></tr></table></figure>

<p>因为经过<code>GROUP BY</code>分组和聚合函数之后，不再存在 salary 字段，<code>HAVING</code>子句中只能使用分组字段或者聚合函数。</p>
<blockquote>
<p>SQLite 虽然允许<code>HAVING</code>子句中出现其他字段，但是得到的结果不正确。</p>
</blockquote>
<p>从性能的角度来说，<code>HAVING</code>子句中如果使用了分组字段作为过滤条件，应该替换成<code>WHERE</code>子句；因为<code>WHERE</code>可以在执行分组操作和计算聚合函数之前过滤掉不需要的数据，性能会更好。下面示例中的语句 1 应该替换成语句 2：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-- 语句 <span class="number">1</span></span><br><span class="line">select dept_id, count(*)</span><br><span class="line">from employee</span><br><span class="line">group by dept_id</span><br><span class="line">having dept_id = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">-- 语句 <span class="number">2</span></span><br><span class="line">select dept_id, count(*)</span><br><span class="line">from employee</span><br><span class="line">where dept_id = <span class="number">1</span></span><br><span class="line">group by dept_id;</span><br></pre></td></tr></table></figure>

<p>当然，<code>WHERE</code>和<code>HAVING</code>可以组合在一起使用。例如：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select dept_id, count(*)</span><br><span class="line">from employee</span><br><span class="line">where salary &gt; <span class="number">10000</span></span><br><span class="line"><span class="function">group by dept_id</span></span><br><span class="line"><span class="function">having <span class="title">count</span><span class="params">(*)</span> &gt; 1</span>;</span><br><span class="line">dept_id|count(*)|</span><br><span class="line">-------|--------|</span><br><span class="line">      <span class="number">1</span>|       <span class="number">3</span>|</span><br></pre></td></tr></table></figure>

<p>该语句返回了月薪大于 10000 的员工人数大于 1 的部门；<code>WHERE</code>用于过滤月薪大于 10000 的员工；<code>HAVING</code>用于过滤员工数量大于 1 的部门。</p>
<h2 id="WHERE-与-ON"><a href="#WHERE-与-ON" class="headerlink" title="WHERE 与 ON"></a>WHERE 与 ON</h2><p>当查询涉及多个表的关联时，我们既可以使用<code>WHERE</code>子句也可以使用<code>ON</code>子句指定连接条件和过滤条件。这两者之间的主要区别在于：</p>
<ul>
<li>对于内连接（inner join）查询，<code>WHERE</code>和<code>ON</code>中的过滤条件等效；</li>
<li>对于外连接（outer join）查询，<code>ON</code>中的过滤条件在连接操作之前执行，<code>WHERE</code>中的过滤条件（逻辑上）在连接操作之后执行。</li>
</ul>
<p>对于内连接查询而言，以下三个语句的结果相同：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">-- 语句 <span class="number">1</span></span><br><span class="line">select d.dept_name, e.emp_name, e.sex, e.salary </span><br><span class="line">from employee e, department d</span><br><span class="line">where e.dept_id = d.dept_id</span><br><span class="line">and e.emp_id = <span class="number">10</span>;</span><br><span class="line">dept_name|emp_name|sex|salary |</span><br><span class="line">---------|--------|---|-------|</span><br><span class="line">研发部   |廖化    |男  |<span class="number">6500.00</span>|</span><br><span class="line"></span><br><span class="line">-- 语句 <span class="number">2</span></span><br><span class="line">select d.dept_name, e.emp_name, e.sex, e.<span class="function">salary </span></span><br><span class="line"><span class="function">from employee e</span></span><br><span class="line"><span class="function">join department d <span class="title">on</span> <span class="params">(e.dept_id = d.dept_id and e.emp_id = <span class="number">10</span>)</span></span>;</span><br><span class="line">dept_name|emp_name|sex|salary |</span><br><span class="line">---------|--------|---|-------|</span><br><span class="line">研发部   |廖化    |男  |<span class="number">6500.00</span>|</span><br><span class="line"></span><br><span class="line">-- 语句 <span class="number">3</span></span><br><span class="line">select d.dept_name, e.emp_name, e.sex, e.<span class="function">salary </span></span><br><span class="line"><span class="function">from employee e</span></span><br><span class="line"><span class="function">join department d <span class="title">on</span> <span class="params">(e.dept_id = d.dept_id)</span></span></span><br><span class="line"><span class="function">where e.emp_id </span>= <span class="number">10</span>;</span><br><span class="line">dept_name|emp_name|sex|salary |</span><br><span class="line">---------|--------|---|-------|</span><br><span class="line">研发部   |廖化    |男  |<span class="number">6500.00</span>|</span><br></pre></td></tr></table></figure>

<p>语句 1 在<code>WHERE</code>中指定连接条件和过滤条件；语句 2 在<code>ON</code>中指定连接条件和过滤条件；语句 3 在<code>ON</code>中指定连接条件，在<code>WHERE</code>中指定其他过滤条件。上面语句不但结果相同，数据库的执行计划也相同。以 MySQL 为例，以上语句的执行计划如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">id|select_type|table|partitions|type |possible_keys       |key    |key_len|ref  |rows|filtered|Extra|</span><br><span class="line">--|-----------|-----|----------|-----|--------------------|-------|-------|-----|----|--------|-----|</span><br><span class="line"> <span class="number">1</span>|SIMPLE     |e    |          |<span class="keyword">const</span>|PRIMARY,idx_emp_dept|PRIMARY|<span class="number">4</span>      |<span class="keyword">const</span>|   <span class="number">1</span>|     <span class="number">100</span>|     |</span><br><span class="line"> <span class="number">1</span>|SIMPLE     |d    |          |<span class="keyword">const</span>|PRIMARY             |PRIMARY|<span class="number">4</span>      |<span class="keyword">const</span>|   <span class="number">1</span>|     <span class="number">100</span>|     |</span><br></pre></td></tr></table></figure>

<p>尽管如此，仍然建议将两个表的连接条件放在<code>ON</code>子句中，将其他过滤条件放在<code>WHERE</code>子句中；这样语义更加明确，更容易阅读和理解。对于上面的示例而言，推荐使用语句 3 的写法。</p>
<p>数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。</p>
<p>在使用 <strong>left join</strong> 时，<strong>on</strong> 和 <strong>where</strong> 条件的区别如下：</p>
<p>1、<strong>on</strong> 条件是在生成临时表时使用的条件，它不管 <strong>on</strong> 中的条件是否为真，都会返回左边表中的记录。</p>
<p>2、where 条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有 <strong>left join</strong> 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。</p>
<p>假设有两张表：</p>
<p>表1：tab1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">id size</span><br><span class="line">1  10</span><br><span class="line">2  20</span><br><span class="line">3  30</span><br></pre></td></tr></table></figure>

<p>表2：tab2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">size name</span><br><span class="line">10   AAA</span><br><span class="line">20   BBB</span><br><span class="line">20   CCC</span><br></pre></td></tr></table></figure>

<p>两条SQL:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、select * from tab1 left join tab2 on tab1.size = tab2.size where tab2.name=&#x27;AAA&#x27;</span><br><span class="line">2、select * from tab1 left join tab2 on tab1.size = tab2.size and tab2.name=&#x27;AAA&#x27;</span><br></pre></td></tr></table></figure>

<p><strong>第一条SQL的过程：</strong></p>
<p>1、中间表</p>
<p>on 条件:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tab1.size = tab2.size</span><br><span class="line">tab1.id tab1.size tab2.size tab2.name</span><br><span class="line">1 10 10 AAA</span><br><span class="line">2 20 20 BBB</span><br><span class="line">2 20 20 CCC</span><br><span class="line">3 30 (null) (null)</span><br></pre></td></tr></table></figure>

<p>2、再对中间表过滤</p>
<p>where 条件：</p>
<p>tab2.name=’AAA’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tab1.id tab1.size tab2.size tab2.name</span><br><span class="line">1 10 10 AAA</span><br></pre></td></tr></table></figure>

<p><strong>第二条SQL的过程：</strong></p>
<p>1、中间表</p>
<p>on 条件:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tab1.size = tab2.size and tab2.name=&#x27;AAA&#x27;</span><br><span class="line">(条件不为真也会返回左表中的记录) tab1.id tab1.size tab2.size tab2.name</span><br><span class="line">1 10 10 AAA</span><br><span class="line">2 20 (null) (null)</span><br><span class="line">3 30 (null) (null)</span><br></pre></td></tr></table></figure>

<p>其实以上结果的关键原因就是 <strong>left join,right join,full join</strong> 的特殊性。</p>
<p>不管 on 上的条件是否为真都会返回 left 或 right 表中的记录，full 则具有 left 和 right 的特性的并集。</p>
<p>而 inner jion 没这个特殊性，则条件放在 on 中和 where 中，返回的结果集是相同的。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><ul>
<li><p>SQL标准要求HAVING必须仅引用GROUP BY子句中的列或聚合函数中使用的列。 但是，MySQL支持对此行为的扩展，并允许HAVING引用SELECT列表中的列和外部子查询中的列。</p>
<p>如果HAVING子句引用了不明确的列，则会出现警告。在下面的语句中，col2不明确，因为它同时用作别名和列名:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(col1) <span class="keyword">AS</span> col2 <span class="keyword">FROM</span> t <span class="keyword">GROUP</span> <span class="keyword">BY</span> col2 <span class="keyword">HAVING</span> col2 <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<p>优先考虑标准SQL行为，因此如果HAVING使用的列名同时出现在GROUP BY和输出列列表使用的别名中，则会优先选择GROUP BY列中的列名。</p>
</li>
<li><p>不要对应该出现在WHERE子句中的项使用HAVING。例如，不要写下面的内容</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> col_name <span class="keyword">FROM</span> tbl_name <span class="keyword">HAVING</span> col_name <span class="operator">&gt;</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure>

<p>改为写这个:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> col_name <span class="keyword">FROM</span> tbl_name <span class="keyword">WHERE</span> col_name <span class="operator">&gt;</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>HAVING子句可以引用聚合函数，而WHERE子句不能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, <span class="built_in">MAX</span>(salary) <span class="keyword">FROM</span> users <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">user</span> <span class="keyword">HAVING</span> <span class="built_in">MAX</span>(salary) <span class="operator">&gt;</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/horses/article/details/105380420">https://blog.csdn.net/horses/article/details/105380420</a></p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/sql-different-on-and-where.html">https://www.runoob.com/w3cnote/sql-different-on-and-where.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/BxScope/p/10859260.html">https://www.cnblogs.com/BxScope/p/10859260.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2022/01/05/SQL-WHERE%E3%80%81ON%E3%80%81HAVING%E7%9A%84%E5%8C%BA%E5%88%AB/" data-id="cl3b9tuam000rakud1c9u7evr" data-title="SQL:WHERE、ON、HAVING的区别" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MapReduce-Join" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/01/MapReduce-Join/" class="article-date">
  <time class="dt-published" datetime="2022-01-01T08:38:34.000Z" itemprop="datePublished">2022-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/01/MapReduce-Join/">MapReduce Join</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>   在传统数据库（如：MySql）中，JOIN操作常常是非常耗时的。而在HADOOP中进行JOIN操作，同样常见且耗时，由于Hadoop的独特设计思想，当进行JOIN操作时，有一些特殊的技巧。下面分别介绍MapReduce中的几种常见join，比如有最常见的 map side join，reduce side join，semi join（这些在Hive中都有） 等。Map side join在处理多个小表关联大表时非常有用，而 reduce join 在处理多表关联时是比较麻烦的，会造成大量的网络IO，效率低下，但在有些时候也是非常有用的。</p>
        
          <p class="article-more-link" align="center">
            <a href="/2022/01/01/MapReduce-Join/#more">阅读更多</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2022/01/01/MapReduce-Join/" data-id="cl3b9tub7001kakud4bash7ns" data-title="MapReduce Join" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MapReduce支持递归子目录作为输入" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/30/MapReduce%E6%94%AF%E6%8C%81%E9%80%92%E5%BD%92%E5%AD%90%E7%9B%AE%E5%BD%95%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5/" class="article-date">
  <time class="dt-published" datetime="2021-12-29T17:35:43.000Z" itemprop="datePublished">2021-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/30/MapReduce%E6%94%AF%E6%8C%81%E9%80%92%E5%BD%92%E5%AD%90%E7%9B%AE%E5%BD%95%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5/">MapReduce支持递归子目录作为输入</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>执行MapReduce程序时，input path中包含子目录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: java.io.FileNotFoundException: Path is not a file: /data/hive/mulit_file/sub_dir</span><br></pre></td></tr></table></figure>

<p>解决办法：mr中或者在mapred-site.xml中设置：mapreduce.input.fileinputformat.input.dir.recursive=true</p>
<ul>
<li><p>mr中设置configuration:<code>conf.set(&quot;mapreduce.input.fileinputformat.input.dir.recursive&quot;,true)</code></p>
</li>
<li><p>etc/hadoop/mapred-site.xml添加属性:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><p>在hive-cli中设置参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.mapred.supports.subdirectories=true;</span><br><span class="line">set mapred.input.dir.recursive=true;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2021/12/30/MapReduce%E6%94%AF%E6%8C%81%E9%80%92%E5%BD%92%E5%AD%90%E7%9B%AE%E5%BD%95%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5/" data-id="cl3b9tuam000qakud00466pm5" data-title="MapReduce支持递归子目录作为输入" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-HIVE-UDF" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/29/HIVE-UDF/" class="article-date">
  <time class="dt-published" datetime="2021-12-28T17:19:11.000Z" itemprop="datePublished">2021-12-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/29/HIVE-UDF/">HIVE UDF 与 HIVE源码编译</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="一、实现UDF"><a href="#一、实现UDF" class="headerlink" title="一、实现UDF"></a>一、实现UDF</h2><p>需求：添加随机数<code>add_random</code>、去除随机数<code>remove_random</code></p>
<p>UDF函数中实现evaluate方法。</p>
<p>UDFAddRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">import java.util.Random;</span><br><span class="line"></span><br><span class="line">public class UDFAddRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		int num = new Random().nextInt(10);</span><br><span class="line">		return s+&quot;_&quot;+num;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFAddRandom input = new UDFAddRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>UDFRemoveRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">public class UDFRemoveRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		return s.split(&quot;_&quot;)[0];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFRemoveRandom input = new UDFRemoveRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK_91&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="二、在查询中使用函数"><a href="#二、在查询中使用函数" class="headerlink" title="二、在查询中使用函数"></a>二、在查询中使用函数</h2><h3 id="临时函数"><a href="#临时函数" class="headerlink" title="临时函数"></a>临时函数</h3><ol>
<li><p>将包含函数的jar包上传到服务器上，我的存放目录是<code>/home/hadoop/lib</code></p>
</li>
<li><p>开启hive会话，执行以下命令添加jar：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; add jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br><span class="line">Added [/home/hadoop/lib/ruozedata-hive-1.0.jar] to class path</span><br><span class="line">Added resources: [/home/hadoop/lib/ruozedata-hive-1.0.jar]</span><br></pre></td></tr></table></figure></li>
<li><p>执行以下命令创建名为add_random的临时函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; create temporary function add_random as &#x27;com.ruozedata.hive.udf.UDFAddRandom&#x27;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.025 seconds</span><br></pre></td></tr></table></figure>

<p>remove_random同理。</p>
</li>
<li><p>使用函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; select * from emp;</span><br><span class="line">OK</span><br><span class="line">emp.empno	emp.ename	emp.job	emp.mgr	emp.hiredate	emp.sal	emp.comm	emp.deptno</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.0	NULL	20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.0	300.0	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.0	500.0	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.0	NULL	20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.0	1400.0	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.0	NULL	30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	NULL	10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.0	NULL	20</span><br><span class="line">7839	KING	PRESIDENT	NULL	1981-11-17	5000.0	NULL	10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.0	0.0	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.0	NULL	20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.0	NULL	30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.0	NULL	20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	NULL	10</span><br><span class="line">Time taken: 0.331 seconds, Fetched: 14 row(s)</span><br><span class="line">hive (hive)&gt; select ename,add_random(ename) from emp;</span><br><span class="line">OK</span><br><span class="line">ename	_c1</span><br><span class="line">SMITH	SMITH_8</span><br><span class="line">ALLEN	ALLEN_5</span><br><span class="line">WARD	WARD_1</span><br><span class="line">JONES	JONES_0</span><br><span class="line">MARTIN	MARTIN_0</span><br><span class="line">BLAKE	BLAKE_9</span><br><span class="line">CLARK	CLARK_5</span><br><span class="line">SCOTT	SCOTT_7</span><br><span class="line">KING	KING_8</span><br><span class="line">TURNER	TURNER_6</span><br><span class="line">ADAMS	ADAMS_6</span><br><span class="line">JAMES	JAMES_2</span><br><span class="line">FORD	FORD_6</span><br><span class="line">MILLER	MILLER_0</span><br><span class="line">Time taken: 0.882 seconds, Fetched: 14 row(s)</span><br><span class="line">hive (hive)&gt; </span><br></pre></td></tr></table></figure></li>
<li><p>这个UDF只在当前会话窗口生效，当您关闭了窗口此函数就不存在了；</p>
</li>
<li><p>如果您想在当前窗口将这个UDF清理掉，请依次执行以下两个命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop temporary function if exists add_random;</span><br><span class="line">delete jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br></pre></td></tr></table></figure></li>
<li><p>删除后再使用add_random会报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; drop temporary function if exists add_random;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.024 seconds</span><br><span class="line">hive (hive)&gt; delete jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br><span class="line">Deleted [/home/hadoop/lib/ruozedata-hive-1.0.jar] from class path</span><br><span class="line">hive (hive)&gt; select ename,add_random(ename) from emp;</span><br><span class="line">FAILED: SemanticException [Error 10011]: Invalid function add_random</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="永久函数"><a href="#永久函数" class="headerlink" title="永久函数"></a>永久函数</h3><ol>
<li><p>UDF永久生效,并且对所有hive会话都生效</p>
</li>
<li><p>hdfs上创建目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mkdir /lib/udflib</span><br></pre></td></tr></table></figure></li>
<li><p>将jar文件上传到hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -put /home/hadoop/lib/ruozedata-hive-1.0.jar /lib/udflib</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /lib/udflib</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       6098 2021-12-28 17:25 /lib/udflib/ruozedata-hive-1.0.jar</span><br></pre></td></tr></table></figure></li>
<li><p>开启hive会话，执行以下命令添加jar：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; create function add_random as &#x27;com.ruozedata.hive.udf.UDFAddRandom&#x27;</span><br><span class="line">           &gt; using jar &#x27;hdfs:///lib/udflib/ruozedata-hive-1.0.jar&#x27;;</span><br><span class="line">Added [/tmp/38a5942b-b210-4c46-b700-9a64ae6090b7_resources/ruozedata-hive-1.0.jar] to class path</span><br><span class="line">Added resources: [hdfs:///lib/udflib/ruozedata-hive-1.0.jar]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.097 seconds</span><br></pre></td></tr></table></figure></li>
<li><p>函数可以使用，新开hive会话也可使用。</p>
</li>
</ol>
<h2 id="三、整合函数到hive源码中，编译hive"><a href="#三、整合函数到hive源码中，编译hive" class="headerlink" title="三、整合函数到hive源码中，编译hive"></a>三、整合函数到hive源码中，编译hive</h2><ol>
<li><p>解压src包到相应目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src</span><br></pre></td></tr></table></figure></li>
<li><p>把函数放到目录<code>ql/src/java/org/apache/hadoop/hive/ql/udf</code>下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd ql/src/java/org/apache/hadoop/hive/ql/udf</span><br><span class="line">[hadoop@hadoop001 udf]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/ql/src/java/org/apache/hadoop/hive/ql/udf</span><br></pre></td></tr></table></figure></li>
<li><p>修改FunctionRegistry.java </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd ql/src/java/org/apache/hadoop/hive/ql/exec</span><br><span class="line">[hadoop@hadoop001 exec]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/ql/src/java/org/apache/hadoop/hive/ql/exec</span><br><span class="line">[hadoop@hadoop001 exec]$ vi FunctionRegistry.java </span><br></pre></td></tr></table></figure>

<p>到相关位置插入添加的函数信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hive.ql.udf.UDFAddRandom; </span><br><span class="line">import org.apache.hadoop.hive.ql.udf.UDFRemoveRandom; </span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">static &#123;</span><br><span class="line">    system.registerUDF(&quot;add_random&quot;, UDFAddRandom.class, false);</span><br><span class="line">	system.registerUDF(&quot;remove_random&quot;, UDFRemoveRandom.class, false);</span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>编译</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</span><br></pre></td></tr></table></figure>

<p>等待编译</p>
<p><img src="/2021/12/29/HIVE-UDF/BUILDSUCCESS.png" alt="img"></p>
<p>目录<code>packaging/target/</code>下的<code>apache-hive-3.1.2-bin.tar.gz</code>就是我们需要的tar包。（不想重新部署的话可以参考第7步）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd packaging/target/</span><br><span class="line">[hadoop@hadoop001 target]$ ll</span><br><span class="line">total 410320</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 antrun</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 apache-hive-3.1.2-bin</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 315855613 Dec 31 06:26 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  77970637 Dec 31 06:27 apache-hive-3.1.2-jdbc.jar</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  26307866 Dec 31 06:27 apache-hive-3.1.2-src.tar.gz</span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop      4096 Dec 31 06:26 archive-tmp</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 maven-shared-archive-resources</span><br><span class="line">drwxrwxr-x. 7 hadoop hadoop      4096 Dec 31 06:26 testconf</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 tmp</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 warehouse</span><br></pre></td></tr></table></figure></li>
<li><p>部署hive（省略）</p>
</li>
<li><p>检查函数</p>
<p>用<code>show functions</code>或者<code>desc function 函数名</code>都可以</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hive</span><br><span class="line">which: no hbase in (/home/hadoop/app/hive/bin:/home/hadoop/app/scala/bin:/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/home/hadoop/app/protobuf/bin:/home/hadoop/app/maven/bin:/usr/local/mysql/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = d35e6d35-1d7b-40ae-b86e-3bf09fc0f5d2</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 5fca1029-8f4a-4c52-9d22-0b2a0942cc85</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc function add_random;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">There is no documentation for function &#x27;add_random&#x27;</span><br><span class="line">Time taken: 0.078 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; desc function remove_random;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">There is no documentation for function &#x27;remove_random&#x27;</span><br><span class="line">Time taken: 0.049 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>替换jar包</p>
<p>找到我们需要替换的<code>hive-exec-3.1.2.jar</code>包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/packaging/target/apache-hive-3.1.2-bin/apache-hive-3.1.2-bin/lib</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.jar </span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 41063609 Dec 31 06:26 hive-exec-3.1.2.jar</span><br></pre></td></tr></table></figure>

<p>进入到正在使用的hive目录下，找到要被替换的包，改名备份</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ pwd</span><br><span class="line">/home/hadoop/software/hive/lib</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.jar </span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 40623961 Aug 23  2019 hive-exec-3.1.2.jar</span><br><span class="line">[hadoop@hadoop001 lib]$ mv hive-exec-3.1.2.jar hive-exec-3.1.2.jar_bak</span><br></pre></td></tr></table></figure>

<p>替换</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ cp /home/hadoop/software/apache-hive-3.1.2-src/packaging/target/apache-hive-3.1.2-bin/apache-hive-3.1.2-bin/lib/hive-exec-3.1.2.jar ./</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.*</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 41063609 Dec 31 06:46 hive-exec-3.1.2.jar</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 40623961 Aug 23  2019 hive-exec-3.1.2.jar_bak</span><br></pre></td></tr></table></figure>

<p>然后重启Hive即可找到函数。</p>
</li>
</ol>
<h2 id="四、Hive源码编译"><a href="#四、Hive源码编译" class="headerlink" title="四、Hive源码编译"></a>四、Hive源码编译</h2><p>个人在上面操作的源码编译上遇到好几个坑，又是改仓库又是修改java文件的。</p>
<p>一开始是直接下载hive官网下载地址<a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hive/hive-3.1.2/%E7%9A%84src%E5%8C%85%E3%80%82">https://dlcdn.apache.org/hive/hive-3.1.2/的src包。</a></p>
<p>在目录下执行命令开始编译</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</span><br></pre></td></tr></table></figure>

<p>问题一：<strong>pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar</strong>缺失</p>
<p>在maven的conf/settings.xml 中添加阿里云仓库地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ vi app/maven/conf/settings.xml </span><br></pre></td></tr></table></figure>

<p>注意：<code>&lt;mirror&gt;&lt;/mirror&gt;</code>标签要在<code>&lt;mirrors&gt;&lt;/mirrors&gt;</code>内,我最开始没注意，原来文本下面有个<code>&lt;mirrors&gt;</code>没注释掉</p>
<p><img src="/2021/12/29/HIVE-UDF/aliyun.png" alt="img"></p>
<p>我这里注释掉了，因为后来使用maven仓库下载的，没有用到阿里云。</p>
<p>问题二：添加阿里云仓库后，重新编译，依然报错</p>
<p><img src="/2021/12/29/HIVE-UDF/LLAP.png" alt="image-20211231161907516"></p>
<p>网上搜查发现有同样问题的，然后需要根据错误提示，参考<a target="_blank" rel="noopener" href="https://github.com/gitlbo/hive/commits/3.1.2%E4%BF%AE%E6%94%B9%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84%E6%9F%90%E5%87%A0%E4%B8%AA%E7%B1%BB%E3%80%82%E4%BA%8E%E6%98%AF%E6%88%91%E6%8C%89%E6%AD%A5%E9%AA%A4%E4%BF%AE%E6%94%B9%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%BC%96%E8%AF%91%E8%BF%99%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%BA%86%EF%BC%8C%E4%BD%86%E5%8F%88%E6%9C%89%E5%85%B6%E4%BB%96%E5%9C%B0%E6%96%B9%E6%8A%A5%E9%94%99%E3%80%82">https://github.com/gitlbo/hive/commits/3.1.2修改源码中的某几个类。于是我按步骤修改后，可以编译这个组件了，但又有其他地方报错。</a></p>
<p>既然都要按照修改，为什么不直接用最新的src包呢，于是我下载了一个新的src包。</p>
<p><img src="/2021/12/29/HIVE-UDF/hive1.png" alt="image-20211231162607453"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive2.png" alt="image-20211231162633319"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive3.png" alt="image-20211231162658887"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive4.png" alt="image-20211231162727453"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 software]$ wget -O apache-hive-3.1.2-src.zip https://codeload.github.com/gitlbo/hive/zip/c073e71ef43699b7aa68cad7c69a2e8f487089fd</span><br></pre></td></tr></table></figure>

<p>然后解压，修改pom.xml里的hadoop.version为我的版本3.2.2.然后其他根据个人需要修改。</p>
<p>使用命令编译<code>mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</code>，没有报错，编译成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd packaging/target/</span><br><span class="line">[hadoop@hadoop001 target]$ ll</span><br><span class="line">total 410320</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 antrun</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 apache-hive-3.1.2-bin</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 315855613 Dec 31 06:26 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  77970637 Dec 31 06:27 apache-hive-3.1.2-jdbc.jar</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  26307866 Dec 31 06:27 apache-hive-3.1.2-src.tar.gz</span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop      4096 Dec 31 06:26 archive-tmp</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 maven-shared-archive-resources</span><br><span class="line">drwxrwxr-x. 7 hadoop hadoop      4096 Dec 31 06:26 testconf</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 tmp</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 warehouse</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2021/12/29/HIVE-UDF/" data-id="cl3b9tuab0009akudcnaa0ppt" data-title="HIVE UDF 与 HIVE源码编译" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/7/">&laquo; 上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/9/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/">Flume的使用案例</a>
          </li>
        
          <li>
            <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process</a>
          </li>
        
          <li>
            <a href="/2022/05/16/Flume%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/">Flume介绍与使用</a>
          </li>
        
          <li>
            <a href="/2022/05/13/Kafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%82%A3%E4%B9%88%E5%BF%AB/">Kafka为什么能那么快</a>
          </li>
        
          <li>
            <a href="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/">我的mysql笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 k12<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>