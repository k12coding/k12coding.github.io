<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"k12coding.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="MapReduce Input Split（输入分&#x2F;切片）详解 输入分片（Input Split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。 分片大小范围可以在mapred-site.xml">
<meta property="og:type" content="article">
<meta property="og:title" content="MapReduce Input Split（输入分&#x2F;切片）详解">
<meta property="og:url" content="https://k12coding.github.io/2021/12/14/MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="MapReduce Input Split（输入分&#x2F;切片）详解 输入分片（Input Split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。 分片大小范围可以在mapred-site.xml">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://images.cnitblog.com/blog/306623/201306/23175247-1cff38de2f154503bccd89a5d057f696.x-png">
<meta property="og:image" content="https://img-blog.csdn.net/20160414132403422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20140407103622015?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20140407103853218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20140407104727562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20140407105057203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20140407104039078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="article:published_time" content="2021-12-14T12:09:56.000Z">
<meta property="article:modified_time" content="2021-12-14T19:19:58.856Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://images.cnitblog.com/blog/306623/201306/23175247-1cff38de2f154503bccd89a5d057f696.x-png">


<link rel="canonical" href="https://k12coding.github.io/2021/12/14/MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://k12coding.github.io/2021/12/14/MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3/","path":"2021/12/14/MapReduce-Input-Split（输入分-切片）详解/","title":"MapReduce Input Split（输入分/切片）详解"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MapReduce Input Split（输入分/切片）详解 | k12的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">k12的博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">k12的笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3"><span class="nav-text">MapReduce Input Split（输入分&#x2F;切片）详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#getSplits-%E6%96%B9%E6%B3%95%E5%9C%A8-FileInputFormat-addInputPath-job-path-%E4%B8%AD"><span class="nav-text">getSplits()方法在 FileInputFormat.addInputPath(job, path)中</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop-map%E5%92%8Creduce%E6%95%B0%E9%87%8F%E4%BC%B0%E7%AE%97"><span class="nav-text">Hadoop map和reduce数量估算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#map-task%E6%95%B0%E9%87%8F"><span class="nav-text">map task数量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduce-task%E6%95%B0%E9%87%8F"><span class="nav-text">reduce task数量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-text">总结：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#map%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B"><span class="nav-text">map数据计算示例</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">k12</p>
  <div class="site-description" itemprop="description">2</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2021/12/14/MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MapReduce Input Split（输入分/切片）详解
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-12-14 20:09:56" itemprop="dateCreated datePublished" datetime="2021-12-14T20:09:56+08:00">2021-12-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-12-15 03:19:58" itemprop="dateModified" datetime="2021-12-15T03:19:58+08:00">2021-12-15</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="MapReduce-Input-Split（输入分-切片）详解"><a href="#MapReduce-Input-Split（输入分-切片）详解" class="headerlink" title="MapReduce Input Split（输入分/切片）详解"></a>MapReduce Input Split（输入分/切片）详解</h2><p><strong><img src="http://images.cnitblog.com/blog/306623/201306/23175247-1cff38de2f154503bccd89a5d057f696.x-png" alt="img"></strong></p>
<p><strong>输入分片（Input Split）</strong>：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。</p>
<p>分片大小范围可以在mapred-site.xml中设置，mapred.min.split.size mapred.max.split.size，</p>
<p>minSplitSize大小默认为1B，maxSplitSize大小默认为Long.MAX_VALUE = 9223372036854775807</p>
<p><strong>那么分片到底是多大呢？</strong></p>
<p>minSize=max{minSplitSize,mapred.min.split.size} </p>
<p>maxSize=mapred.max.split.size</p>
<p>splitSize=max{minSize,min{maxSize,blockSize}}</p>
<p>我们再来看一下源码</p>
<p><img src="https://img-blog.csdn.net/20160414132403422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p><strong>所以在我们没有设置分片的范围的时候，分片大小是由block块大小决定的，和它的大小一样。比如把一个258MB的文件上传到HDFS上，假设block块大小是128MB，那么它就会被分成三个block块，与之对应产生三个split****，所以最终会产生三个map task。我又发现了另一个问题，第三个block块里存的文件大小只有2MB，而它的block块大小是128MB，那它实际占用Linux file system的多大空间？</strong></p>
<p><em><strong>*答案是实际的文件大小，而非一个块的大小。<br>*</strong></em></p>
<p><strong>有大神已经验证这个答案了：<a target="_blank" rel="noopener" href="http://blog.csdn.net/samhacker/article/details/23089157">http://blog.csdn.net/samhacker/article/details/23089157</a></strong></p>
<p>1、往hdfs里面添加新文件前，hadoop在linux上面所占的空间为 464 MB：</p>
<p><img src="https://img-blog.csdn.net/20140407103622015?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>2、往hdfs里面添加大小为2673375 byte(大概2.5 MB)的文件：</p>
<p><em>2673375 derby.jar</em></p>
<p>3、此时，hadoop在linux上面所占的空间为 467 MB——**增加了一个实际文件大小(2.5 MB)的空间，而非一个block size(128 MB)**：</p>
<p><img src="https://img-blog.csdn.net/20140407103853218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>4、使用hadoop dfs -stat查看文件信息： </p>
<p><img src="https://img-blog.csdn.net/20140407104727562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这里就很清楚地反映出： 文件的实际大小(file size)是2673375 byte， 但它的block size是128 MB。</p>
<p>5、通过NameNode的web console来查看文件信息: </p>
<p><img src="https://img-blog.csdn.net/20140407105057203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>结果是一样的： 文件的实际大小(file size)是2673375 byte， 但它的block size是128 MB。</p>
<p>6、不过使用‘hadoop fsck’查看文件信息，看出了一些不一样的内容—— ‘1（avg.block size 2673375 B）’: </p>
<p><img src="https://img-blog.csdn.net/20140407104039078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FtaGFja2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>值得注意的是，结果中有一个 ‘1（avg.block size 2673375 B）’的字样。这里的 ‘block size’ 并不是指平常说的文件块大小(Block Size)—— 后者是一个元数据的概念，相反它反映的是文件的实际大小(file size)。以下是Hadoop Community的专家给我的回复： </p>
<p><em>“The fsck is showing you an “average blocksize”, not the block size metadata attribute of the file like stat shows. In this specific case, the average is just the length of your file, which is lesser than one whole block.”</em></p>
<p>最后一个问题是： 如果hdfs占用Linux file system的磁盘空间按实际文件大小算，那么这个”块大小“有必要存在吗？</p>
<p>其实块大小还是必要的，一个显而易见的作用就是当文件通过append操作不断增长的过程中，可以通过来block size决定何时split文件。以下是Hadoop Community的专家给我的回复： </p>
<p><em>“The block size is a meta attribute. If you append tothe file later, it still needs to know when to split further - so it keeps that value as a mere metadata it can use to advise itself on write boundaries.”</em> </p>
<p><strong>补充：我还查到这样一段话</strong></p>
<p>原文地址：<a target="_blank" rel="noopener" href="http://blog.csdn.net/lylcore/article/details/9136555">http://blog.csdn.net/lylcore/article/details/9136555</a></p>
<p>一个split的大小是由goalSize, minSize, blockSize这三个值决定的。computeSplitSize的逻辑是，先从goalSize和blockSize两个值中选出最小的那个（比如一般不设置map数，这时blockSize为当前文件的块size，而goalSize是文件大小除以用户设置的map数得到的，如果没设置的话，默认是1）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>hadooop提供了一个设置map个数的参数mapred.map.tasks，我们可以通过这个参数来控制map的个数。但是通过这种方式设置map的个数，并不是每次都有效的。原因是mapred.map.tasks只是一个hadoop的参考数值，最终map的个数，还取决于其他的因素。</p>
<p>为了方便介绍，先来看几个名词：</p>
<p><strong>block_size</strong> : hdfs的文件块大小，默认为64M，可以通过参数dfs.block.size设置</p>
<p><strong>total_size</strong> : 输入文件整体的大小</p>
<p><strong>input_file_num</strong> : 输入文件的个数</p>
<ol>
<li><p><strong>默认map个数</strong></p>
<p>如果不进行任何设置，默认的map个数是和blcok_size相关的。</p>
<p>default_num = total_size / block_size;</p>
</li>
<li><p><strong>期望大小</strong></p>
<p>可以通过参数 mapred.map.tasks来设置程序员期望的map个数，但是这个个数只有在大于default_num的时候，才会生效。</p>
<p>goal_num = mapred.map.tasks;</p>
</li>
<li><p><strong>设置处理的文件大小</strong></p>
<p>可以通过mapred.min.split.size 设置每个task处理的文件大小，但是这个大小只有在大于 block_size的时候才会生效。</p>
<p>split_size = max( mapred.min.split.size, block_size );</p>
<p>split_num = total_size / split_size;</p>
</li>
<li><p><strong>计算的map个数</strong></p>
<p>compute_map_num = min(split_num,  max(default_num, goal_num))</p>
</li>
</ol>
<p>除了这些配置以外，mapreduce还要遵循一些原则。 mapreduce的每一个map处理的数据是不能跨越文件的，也就是说min_map_num &gt;= input_file_num。 所以，最终的map个数应该为：</p>
<p>   final_map_num = max(compute_map_num, input_file_num)</p>
<p>   经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：</p>
<p>（1）如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。</p>
<p>（2）如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。</p>
<p>（3）如果输入中有很多小文件，依然想减少map个数，则需要将小文件merger为大文件，然后使用准则2。</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Dr_Guo/article/details/51150278">https://blog.csdn.net/Dr_Guo/article/details/51150278</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lylcore/article/details/9136555">https://blog.csdn.net/lylcore/article/details/9136555</a></p>
<h2 id="getSplits-方法在-FileInputFormat-addInputPath-job-path-中"><a href="#getSplits-方法在-FileInputFormat-addInputPath-job-path-中" class="headerlink" title="getSplits()方法在 FileInputFormat.addInputPath(job, path)中"></a>getSplits()方法在 FileInputFormat.addInputPath(job, path)中</h2><pre><code>    /** 
     * Generate the list of files and make them into FileSplits.
     * @param job the job context
     * @throws IOException
     */
    public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123;
        //用于记录分片开始的时间，最后会得到一个分片总用时，时间单位是纳秒
      StopWatch sw = new StopWatch().start();
      //用来计算分片大小
      //minSize 就是 1
      //maxSize 追到最下面可以发现其实就是long的最大值
      long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
      long maxSize = getMaxSplitSize(job);
  
      //存放切片对象
      List&lt;InputSplit&gt; splits = new ArrayList&lt;InputSplit&gt;();
      //得到路径下的所有文件
      List&lt;FileStatus&gt; files = listStatus(job);
      //遍历得到的文件
      for (FileStatus file: files) &#123;
        //得到文件路径
        Path path = file.getPath();
        //获取文件大小
        long length = file.getLen();
        //如果文件大小不为0的话
        if (length != 0) &#123;
            //定义块数组，存放块在datanode上的位置
          BlockLocation[] blkLocations;
          if (file instanceof LocatedFileStatus) &#123;
            blkLocations = ((LocatedFileStatus) file).getBlockLocations();
          &#125; else &#123;
            FileSystem fs = path.getFileSystem(job.getConfiguration());
            blkLocations = fs.getFileBlockLocations(file, 0, length);
          //如果这个文件可以分片的话进行分片，zip、视频等不能进行分片
          if (isSplitable(job, path)) &#123;
            //获取块大小，hadoop1默认是64M  hadoop2默认是128M  hadoop3默认是256M
            long blockSize = file.getBlockSize();
            //得到片大小
            //--&gt; 最终决定出切片的大小(128M) --&gt; blockSize值
              //Math.max(minSize, Math.min(maxSize, blockSize));这是实现
            long splitSize = computeSplitSize(blockSize, minSize, maxSize);
            //获取文件大小
            long bytesRemaining = length;
            //文件大小/片大小&gt;1.1 开始分片
            //例如  文件大小为260M  260/128=2.03&gt;1.1 进入循环开始分片
            //132/128 &lt;1.1  不再进行分片，循环结束
            while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;
              int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
              splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                          blkLocations[blkIndex].getHosts(),
                           blkLocations[blkIndex].getCachedHosts()));
              //文件大小 = 原文件大小 - 当前分片大小
              //260 -128 = 132 现在文件大小是132 MB
              bytesRemaining -= splitSize;
            &#125;
             //循环结束之后,只要文件大小不等于0 此时也会在切一个片
            if (bytesRemaining != 0) &#123;
              int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
              splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                         blkLocations[blkIndex].getHosts(),
                         blkLocations[blkIndex].getCachedHosts()));
            &#125;
          &#125; else &#123; // not splitable
            splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                        blkLocations[0].getCachedHosts()));
          &#125;
        &#125; else &#123; 
          //为零长度文件创建空主机数组
          splits.add(makeSplit(path, 0, length, new String[0]));
        &#125;
      &#125;
      // 保存文件数
      job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
      sw.stop();
      //返回携带着切片文件的集合
      return splits;
    &#125;
</code></pre>
<h2 id="Hadoop-map和reduce数量估算"><a href="#Hadoop-map和reduce数量估算" class="headerlink" title="Hadoop map和reduce数量估算"></a>Hadoop map和reduce数量估算</h2><p>Hadoop在运行一个mapreduce job之前，需要估算这个job的maptask数和reducetask数。</p>
<h3 id="map-task数量"><a href="#map-task数量" class="headerlink" title="map task数量"></a>map task数量</h3><p>首先分析一下job的maptask数，当一个job提交时，jobclient首先分析job被拆分的split数量，然后吧job.split文件放置在HDFS中，一个job的MapTask数量就等于split的个数。</p>
<p>job.split中包含split的个数由FileInputFormat.getSplits计算出，方法的逻辑如下：</p>
<ol>
<li><p>读取参数mapred.map.tasks，这个参数默认设置为0，生产系统中很少修改。</p>
</li>
<li><p>计算input文件的总字节数，总字节数/(mapred.map.tasks==0 ? 1: mapred.map.tasks )=goalsize</p>
</li>
<li><p>每个split的最小值minSize由mapred.min.split.size参数设置，这个参数默认设置为0，生产系统中很少修改。</p>
</li>
<li><p>调用computeSplitSize方法，计算出splitsize= Math.max(minSize, Math.min(goalSize, blockSize)),通常这个值=blockSize，输入的文件较小，文件字节数之和小于blocksize时，splitsize=输入文件字节数之和。</p>
</li>
<li><p>对于input的每个文件，计算split的个数。</p>
<p>a) 文件大小/splitsize&gt;1.1，创建一个split，这个split的字节数=splitsize，文件剩余字节数=文件大小-splitsize</p>
<p>b) 文件剩余字节数/splitsize&lt;1.1，剩余的部分作为一个split</p>
</li>
</ol>
<p>举例说明：</p>
<ol>
<li><p>input只有一个文件，大小为100M,splitsize=blocksize,则split数为2，第一个split为64M,第二个为36M</p>
</li>
<li><p>input只有一个文件，大小为65M,splitsize=blocksize，则split数为1，split大小为65M</p>
</li>
<li><p>input只有一个文件，大小为129M,splitsize=blocksize，则split数为2，第一个split为64M,第二个为65M(最后一个split的大小可能超过splitsize)</p>
</li>
<li><p>input只有一个文件，大小为20M ,splitsize=blocksize，则split数为1，split大小为20M</p>
</li>
<li><p>input有两个文件，大小为100M和20M,splitsize=blocksize,则split数为3，第一个文件分为两个split，第一个split为64M,第二个为36M，第二个文件为一个split，大小为20M</p>
</li>
<li><p>input有两个文件，大小为25M和20M,splitsize=blocksize,则split数为2，第一个文件为一个split，大小为25M，第二个文件为一个split，大小为20M</p>
</li>
</ol>
<p>假设一个job的input大小固定为100M,当只包含一个文件时，split个数为2，maptask数为2，但当包含10个10M的文件时，maptask数为10。</p>
<h3 id="reduce-task数量"><a href="#reduce-task数量" class="headerlink" title="reduce task数量"></a>reduce task数量</h3><p>下面来分析reducetask，纯粹的mapreduce task的reduce task数很简单，就是参数mapred.reduce.tasks的值，hadoop-site.xml文件中和mapreduce job运行时不设置的话默认为1。</p>
<p>在HIVE中运行sql的情况又不同，hive会估算reduce task的数量，估算方法如下：</p>
<p>通常是ceil(input文件大小/1024<em>1024</em>1024)，每1GB大小的输入文件对应一个reduce task。</p>
<p>特殊的情况是当sql只查询count(*)时，reduce task数被设置成1。</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>通过map和reduce task数量的分析可以看出，hadoop/hive估算的map和reduce task数可能和实际情况相差甚远。假定某个job的input数据量庞大，reduce task数量也会随之变大，而通过join和group by，实际output的数据可能不多，但reduce会输出大量的小文件，这个job的下游任务将会启动同样多的map来处理前面reduce产生的大量文件。在生产环境中每个user group有一个map task数的限额，一个job启动大量的map task很显然会造成其他job等待释放资源。</p>
<p>Hive对于上面描述的情况有一种补救措施，参数hive.merge.smallfiles.avgsize控制hive对output小文件的合并，当hiveoutput的文件的平均大小小于hive.merge.smallfiles.avgsize-默认为16MB左右，hive启动一个附加的mapreducejob合并小文件，合并后文件大小不超过hive.merge.size.per.task-默认为256MB。</p>
<p>尽管Hive可以启动小文件合并的过程，但会消耗掉额外的计算资源，控制单个reduce task的输出大小&gt;64MB才是最好的解决办法。</p>
<h3 id="map数据计算示例"><a href="#map数据计算示例" class="headerlink" title="map数据计算示例"></a>map数据计算示例</h3><p>hive&gt; set dfs.block.size;<br>dfs.block.size=268435456<br>hive&gt; set mapred.map.tasks;<br>mapred.map.tasks=2</p>
<p>文件块大小为256MB,map.tasks为2</p>
<p>查看文件大小和文件数：（共4539.059804MB,18个文件）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[dwapp@dw-yuntigw-63 hadoop]$ hadoop dfs -ls /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25;</span><br><span class="line">Found 18 items</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290700555 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000000_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290695945 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000001_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290182606 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000002_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 271979933 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000003_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258448208 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000004_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258440338 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000005_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258419852 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000006_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258347423 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000007_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258349480 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000008_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258301657 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000009_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258270954 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000010_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258266805 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000011_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258253133 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000012_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258236047 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000013_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258239072 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000014_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258170671 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000015_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258160711 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000016_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258085783 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000017_0</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>文件：</th>
<th>大小Bytes</th>
<th>大小MB</th>
<th></th>
<th>splitsize(MB)</th>
<th>每个文件需要的map数量</th>
</tr>
</thead>
<tbody><tr>
<td>文件1</td>
<td>290700555</td>
<td>277.2336531</td>
<td></td>
<td>256</td>
<td>1.082943957</td>
</tr>
<tr>
<td>文件2</td>
<td>290695945</td>
<td>277.2292566</td>
<td></td>
<td>256</td>
<td>1.082926784</td>
</tr>
<tr>
<td>文件3</td>
<td>290182606</td>
<td>276.7396984</td>
<td></td>
<td>256</td>
<td>1.081014447</td>
</tr>
<tr>
<td>文件4</td>
<td>271979933</td>
<td>259.3802767</td>
<td></td>
<td>256</td>
<td>1.013204206</td>
</tr>
<tr>
<td>文件5</td>
<td>258448208</td>
<td>246.4754181</td>
<td></td>
<td>256</td>
<td>0.962794602</td>
</tr>
<tr>
<td>文件6</td>
<td>258440338</td>
<td>246.4679127</td>
<td></td>
<td>256</td>
<td>0.962765284</td>
</tr>
<tr>
<td>文件7</td>
<td>258419852</td>
<td>246.4483757</td>
<td></td>
<td>256</td>
<td>0.962688968</td>
</tr>
<tr>
<td>文件8</td>
<td>258347423</td>
<td>246.379302</td>
<td></td>
<td>256</td>
<td>0.962419149</td>
</tr>
<tr>
<td>文件9</td>
<td>258349480</td>
<td>246.3812637</td>
<td></td>
<td>256</td>
<td>0.962426811</td>
</tr>
<tr>
<td>文件10</td>
<td>258301657</td>
<td>246.3356562</td>
<td></td>
<td>256</td>
<td>0.962248657</td>
</tr>
<tr>
<td>文件11</td>
<td>258270954</td>
<td>246.3063755</td>
<td></td>
<td>256</td>
<td>0.962134279</td>
</tr>
<tr>
<td>文件12</td>
<td>258266805</td>
<td>246.3024187</td>
<td></td>
<td>256</td>
<td>0.962118823</td>
</tr>
<tr>
<td>文件13</td>
<td>258253133</td>
<td>246.2893801</td>
<td></td>
<td>256</td>
<td>0.962067891</td>
</tr>
<tr>
<td>文件14</td>
<td>258236047</td>
<td>246.2730856</td>
<td></td>
<td>256</td>
<td>0.962004241</td>
</tr>
<tr>
<td>文件15</td>
<td>258239072</td>
<td>246.2759705</td>
<td></td>
<td>256</td>
<td>0.96201551</td>
</tr>
<tr>
<td>文件16</td>
<td>258170671</td>
<td>246.2107382</td>
<td></td>
<td>256</td>
<td>0.961760696</td>
</tr>
<tr>
<td>文件17</td>
<td>258160711</td>
<td>246.2012396</td>
<td></td>
<td>256</td>
<td>0.961723592</td>
</tr>
<tr>
<td>文件18</td>
<td>258085783</td>
<td>246.1297827</td>
<td></td>
<td>256</td>
<td>0.961444464</td>
</tr>
<tr>
<td>总文件大小：</td>
<td>4759549173</td>
<td>4539.059804</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>goalSize = 4539.059804 （文件总大小）/ mapred.map.tasks(2) = 2269.529902MB</p>
<p>因此splitsize取值为256MB，所以一共分配18个map。</p>
<p>修改map.tasks参数为32<br>set mapred.map.tasks = 32;</p>
<table>
<thead>
<tr>
<th>文件：</th>
<th>大小Bytes</th>
<th>大小MB</th>
<th></th>
<th>splitsize(MB)</th>
<th>每个文件需要的map数量</th>
</tr>
</thead>
<tbody><tr>
<td>文件1</td>
<td>290700555</td>
<td>277.2336531</td>
<td></td>
<td>141.8</td>
<td>1.955103336</td>
</tr>
<tr>
<td>文件2</td>
<td>290695945</td>
<td>277.2292566</td>
<td></td>
<td>141.8</td>
<td>1.955072332</td>
</tr>
<tr>
<td>文件3</td>
<td>290182606</td>
<td>276.7396984</td>
<td></td>
<td>141.8</td>
<td>1.951619876</td>
</tr>
<tr>
<td>文件4</td>
<td>271979933</td>
<td>259.3802767</td>
<td></td>
<td>141.8</td>
<td>1.829198002</td>
</tr>
<tr>
<td>文件5</td>
<td>258448208</td>
<td>246.4754181</td>
<td></td>
<td>141.8</td>
<td>1.738190537</td>
</tr>
<tr>
<td>文件6</td>
<td>258440338</td>
<td>246.4679127</td>
<td></td>
<td>141.8</td>
<td>1.738137607</td>
</tr>
<tr>
<td>文件7</td>
<td>258419852</td>
<td>246.4483757</td>
<td></td>
<td>141.8</td>
<td>1.737999829</td>
</tr>
<tr>
<td>文件8</td>
<td>258347423</td>
<td>246.379302</td>
<td></td>
<td>141.8</td>
<td>1.737512708</td>
</tr>
<tr>
<td>文件9</td>
<td>258349480</td>
<td>246.3812637</td>
<td></td>
<td>141.8</td>
<td>1.737526543</td>
</tr>
<tr>
<td>文件10</td>
<td>258301657</td>
<td>246.3356562</td>
<td></td>
<td>141.8</td>
<td>1.737204909</td>
</tr>
<tr>
<td>文件11</td>
<td>258270954</td>
<td>246.3063755</td>
<td></td>
<td>141.8</td>
<td>1.736998417</td>
</tr>
<tr>
<td>文件12</td>
<td>258266805</td>
<td>246.3024187</td>
<td></td>
<td>141.8</td>
<td>1.736970513</td>
</tr>
<tr>
<td>文件13</td>
<td>258253133</td>
<td>246.2893801</td>
<td></td>
<td>141.8</td>
<td>1.736878562</td>
</tr>
<tr>
<td>文件14</td>
<td>258236047</td>
<td>246.2730856</td>
<td></td>
<td>141.8</td>
<td>1.73676365</td>
</tr>
<tr>
<td>文件15</td>
<td>258239072</td>
<td>246.2759705</td>
<td></td>
<td>141.8</td>
<td>1.736783995</td>
</tr>
<tr>
<td>文件16</td>
<td>258170671</td>
<td>246.2107382</td>
<td></td>
<td>141.8</td>
<td>1.736323965</td>
</tr>
<tr>
<td>文件17</td>
<td>258160711</td>
<td>246.2012396</td>
<td></td>
<td>141.8</td>
<td>1.736256979</td>
</tr>
<tr>
<td>文件18</td>
<td>258085783</td>
<td>246.1297827</td>
<td></td>
<td>141.8</td>
<td>1.735753051</td>
</tr>
<tr>
<td>总文件大小：</td>
<td>4759549173</td>
<td>4539.059804</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>goalSize = 4539.059804 / mapred.map.tasks(32) = 141.8456189</p>
<p>因此splitsize取值为141.8MB，所以一共分配36个map。</p>
<p>原文地址：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ibook360/p/4137592.html">https://www.cnblogs.com/ibook360/p/4137592.html</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/12/13/HDFS-API/" rel="prev" title="HDFS API">
                  <i class="fa fa-chevron-left"></i> HDFS API
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k12</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





</body>
</html>
