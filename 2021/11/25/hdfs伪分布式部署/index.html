<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"k12coding.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="hdfs伪分布式部署Releases Archive中选择要部署的版本，我们以Release 3.2.2 available版本为例 参考文档：Hadoop: Setting up a Single Node Cluster. 一、部署1.软件要求 Java：Hadoop对Java版本有要求，具体参考Hadoop Java Versions，基本上Java8通用 ssh  补充：组件名称大写-数字">
<meta property="og:type" content="article">
<meta property="og:title" content="hdfs伪分布式部署">
<meta property="og:url" content="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="hdfs伪分布式部署Releases Archive中选择要部署的版本，我们以Release 3.2.2 available版本为例 参考文档：Hadoop: Setting up a Single Node Cluster. 一、部署1.软件要求 Java：Hadoop对Java版本有要求，具体参考Hadoop Java Versions，基本上Java8通用 ssh  补充：组件名称大写-数字">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/image-20211126093406585.png">
<meta property="article:published_time" content="2021-11-25T05:09:16.000Z">
<meta property="article:modified_time" content="2021-11-27T06:16:46.391Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/image-20211126093406585.png">


<link rel="canonical" href="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/","path":"2021/11/25/hdfs伪分布式部署/","title":"hdfs伪分布式部署"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>hdfs伪分布式部署 | k12的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">k12的博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">k12的笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="nav-text">hdfs伪分布式部署</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E9%83%A8%E7%BD%B2"><span class="nav-text">一、部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%BD%AF%E4%BB%B6%E8%A6%81%E6%B1%82"><span class="nav-text">1.软件要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-tar%E5%8C%85%E8%A7%A3%E5%8E%8B"><span class="nav-text">2.tar包解压</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95"><span class="nav-text">3.查看文件目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AEJava%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%88%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="nav-text">4.手动配置Java环境变量（必须）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%89%A7%E8%A1%8Cbin-hadoop%E6%9F%A5%E7%9C%8Bhdfs%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-text">5.执行bin&#x2F;hadoop查看hdfs使用说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="nav-text">6.修改配置文件：伪分布式部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%AE%BE%E7%BD%AESSH%E7%A7%81%E9%92%A5%E5%8F%96%E6%B6%88%E5%AF%86%E7%A0%81"><span class="nav-text">7.设置SSH私钥取消密码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%89%A7%E8%A1%8C"><span class="nav-text">二、执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E6%89%A7%E8%A1%8C"><span class="nav-text">本地执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-text">1.格式化文件系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%90%AF%E5%8A%A8NameNode%E8%8A%82%E7%82%B9%E5%92%8CDataNode%E8%8A%82%E7%82%B9"><span class="nav-text">2.启动NameNode节点和DataNode节点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AENameNode"><span class="nav-text">3.通过浏览器访问NameNode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%9C%A8hdfs%E4%B8%AD%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95%E6%89%A7%E8%A1%8CMapReduce-jobs"><span class="nav-text">4.在hdfs中创建目录执行MapReduce jobs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%A4%8D%E5%88%B6input%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%B0hdfs%E7%B3%BB%E7%BB%9F%E4%B8%AD"><span class="nav-text">5.复制input文件夹下的文件到hdfs系统中</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-%E6%89%A7%E8%A1%8CMapReduce%E4%BB%BB%E5%8A%A1"><span class="nav-text">6.执行MapReduce任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="nav-text">7.查看执行结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-%E5%81%9C%E6%AD%A2%E6%9C%8D%E5%8A%A1"><span class="nav-text">8.停止服务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YARN%E4%B8%8A%E6%89%A7%E8%A1%8C"><span class="nav-text">YARN上执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-text">1.修改配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%90%AF%E5%8A%A8ResourceManager%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B%E5%92%8CNodeManager%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B"><span class="nav-text">2.启动ResourceManager守护进程和NodeManager守护进程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AE%E8%AE%BF%E9%97%AEResourceManager"><span class="nav-text">3.通过浏览器访问访问ResourceManager</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E6%89%A7%E8%A1%8CMapReduce%E4%BB%BB%E5%8A%A1"><span class="nav-text">4.执行MapReduce任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%81%9C%E6%AD%A2%E6%9C%8D%E5%8A%A1"><span class="nav-text">5.停止服务</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">k12</p>
  <div class="site-description" itemprop="description">2</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hdfs伪分布式部署
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-11-25 13:09:16" itemprop="dateCreated datePublished" datetime="2021-11-25T13:09:16+08:00">2021-11-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-11-27 14:16:46" itemprop="dateModified" datetime="2021-11-27T14:16:46+08:00">2021-11-27</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="hdfs伪分布式部署"><a href="#hdfs伪分布式部署" class="headerlink" title="hdfs伪分布式部署"></a>hdfs伪分布式部署</h1><p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/release.html">Releases Archive</a>中选择要部署的版本，我们以<a target="_blank" rel="noopener" href="https://hadoop.apache.org/release/3.2.2.html">Release 3.2.2 available</a>版本为例</p>
<p>参考文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop: Setting up a Single Node Cluster.</a></p>
<h2 id="一、部署"><a href="#一、部署" class="headerlink" title="一、部署"></a>一、部署</h2><h3 id="1-软件要求"><a href="#1-软件要求" class="headerlink" title="1.软件要求"></a>1.软件要求</h3><ul>
<li>Java：Hadoop对Java版本有要求，具体参考<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions">Hadoop Java Versions</a>，基本上Java8通用</li>
<li>ssh</li>
</ul>
<p>补充：组件名称<code>大写-数字</code>，如：SPARK-2908，表明该组件是有问题的</p>
<span id="more"></span>

<h3 id="2-tar包解压"><a href="#2-tar包解压" class="headerlink" title="2.tar包解压"></a>2.tar包解压</h3><p>点击下载:<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz">hadoop-3.2.2.tar.gz</a></p>
<p>下载后通过rz命令上传至Linux系统</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ ll software/</span><br><span class="line">total 805204</span><br><span class="line">-rw-r--r--.  1 hadoop hadoop 395448622 Nov 21 10:03 hadoop-3.2.2.tar.gz</span><br></pre></td></tr></table></figure>

<p>解压hadoop到app目录下，创建软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ tar -xzvf software/hadoop-3.2.2.tar.gz -C app/</span><br><span class="line">[hadoop@hadoop001 ~]$ cd app</span><br><span class="line">[hadoop@hadoop001 app]$ </span><br><span class="line">[hadoop@hadoop001 app]$ ln -s /home/hadoop/app/hadoop-3.2.2 hadoop</span><br><span class="line">[hadoop@hadoop001 app]$ ll</span><br><span class="line">total 2</span><br><span class="line">lrwxrwxrwx. 1 hadoop hadoop   29 Nov 25 16:28 hadoop -&gt; /home/hadoop/app/hadoop-3.2.2</span><br><span class="line">drwxr-xr-x. 9 hadoop hadoop 4096 Jan  3  2021 hadoop-3.2.2</span><br></pre></td></tr></table></figure>

<h3 id="3-查看文件目录"><a href="#3-查看文件目录" class="headerlink" title="3.查看文件目录"></a>3.查看文件目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 app]$ cd hadoop/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll</span><br><span class="line">total 216</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 bin				#命令执行脚本</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 etc				#配置文件</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 include</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:14 input</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 lib</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 libexec</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 150569 Dec  5  2020 LICENSE.txt</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:48 logs</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  21943 Dec  5  2020 NOTICE.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Nov 21 11:01 output</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop   1361 Dec  5  2020 README.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 sbin			#启动停止脚本</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 share</span><br></pre></td></tr></table></figure>

<p>大部分的大数据项目解压后目录：bin</p>
<h3 id="4-手动配置Java环境变量（必须）"><a href="#4-手动配置Java环境变量（必须）" class="headerlink" title="4.手动配置Java环境变量（必须）"></a>4.手动配置Java环境变量（必须）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use. By default, this environment</span><br><span class="line"># variable is REQUIRED on ALL platforms except OS X!</span><br><span class="line"># export JAVA_HOME=/usr/java/latest</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure>

<h3 id="5-执行bin-hadoop查看hdfs使用说明"><a href="#5-执行bin-hadoop查看hdfs使用说明" class="headerlink" title="5.执行bin/hadoop查看hdfs使用说明"></a>5.执行bin/hadoop查看hdfs使用说明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ bin/hadoop</span><br><span class="line">Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]</span><br><span class="line"> or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]</span><br><span class="line">  where CLASSNAME is a user-provided Java class</span><br><span class="line"></span><br><span class="line">  OPTIONS is none or any of:</span><br><span class="line"></span><br><span class="line">buildpaths                       attempt to add class files from build tree</span><br><span class="line">--config dir                     Hadoop config directory</span><br><span class="line">--debug                          turn on shell script debug mode</span><br><span class="line">--help                           usage information</span><br><span class="line">hostnames list[,of,host,names]   hosts to use in slave mode</span><br><span class="line">hosts filename                   list of hosts to use in slave mode</span><br><span class="line">loglevel level                   set the log4j level for this command</span><br><span class="line">workers                          turn on worker mode</span><br><span class="line"></span><br><span class="line">  SUBCOMMAND is one of:</span><br><span class="line">  </span><br><span class="line">    Admin Commands:</span><br><span class="line"></span><br><span class="line">daemonlog     get/set the log level for each daemon</span><br><span class="line"></span><br><span class="line">    Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN applications, not</span><br><span class="line">              this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br><span class="line"></span><br><span class="line">    Daemon Commands:</span><br><span class="line"></span><br><span class="line">kms           run KMS, the Key Management Server</span><br><span class="line"></span><br><span class="line">SUBCOMMAND may print help when invoked w/o parameters or with -h.</span><br></pre></td></tr></table></figure>

<h3 id="6-修改配置文件：伪分布式部署"><a href="#6-修改配置文件：伪分布式部署" class="headerlink" title="6.修改配置文件：伪分布式部署"></a>6.修改配置文件：伪分布式部署</h3><ul>
<li>前置修改： <strong>/etc/host</strong></li>
</ul>
<p>通过命令<code>ifconfig</code>找到本机ip地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 00:0C:29:E2:5A:5E  </span><br><span class="line">          inet addr:XXX.XXX.XXX.XXX//这个ip地址</span><br><span class="line">          inet6 addr: fea2::24c:29fs:fee2:5a7e/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:2742 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:3033 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:267707 (261.4 KiB)  TX bytes:1724144 (1.6 MiB)</span><br></pre></td></tr></table></figure>

<p><code>vi  /etc/host</code>修改域名与ip的对应关系（注意，在后面追加即可，前面的信息不要修改）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat /etc/host</span><br><span class="line">cat: /etc/host: No such file or directory</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">XXX.XXX.XXX.XXX hadoop001</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/core-site.xml</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/core-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>我的域名：hadoop001,所以</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>个人补充：可顺便添加以下代码更改tmp目录地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/hdfs-site.xml</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hdfs-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>个人补充：可顺便添加以下代码（我的域名：hadoop001）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop001:9868&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop001:9869&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>个人补充：</strong></p>
<ul>
<li><strong>etc/hadoop/workers</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/workers</span><br><span class="line">hadoop001</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/hadoop-env.sh</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.sh</span><br><span class="line"># Where pid files are stored.  /tmp by default.</span><br><span class="line"># export HADOOP_PID_DIR=/tmp</span><br><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>

<h3 id="7-设置SSH私钥取消密码"><a href="#7-设置SSH私钥取消密码" class="headerlink" title="7.设置SSH私钥取消密码"></a>7.设置<em>SSH</em>私钥取消密码</h3><p>通过命令<code>ssh localhost</code>检查是否可以用ssh免密连接到localhost</p>
<ul>
<li>成功：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Sat Oct  9 17:05:54 2021 from localhost</span><br></pre></td></tr></table></figure>

<ul>
<li>失败：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">hadoop@localhost&#x27;s password: </span><br><span class="line">Permission denied, please try again.</span><br></pre></td></tr></table></figure>

<p>执行以下命令：<code>ssh-keygen</code>，然后回车两次，若有Overwrite (y/n)?，则输入 y回车</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): </span><br><span class="line">/home/hadoop/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/hadoop/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">9c:1c:53:05:cd:dc:23:1b:51:62:06:b0:92:57:66:da hadoop@hadoop001</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+--[ RSA 2048]----+</span><br><span class="line">|        ..BB*+ . |</span><br><span class="line">|       . O o*.o  |</span><br><span class="line">|      o * E  +.  |</span><br><span class="line">|       = + .     |</span><br><span class="line">|       S         |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">+-----------------+</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll ~/.ssh/</span><br><span class="line">total 16</span><br><span class="line">-rw-------. 1 hadoop hadoop  796 Nov 21 10:34 authorized_keys</span><br><span class="line">-rw-------. 1 hadoop hadoop 1675 Nov 25 17:04 id_rsa</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  398 Nov 25 17:04 id_rsa.pub</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  798 Nov 21 10:29 known_hosts</span><br></pre></td></tr></table></figure>

<p>添加ssh密钥到authorized_keys中,更改权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Thu Nov 25 16:57:28 2021 from localhost</span><br></pre></td></tr></table></figure>

<h2 id="二、执行"><a href="#二、执行" class="headerlink" title="二、执行"></a>二、执行</h2><p>我的系统中环境变量配置了HADOOP_HOME：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br></pre></td></tr></table></figure>

<h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><p>以下介绍为在本地执行一个MapReduce任务：</p>
<h4 id="1-格式化文件系统"><a href="#1-格式化文件系统" class="headerlink" title="1.格式化文件系统"></a>1.格式化文件系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h4 id="2-启动NameNode节点和DataNode节点"><a href="#2-启动NameNode节点和DataNode节点" class="headerlink" title="2.启动NameNode节点和DataNode节点"></a>2.启动NameNode节点和DataNode节点</h4><p>The hadoop daemon log output is written to the <code>$HADOOP_LOG_DIR</code> directory (defaults to <code>$HADOOP_HOME/logs</code>).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br></pre></td></tr></table></figure>

<p>启动后，可以用jps命令查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">6530 Jps</span><br><span class="line">6355 SecondaryNameNode</span><br><span class="line">6197 DataNode</span><br><span class="line">6089 NameNode</span><br></pre></td></tr></table></figure>

<hr>
<p>个人补充：jps后发现DataNode节点丢失，没在运行。原因大概是我格式化太多次namenode导致csid不同步，网上解决办法是data和name文件夹的dfs/data/cruurt/VERSION的id进行同步。最终个人解决方法如下：</p>
<ol>
<li><p>找到自己临时文档/tmp/即core.site.xml文件中的/home/hadoop/data/tmp路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat etc/hadoop/core-site.xml </span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>删除<code>home/hadoop/tmp</code>目录下文件，重新格式化Namenode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ rm -rf tmp/*</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li>
<li><p>重启hdfs，问题解决</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br><span class="line">2021-11-25 18:17:14,032 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">14148 SecondaryNameNode</span><br><span class="line">13991 DataNode</span><br><span class="line">13883 NameNode</span><br><span class="line">14270 Jps</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-通过浏览器访问NameNode"><a href="#3-通过浏览器访问NameNode" class="headerlink" title="3.通过浏览器访问NameNode"></a>3.通过浏览器访问NameNode</h4><ul>
<li>NameNode - <code>http://localhost:9870/</code></li>
<li>hadoop 2.x版本是50070端口，现在版本是9870端口</li>
</ul>
<h4 id="4-在hdfs中创建目录执行MapReduce-jobs"><a href="#4-在hdfs中创建目录执行MapReduce-jobs" class="headerlink" title="4.在hdfs中创建目录执行MapReduce jobs"></a>4.在hdfs中创建目录执行MapReduce jobs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:58 /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user/hadoop</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:59 /user/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="5-复制input文件夹下的文件到hdfs系统中"><a href="#5-复制input文件夹下的文件到hdfs系统中" class="headerlink" title="5.复制input文件夹下的文件到hdfs系统中"></a>5.复制input文件夹下的文件到hdfs系统中</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -put etc/hadoop/*.xml input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 20:05 /user/hadoop/input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/input</span><br><span class="line">Found 9 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       9213 2021-11-25 20:05 /user/hadoop/input/capacity-scheduler.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        975 2021-11-25 20:05 /user/hadoop/input/core-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup      11392 2021-11-25 20:05 /user/hadoop/input/hadoop-policy.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       1068 2021-11-25 20:05 /user/hadoop/input/hdfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        620 2021-11-25 20:05 /user/hadoop/input/httpfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       3518 2021-11-25 20:05 /user/hadoop/input/kms-acls.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        682 2021-11-25 20:05 /user/hadoop/input/kms-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        758 2021-11-25 20:05 /user/hadoop/input/mapred-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        690 2021-11-25 20:05 /user/hadoop/input/yarn-site.xml</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure>

<p>可以看到，在第一句命令中，input的路径并没有写成<code>/user/hadoop/input</code>，因为执行后会在当前用户的路径下执行</p>
<h4 id="6-执行MapReduce任务"><a href="#6-执行MapReduce任务" class="headerlink" title="6.执行MapReduce任务"></a>6.执行MapReduce任务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">2021-11-25 20:10:12,608 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-11-25 20:10:13,702 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: JobTracker metrics system started</span><br><span class="line">2021-11-25 20:10:14,497 INFO input.FileInputFormat: Total input files to process : 9</span><br><span class="line">......</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=232</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=90</span><br></pre></td></tr></table></figure>

<h4 id="7-查看执行结果"><a href="#7-查看执行结果" class="headerlink" title="7.查看执行结果"></a>7.查看执行结果</h4><ul>
<li><p>直接在hdfs系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-11-25 20:10 /user/hadoop/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         90 2021-11-25 20:10 /user/hadoop/output/part-r-00000</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -cat output/*</span><br><span class="line">cat: `output/output&#x27;: No such file or directory</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
<li><p>拿到linux系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -get output output</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat output/*</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="8-停止服务"><a href="#8-停止服务" class="headerlink" title="8.停止服务"></a>8.停止服务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ stop-dfs.sh </span><br><span class="line">Stopping namenodes on [hadoop001]</span><br><span class="line">Stopping datanodes</span><br><span class="line">Stopping secondary namenodes [hadoop001]</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure>


<h3 id="YARN上执行"><a href="#YARN上执行" class="headerlink" title="YARN上执行"></a>YARN上执行</h3><p>想要在YARN上执行MapReduce任务，需要设置参数运行ResourceManager守护进程和NodeManager守护进程。下面执行的基础是已经执行上述<a href="#%E6%9C%AC%E5%9C%B0%E6%89%A7%E8%A1%8C">本地执行1~4</a>的步骤。</p>
<h4 id="1-修改配置文件"><a href="#1-修改配置文件" class="headerlink" title="1.修改配置文件"></a>1.修改配置文件</h4><ul>
<li><p><strong>etc/hadoop/mapred-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li>
<li><p><strong>etc/hadoop/yarn-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>个人：HADOOP_CONF_DIR还没配置</p>
</li>
</ul>
<h4 id="2-启动ResourceManager守护进程和NodeManager守护进程"><a href="#2-启动ResourceManager守护进程和NodeManager守护进程" class="headerlink" title="2.启动ResourceManager守护进程和NodeManager守护进程"></a>2.启动ResourceManager守护进程和NodeManager守护进程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-yarn.sh </span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">20592 Jps</span><br><span class="line">18579 NameNode</span><br><span class="line">20392 ResourceManager</span><br><span class="line">18843 SecondaryNameNode</span><br><span class="line">18686 DataNode</span><br><span class="line">20495 NodeManager</span><br></pre></td></tr></table></figure>

<h4 id="3-通过浏览器访问访问ResourceManager"><a href="#3-通过浏览器访问访问ResourceManager" class="headerlink" title="3.通过浏览器访问访问ResourceManager"></a>3.通过浏览器访问访问ResourceManager</h4><ul>
<li>ResourceManager - <code>http://localhost:8088/</code></li>
</ul>
<h4 id="4-执行MapReduce任务"><a href="#4-执行MapReduce任务" class="headerlink" title="4.执行MapReduce任务"></a>4.执行MapReduce任务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">2021-11-26 08:15:32,639 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-11-26 08:15:34,024 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-11-26 08:15:35,285 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:36,280 INFO input.FileInputFormat: Total input files to process : 9</span><br><span class="line">2021-11-26 08:15:36,379 INFO mapreduce.JobSubmitter: number of splits:9</span><br><span class="line">2021-11-26 08:15:36,974 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:36,976 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2021-11-26 08:15:37,319 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2021-11-26 08:15:37,320 INFO resource.ResourceUtils: Unable to find &#x27;resource-types.xml&#x27;.</span><br><span class="line">2021-11-26 08:15:37,878 INFO impl.YarnClientImpl: Submitted application application_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:37,945 INFO mapreduce.Job: The url to track the job: http://hadoop001:8088/proxy/application_1637885511879_0001/</span><br><span class="line">2021-11-26 08:15:37,945 INFO mapreduce.Job: Running job: job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:55,539 INFO mapreduce.Job: Job job_1637885511879_0001 running in uber mode : false</span><br><span class="line">2021-11-26 08:15:55,550 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2021-11-26 08:16:43,204 INFO mapreduce.Job:  map 44% reduce 0%</span><br><span class="line">2021-11-26 08:16:44,369 INFO mapreduce.Job:  map 67% reduce 0%</span><br><span class="line">2021-11-26 08:17:05,488 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2021-11-26 08:17:06,495 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2021-11-26 08:17:07,508 INFO mapreduce.Job: Job job_1637885511879_0001 completed successfully</span><br><span class="line">2021-11-26 08:17:07,638 INFO mapreduce.Job: Counters: 55</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=128</span><br><span class="line">		FILE: Number of bytes written=2350649</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=29993</span><br><span class="line">		HDFS: Number of bytes written=232</span><br><span class="line">		HDFS: Number of read operations=32</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">		HDFS: Number of bytes read erasure-coded=0</span><br><span class="line">	Job Counters </span><br><span class="line">		Killed map tasks=1</span><br><span class="line">		Launched map tasks=10</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=10</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=333545</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=18937</span><br><span class="line">		Total time spent by all map tasks (ms)=333545</span><br><span class="line">		Total time spent by all reduce tasks (ms)=18937</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=333545</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=18937</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=341550080</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=19391488</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=781</span><br><span class="line">		Map output records=4</span><br><span class="line">		Map output bytes=114</span><br><span class="line">		Map output materialized bytes=176</span><br><span class="line">		Input split bytes=1077</span><br><span class="line">		Combine input records=4</span><br><span class="line">		Combine output records=4</span><br><span class="line">		Reduce input groups=4</span><br><span class="line">		Reduce shuffle bytes=176</span><br><span class="line">		Reduce input records=4</span><br><span class="line">		Reduce output records=4</span><br><span class="line">		Spilled Records=8</span><br><span class="line">		Shuffled Maps =9</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=9</span><br><span class="line">		GC time elapsed (ms)=6070</span><br><span class="line">		CPU time spent (ms)=7920</span><br><span class="line">		Physical memory (bytes) snapshot=1882726400</span><br><span class="line">		Virtual memory (bytes) snapshot=27148435456</span><br><span class="line">		Total committed heap usage (bytes)=1269469184</span><br><span class="line">		Peak Map Physical memory (bytes)=211144704</span><br><span class="line">		Peak Map Virtual memory (bytes)=2715578368</span><br><span class="line">		Peak Reduce Physical memory (bytes)=106315776</span><br><span class="line">		Peak Reduce Virtual memory (bytes)=2720391168</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=28916</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=232</span><br><span class="line">2021-11-26 08:17:07,682 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://hadoop001:9000/user/hadoop/output already exists</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)</span><br><span class="line">	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)</span><br><span class="line">	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1565)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1562)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1562)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1583)</span><br><span class="line">	at org.apache.hadoop.examples.Grep.run(Grep.java:94)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.hadoop.examples.Grep.main(Grep.java:103)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)</span><br><span class="line">	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)</span><br><span class="line">	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/image-20211126093406585.png" alt="通过浏览器查看任务"></p>
<h4 id="5-停止服务"><a href="#5-停止服务" class="headerlink" title="5.停止服务"></a>5.停止服务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ stop-yarn.sh </span><br><span class="line">Stopping nodemanagers</span><br><span class="line">localhost: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9</span><br><span class="line">Stopping resourcemanager</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/11/24/Mysql%E9%83%A8%E7%BD%B2/" rel="prev" title="在Linux系统上部署Mysql">
                  <i class="fa fa-chevron-left"></i> 在Linux系统上部署Mysql
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="next" title="Hadoop基础知识">
                  Hadoop基础知识 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k12</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





</body>
</html>
