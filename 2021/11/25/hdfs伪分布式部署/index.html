<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"k12coding.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="hdfs伪分布式部署Releases Archive中选择要部署的版本，我们以Release 3.2.2 available版本为例 参考文档：Hadoop: Setting up a Single Node Cluster. 1.软件要求 Java：Hadoop对Java版本有要求，具体参考Hadoop Java Versions，基本上Java8通用 ssh  补充：组件名称大写-数字，如：S">
<meta property="og:type" content="article">
<meta property="og:title" content="hdfs伪分布式部署">
<meta property="og:url" content="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="hdfs伪分布式部署Releases Archive中选择要部署的版本，我们以Release 3.2.2 available版本为例 参考文档：Hadoop: Setting up a Single Node Cluster. 1.软件要求 Java：Hadoop对Java版本有要求，具体参考Hadoop Java Versions，基本上Java8通用 ssh  补充：组件名称大写-数字，如：S">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-11-25T05:09:16.000Z">
<meta property="article:modified_time" content="2021-11-25T13:07:56.172Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/","path":"2021/11/25/hdfs伪分布式部署/","title":"hdfs伪分布式部署"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>hdfs伪分布式部署 | k12的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">k12的博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">k12的笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="nav-text">hdfs伪分布式部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%BD%AF%E4%BB%B6%E8%A6%81%E6%B1%82"><span class="nav-text">1.软件要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-tar%E5%8C%85%E8%A7%A3%E5%8E%8B"><span class="nav-text">2.tar包解压</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95"><span class="nav-text">3.查看文件目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AEJava%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%88%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="nav-text">4.手动配置Java环境变量（必须）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%89%A7%E8%A1%8Cbin-hadoop%E6%9F%A5%E7%9C%8Bhdfs%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-text">5.执行bin&#x2F;hadoop查看hdfs使用说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="nav-text">6.修改配置文件：伪分布式部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%AE%BE%E7%BD%AESSH%E7%A7%81%E9%92%A5%E5%8F%96%E6%B6%88%E5%AF%86%E7%A0%81"><span class="nav-text">7.设置SSH私钥取消密码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E6%89%A7%E8%A1%8C"><span class="nav-text">8.执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-text">格式化文件系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8NameNode%E8%8A%82%E7%82%B9%E5%92%8CDataNode%E8%8A%82%E7%82%B9"><span class="nav-text">启动NameNode节点和DataNode节点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AENameNode"><span class="nav-text">通过浏览器访问NameNode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8hdfs%E4%B8%AD%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95%E6%89%A7%E8%A1%8CMapReduce-jobs"><span class="nav-text">在hdfs中创建目录执行MapReduce jobs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%8D%E5%88%B6input%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%B0hdfs%E7%B3%BB%E7%BB%9F%E4%B8%AD"><span class="nav-text">复制input文件夹下的文件到hdfs系统中</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A7%E8%A1%8Cjar%E5%8C%85"><span class="nav-text">执行jar包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="nav-text">查看执行结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2%E6%9C%8D%E5%8A%A1"><span class="nav-text">停止服务</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">k12</p>
  <div class="site-description" itemprop="description">2</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hdfs伪分布式部署
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-11-25 13:09:16 / 修改时间：21:07:56" itemprop="dateCreated datePublished" datetime="2021-11-25T13:09:16+08:00">2021-11-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="hdfs伪分布式部署"><a href="#hdfs伪分布式部署" class="headerlink" title="hdfs伪分布式部署"></a>hdfs伪分布式部署</h1><p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/release.html">Releases Archive</a>中选择要部署的版本，我们以<a target="_blank" rel="noopener" href="https://hadoop.apache.org/release/3.2.2.html">Release 3.2.2 available</a>版本为例</p>
<p>参考文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop: Setting up a Single Node Cluster.</a></p>
<h3 id="1-软件要求"><a href="#1-软件要求" class="headerlink" title="1.软件要求"></a>1.软件要求</h3><ul>
<li>Java：Hadoop对Java版本有要求，具体参考<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions">Hadoop Java Versions</a>，基本上Java8通用</li>
<li>ssh</li>
</ul>
<p>补充：组件名称<code>大写-数字</code>，如：SPARK-2908，表明该组件是有问题的</p>
<span id="more"></span>

<h3 id="2-tar包解压"><a href="#2-tar包解压" class="headerlink" title="2.tar包解压"></a>2.tar包解压</h3><p>点击下载:<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz">hadoop-3.2.2.tar.gz</a></p>
<p>下载后通过rz命令上传至Linux系统</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ ll software/</span><br><span class="line">total 805204</span><br><span class="line">-rw-r--r--.  1 hadoop hadoop 395448622 Nov 21 10:03 hadoop-3.2.2.tar.gz</span><br></pre></td></tr></table></figure>

<p>解压hadoop到app目录下，创建软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ tar -xzvf software/hadoop-3.2.2.tar.gz -C app/</span><br><span class="line">[hadoop@hadoop001 ~]$ cd app</span><br><span class="line">[hadoop@hadoop001 app]$ </span><br><span class="line">[hadoop@hadoop001 app]$ ln -s /home/hadoop/app/hadoop-3.2.2 hadoop</span><br><span class="line">[hadoop@hadoop001 app]$ ll</span><br><span class="line">total 2</span><br><span class="line">lrwxrwxrwx. 1 hadoop hadoop   29 Nov 25 16:28 hadoop -&gt; /home/hadoop/app/hadoop-3.2.2</span><br><span class="line">drwxr-xr-x. 9 hadoop hadoop 4096 Jan  3  2021 hadoop-3.2.2</span><br></pre></td></tr></table></figure>

<h3 id="3-查看文件目录"><a href="#3-查看文件目录" class="headerlink" title="3.查看文件目录"></a>3.查看文件目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 app]$ cd hadoop/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll</span><br><span class="line">total 216</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 bin				#命令执行脚本</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 etc				#配置文件</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 include</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:14 input</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 lib</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 libexec</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 150569 Dec  5  2020 LICENSE.txt</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:48 logs</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  21943 Dec  5  2020 NOTICE.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Nov 21 11:01 output</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop   1361 Dec  5  2020 README.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 sbin			#启动停止脚本</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 share</span><br></pre></td></tr></table></figure>

<p>大部分的大数据项目解压后目录：bin</p>
<h3 id="4-手动配置Java环境变量（必须）"><a href="#4-手动配置Java环境变量（必须）" class="headerlink" title="4.手动配置Java环境变量（必须）"></a>4.手动配置Java环境变量（必须）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use. By default, this environment</span><br><span class="line"># variable is REQUIRED on ALL platforms except OS X!</span><br><span class="line"># export JAVA_HOME=</span><br><span class="line">export export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure>

<h3 id="5-执行bin-hadoop查看hdfs使用说明"><a href="#5-执行bin-hadoop查看hdfs使用说明" class="headerlink" title="5.执行bin/hadoop查看hdfs使用说明"></a>5.执行bin/hadoop查看hdfs使用说明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ bin/hadoop</span><br><span class="line">Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]</span><br><span class="line"> or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]</span><br><span class="line">  where CLASSNAME is a user-provided Java class</span><br><span class="line"></span><br><span class="line">  OPTIONS is none or any of:</span><br><span class="line"></span><br><span class="line">buildpaths                       attempt to add class files from build tree</span><br><span class="line">--config dir                     Hadoop config directory</span><br><span class="line">--debug                          turn on shell script debug mode</span><br><span class="line">--help                           usage information</span><br><span class="line">hostnames list[,of,host,names]   hosts to use in slave mode</span><br><span class="line">hosts filename                   list of hosts to use in slave mode</span><br><span class="line">loglevel level                   set the log4j level for this command</span><br><span class="line">workers                          turn on worker mode</span><br><span class="line"></span><br><span class="line">  SUBCOMMAND is one of:</span><br><span class="line">  </span><br><span class="line">    Admin Commands:</span><br><span class="line"></span><br><span class="line">daemonlog     get/set the log level for each daemon</span><br><span class="line"></span><br><span class="line">    Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN applications, not</span><br><span class="line">              this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br><span class="line"></span><br><span class="line">    Daemon Commands:</span><br><span class="line"></span><br><span class="line">kms           run KMS, the Key Management Server</span><br><span class="line"></span><br><span class="line">SUBCOMMAND may print help when invoked w/o parameters or with -h.</span><br></pre></td></tr></table></figure>

<h3 id="6-修改配置文件：伪分布式部署"><a href="#6-修改配置文件：伪分布式部署" class="headerlink" title="6.修改配置文件：伪分布式部署"></a>6.修改配置文件：伪分布式部署</h3><p><strong>etc/hadoop/core-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/core-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>个人补充：可顺便添加以下代码更改tmp目录地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>etc/hadoop/hdfs-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hdfs-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h3 id="7-设置SSH私钥取消密码"><a href="#7-设置SSH私钥取消密码" class="headerlink" title="7.设置SSH私钥取消密码"></a>7.设置<em>SSH</em>私钥取消密码</h3><p>通过命令<code>ssh localhost</code>检查是否可以用ssh免密连接到localhost</p>
<ul>
<li>成功：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Sat Oct  9 17:05:54 2021 from localhost</span><br></pre></td></tr></table></figure>

<ul>
<li>失败：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">hadoop@localhost&#x27;s password: </span><br><span class="line">Permission denied, please try again.</span><br></pre></td></tr></table></figure>

<p>执行以下命令：<code>ssh-keygen</code>，然后回车两次，若有Overwrite (y/n)?，则输入 y回车</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): </span><br><span class="line">/home/hadoop/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/hadoop/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">9c:1c:53:05:cd:dc:23:1b:51:62:06:b0:92:57:66:da hadoop@hadoop001</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+--[ RSA 2048]----+</span><br><span class="line">|        ..BB*+ . |</span><br><span class="line">|       . O o*.o  |</span><br><span class="line">|      o * E  +.  |</span><br><span class="line">|       = + .     |</span><br><span class="line">|       S         |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">+-----------------+</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll ~/.ssh/</span><br><span class="line">total 16</span><br><span class="line">-rw-------. 1 hadoop hadoop  796 Nov 21 10:34 authorized_keys</span><br><span class="line">-rw-------. 1 hadoop hadoop 1675 Nov 25 17:04 id_rsa</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  398 Nov 25 17:04 id_rsa.pub</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  798 Nov 21 10:29 known_hosts</span><br></pre></td></tr></table></figure>

<p>添加ssh密钥到authorized_keys中,更改权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Thu Nov 25 16:57:28 2021 from localhost</span><br></pre></td></tr></table></figure>

<h3 id="8-执行"><a href="#8-执行" class="headerlink" title="8.执行"></a>8.执行</h3><p>以下介绍为在本地执行一个MapReduce任务，我的系统中环境变量配置了HADOOP_HOME：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br></pre></td></tr></table></figure>

<ol>
<li><h4 id="格式化文件系统"><a href="#格式化文件系统" class="headerlink" title="格式化文件系统"></a>格式化文件系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li>
<li><h4 id="启动NameNode节点和DataNode节点"><a href="#启动NameNode节点和DataNode节点" class="headerlink" title="启动NameNode节点和DataNode节点"></a>启动NameNode节点和DataNode节点</h4><p>The hadoop daemon log output is written to the <code>$HADOOP_LOG_DIR</code> directory (defaults to <code>$HADOOP_HOME/logs</code>).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br></pre></td></tr></table></figure>

<p>启动后，可以用jps命令查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">6530 Jps</span><br><span class="line">6355 SecondaryNameNode</span><br><span class="line">6197 DataNode</span><br><span class="line">6089 NameNode</span><br></pre></td></tr></table></figure>

<hr>
<p>个人补充：jps后发现DataNode节点丢失，没在运行。原因大概是我格式化太多次namenode导致csid不同步，网上解决办法是data和name文件夹的dfs/data/cruurt/VERSION的id进行同步。最终个人解决方法如下：</p>
<ol>
<li><p>找到自己临时文档/tmp/即core.site.xml文件中的/home/hadoop/data/tmp路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat etc/hadoop/core-site.xml </span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>删除<code>home/hadoop/tmp</code>目录下文件，重新格式化Namenode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ rm -rf tmp/*</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li>
<li><p>重启hdfs，问题解决</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br><span class="line">2021-11-25 18:17:14,032 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">14148 SecondaryNameNode</span><br><span class="line">13991 DataNode</span><br><span class="line">13883 NameNode</span><br><span class="line">14270 Jps</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><h4 id="通过浏览器访问NameNode"><a href="#通过浏览器访问NameNode" class="headerlink" title="通过浏览器访问NameNode"></a>通过浏览器访问NameNode</h4><ul>
<li>NameNode - <code>http://localhost:9870/</code></li>
<li>hadoop 2.x版本是50070端口，现在版本是9870端口</li>
</ul>
</li>
<li><h4 id="在hdfs中创建目录执行MapReduce-jobs"><a href="#在hdfs中创建目录执行MapReduce-jobs" class="headerlink" title="在hdfs中创建目录执行MapReduce jobs"></a>在hdfs中创建目录执行MapReduce jobs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:58 /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user/hadoop</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:59 /user/hadoop</span><br></pre></td></tr></table></figure></li>
<li><h4 id="复制input文件夹下的文件到hdfs系统中"><a href="#复制input文件夹下的文件到hdfs系统中" class="headerlink" title="复制input文件夹下的文件到hdfs系统中"></a>复制input文件夹下的文件到hdfs系统中</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -put etc/hadoop/*.xml input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 20:05 /user/hadoop/input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/input</span><br><span class="line">Found 9 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       9213 2021-11-25 20:05 /user/hadoop/input/capacity-scheduler.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        975 2021-11-25 20:05 /user/hadoop/input/core-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup      11392 2021-11-25 20:05 /user/hadoop/input/hadoop-policy.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       1068 2021-11-25 20:05 /user/hadoop/input/hdfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        620 2021-11-25 20:05 /user/hadoop/input/httpfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       3518 2021-11-25 20:05 /user/hadoop/input/kms-acls.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        682 2021-11-25 20:05 /user/hadoop/input/kms-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        758 2021-11-25 20:05 /user/hadoop/input/mapred-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        690 2021-11-25 20:05 /user/hadoop/input/yarn-site.xml</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure>

<p>可以看到，在第一句命令中，input的路径并没有写成<code>/user/hadoop/input</code>，因为执行后会在当前用户的路径下执行</p>
</li>
<li><h4 id="执行jar包"><a href="#执行jar包" class="headerlink" title="执行jar包"></a>执行jar包</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">2021-11-25 20:10:12,608 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-11-25 20:10:13,702 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: JobTracker metrics system started</span><br><span class="line">2021-11-25 20:10:14,497 INFO input.FileInputFormat: Total input files to process : 9</span><br><span class="line">......</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=232</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=90</span><br></pre></td></tr></table></figure></li>
<li><h4 id="查看执行结果"><a href="#查看执行结果" class="headerlink" title="查看执行结果"></a>查看执行结果</h4><ul>
<li><p>直接在hdfs系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-11-25 20:10 /user/hadoop/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         90 2021-11-25 20:10 /user/hadoop/output/part-r-00000</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -cat output/*</span><br><span class="line">cat: `output/output&#x27;: No such file or directory</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
<li><p>拿到linux系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -get output output</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat output/*</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><h4 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ stop-dfs.sh </span><br><span class="line">Stopping namenodes on [hadoop001]</span><br><span class="line">Stopping datanodes</span><br><span class="line">Stopping secondary namenodes [hadoop001]</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/11/24/Mysql%E9%83%A8%E7%BD%B2/" rel="prev" title="在Linux系统上部署Mysql">
                  <i class="fa fa-chevron-left"></i> 在Linux系统上部署Mysql
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="next" title="Hadoop基础知识">
                  Hadoop基础知识 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k12</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





</body>
</html>
