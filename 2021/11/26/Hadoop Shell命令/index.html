<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hadoop Shell命令 | k12的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Hadoop Shell命令FS Shell调用文件系统(FS)Shell命令应使用 bin&#x2F;hadoop fs &lt;args&gt;的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme:&#x2F;&#x2F;authority&#x2F;path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop Shell命令">
<meta property="og:url" content="https://k12coding.github.io/2021/11/26/Hadoop%20Shell%E5%91%BD%E4%BB%A4/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="Hadoop Shell命令FS Shell调用文件系统(FS)Shell命令应使用 bin&#x2F;hadoop fs &lt;args&gt;的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme:&#x2F;&#x2F;authority&#x2F;path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-11-26T05:24:25.000Z">
<meta property="article:modified_time" content="2021-12-03T00:15:28.613Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="k12的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">k12的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">k12的笔记</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <!--
      <nav id="sub-nav">
        
          此处隐藏rss,注释掉 <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a> -->
        
        <!--此处隐藏,注释掉 <a id="nav-search-btn" class="nav-icon" title="搜索"></a> 
      </nav>
      -->
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://k12coding.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Hadoop Shell命令" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/11/26/Hadoop%20Shell%E5%91%BD%E4%BB%A4/" class="article-date">
  <time class="dt-published" datetime="2021-11-26T05:24:25.000Z" itemprop="datePublished">2021-11-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Hadoop Shell命令
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop-Shell命令"><a href="#Hadoop-Shell命令" class="headerlink" title="Hadoop Shell命令"></a>Hadoop Shell命令</h1><h2 id="FS-Shell"><a href="#FS-Shell" class="headerlink" title="FS Shell"></a>FS Shell</h2><p>调用文件系统(FS)Shell命令应使用 bin/hadoop fs &lt;args&gt;的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是<em>scheme://authority/path</em>。对HDFS文件系统，scheme是<em>hdfs</em>，对本地文件系统，scheme是<em>file</em>。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如*/parent/child<em>可以表示成</em>hdfs://namenode:namenodeport/parent/child<em>，或者更简单的</em>/parent/child<em>（假设你配置文件中的默认值是</em>namenode:namenodeport<em>）。大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到</em>stderr<em>，其他信息输出到</em>stdout*。</p>
<span id="more"></span>

<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>使用方法：<code>hadoop fs -cat URI [URI …]</code></p>
<p>将路径指定文件的内容输出到<em>stdout</em>。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2</code></li>
<li><code>hadoop fs -cat file:///file3 /user/hadoop/file4</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h3><p>使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] </p>
<p>改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>使用方法：<code>hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI …]</code></p>
<p>改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>使用方法：<code>hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</code></p>
<p>改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h3><p>使用方法：<code>hadoop fs -copyFromLocal &lt;localsrc&gt; URI</code></p>
<p>除了限定源路径是一个本地文件外，和<a href="#put"><strong>put</strong></a>命令相似。</p>
<h3 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a>copyToLocal</h3><p>使用方法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</code></p>
<p>除了限定目标路径是一个本地文件外，和<a href="#get"><strong>get</strong></a>命令类似。</p>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>使用方法：<code>hadoop fs -cp URI [URI …] &lt;dest&gt;</code></p>
<p>将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>示例：</p>
<ul>
<li><code>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2</code></li>
<li><code>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="du"><a href="#du" class="headerlink" title="du"></a>du</h3><p>使用方法：<code>hadoop fs -du URI [URI …]</code></p>
<p>显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：<br><code>hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1</code><br>返回值：成功返回0，失败返回-1。</p>
<h3 id="dus"><a href="#dus" class="headerlink" title="dus"></a>dus</h3><p>使用方法：<code>hadoop fs -dus &lt;args&gt;</code></p>
<p>显示文件的大小。</p>
<h3 id="expunge"><a href="#expunge" class="headerlink" title="expunge"></a>expunge</h3><p>使用方法：<code>hadoop fs -expunge</code></p>
<p>清空回收站。请参考 <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS Architecture guide</a> 文档以获取更多关于回收站特性的信息。</p>
<h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><p>使用方法：<code>hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;</code></p>
<p>复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -get /user/hadoop/file localfile</code></li>
<li><code>hadoop fs -get hdfs://host:port/user/hadoop/file localfile</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="getmerge"><a href="#getmerge" class="headerlink" title="getmerge"></a>getmerge</h3><p>使用方法：<code>hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</code></p>
<p>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</p>
<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>使用方法：<code>hadoop fs -ls &lt;args&gt;</code></p>
<p>如果是文件，则按照如下格式返回文件信息：<br>文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>目录名 &lt;dir&gt; 修改日期 修改时间 权限 用户ID 组ID<br>示例：</p>
<ul>
<li><code>hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="lsr"><a href="#lsr" class="headerlink" title="lsr"></a>lsr</h3><p>使用方法：<code>hadoop fs -lsr &lt;args&gt;</code><br>ls命令的递归版本。类似于Unix中的ls -R。</p>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>使用方法：<code>hadoop fs -mkdir &lt;paths&gt;</code></p>
<p>接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2</code></li>
<li><code>hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="movefromLocal"><a href="#movefromLocal" class="headerlink" title="movefromLocal"></a>movefromLocal</h3><p>使用方法：<code>dfs -moveFromLocal &lt;src&gt; &lt;dst&gt;</code></p>
<p>输出一个”not implemented“信息。</p>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>使用方法：<code>hadoop fs -mv URI [URI …] &lt;dest&gt;</code></p>
<p>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p>
<ul>
<li><code>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</code></li>
<li><code>hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><p>使用方法：<code>hadoop fs -put &lt;localsrc&gt; ... &lt;dst&gt;</code></p>
<p>从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p>
<ul>
<li><code>hadoop fs -put localfile /user/hadoop/hadoopfile</code></li>
<li><code>hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</code></li>
<li><code>hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile</code></li>
<li><code>hadoop fs -put - hdfs://host:port/hadoop/hadoopfile</code><br>从标准输入中读取输入。</li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>使用方法：<code>hadoop fs -rm URI [URI …]</code></p>
<p>删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p>
<ul>
<li><code>hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h3><p>使用方法：<code>hadoop fs -rmr URI [URI …]</code></p>
<p>delete的递归版本。<br>示例：</p>
<ul>
<li><code>hadoop fs -rmr /user/hadoop/dir</code></li>
<li><code>hadoop fs -rmr hdfs://host:port/user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a>setrep</h3><p>使用方法：<code>hadoop fs -setrep [-R] &lt;path&gt;</code></p>
<p>改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h3><p>使用方法：<code>hadoop fs -stat URI [URI …]</code></p>
<p>返回指定路径的统计信息。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -stat path</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>使用方法：<code>hadoop fs -tail [-f] URI</code></p>
<p>将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -tail pathname</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>使用方法：<code>hadoop fs -test -[ezd] URI</code></p>
<p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -test -e filename</code></li>
</ul>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><p>使用方法：<code>hadoop fs -text &lt;src&gt;</code></p>
<p>将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p>
<h3 id="touchz"><a href="#touchz" class="headerlink" title="touchz"></a>touchz</h3><p>使用方法：<code>hadoop fs -touchz URI [URI …]</code></p>
<p>创建一个0字节的空文件。</p>
<p>示例：</p>
<ul>
<li><code>hadoop -touchz pathname</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h2 id="hdfs-其他命令"><a href="#hdfs-其他命令" class="headerlink" title="hdfs 其他命令"></a>hdfs 其他命令</h2><ul>
<li><p>安全模式</p>
<p>hdfs dfsadmin     [-safemode &lt;enter | leave | get | wait&gt;]</p>
<p>安全模式关闭：读写正常</p>
<p>log看到safemode:on，必然是集群有问题的，可以手动退出，就能正常对外提供服务</p>
<p>启动安全模式：<code>hdfs dfsadmin -safemode enter</code></p>
<p>启动后可读不可写</p>
</li>
<li><p>hdfs fsck /</p>
<p>检查系统问题</p>
</li>
<li><p>集群平衡</p>
<p>dfs.disk.balancer.enabled:  true</p>
<p>执行命令：先生成计划再执行</p>
<p><code>hdfs balancer</code> </p>
<p>DN1 DN2节点和节点之前的平衡 2.X</p>
<p><code>hdfs diskbalancer</code></p>
<p>单个节点多盘的平衡 3.X</p>
</li>
<li><p>回收站</p>
<p>linux有回收站吗？没有。要做，怎么办？：</p>
<ul>
<li><p>狸猫换太子</p>
<p>写脚本封装</p>
</li>
</ul>
<p><code>etc/hadoop/core-default.xml</code></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>fs.trash.interval</td>
<td>0</td>
<td>Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.</td>
</tr>
</tbody></table>
</li>
</ul>
<p>hadoop命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN applications, not</span><br><span class="line">              this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br></pre></td></tr></table></figure>

<p>查看当前版本压缩情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop checknative</span><br><span class="line">2021-11-28 14:06:21,011 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  false </span><br><span class="line">zlib:    false </span><br><span class="line">zstd  :  false </span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     false </span><br><span class="line">bzip2:   false </span><br><span class="line">openssl: false </span><br><span class="line">ISA-L:   false </span><br><span class="line">PMDK:    false </span><br><span class="line">2021-11-28 14:06:21,370 INFO util.ExitUtil: Exiting with status 1: ExitException</span><br><span class="line">[hadoop@hadoop001 ~]$ </span><br></pre></td></tr></table></figure>

<p>打印类的路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop classpath</span><br><span class="line">/home/hadoop/app/hadoop/etc/hadoop:/home/hadoop/app/hadoop/share/hadoop/common/lib/*:/home/hadoop/app/hadoop/share/hadoop/common/*:/home/hadoop/app/hadoop/share/hadoop/hdfs:/home/hadoop/app/hadoop/share/hadoop/hdfs/lib/*:/home/hadoop/app/hadoop/share/hadoop/hdfs/*:/home/hadoop/app/hadoop/share/hadoop/mapreduce/lib/*:/home/hadoop/app/hadoop/share/hadoop/mapreduce/*:/home/hadoop/app/hadoop/share/hadoop/yarn:/home/hadoop/app/hadoop/share/hadoop/yarn/lib/*:/home/hadoop/app/hadoop/share/hadoop/yarn/*</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2021/11/26/Hadoop%20Shell%E5%91%BD%E4%BB%A4/" data-id="cl3b9tuac000bakudchuf20sv" data-title="Hadoop Shell命令" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/11/29/SQL%E7%BB%83%E4%B9%A0%E9%A2%98/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          SQL练习题
        
      </div>
    </a>
  
  
    <a href="/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Hadoop基础知识</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/">Flume的使用案例</a>
          </li>
        
          <li>
            <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process</a>
          </li>
        
          <li>
            <a href="/2022/05/16/Flume%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/">Flume介绍与使用</a>
          </li>
        
          <li>
            <a href="/2022/05/13/Kafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%82%A3%E4%B9%88%E5%BF%AB/">Kafka为什么能那么快</a>
          </li>
        
          <li>
            <a href="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/">我的mysql笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 k12<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>