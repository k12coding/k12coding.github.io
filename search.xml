<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HDFS API</title>
    <url>/2021/12/13/HDFS-API/</url>
    <content><![CDATA[<h2 id="HDFS-API编程"><a href="#HDFS-API编程" class="headerlink" title="HDFS API编程"></a>HDFS API编程</h2><p><strong>FileSystem：编程的入口点</strong></p>
<span id="more"></span>

<h3 id="一、添加依赖和导入package"><a href="#一、添加依赖和导入package" class="headerlink" title="一、添加依赖和导入package"></a>一、添加依赖和导入package</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.2.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>导入需要的package</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br></pre></td></tr></table></figure>

<p>类中具体方法可参考：</p>
<p><a href="https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/FileSystem.html">https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/FileSystem.html</a></p>
<p><a href="https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/conf/Configurable.html">https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/conf/Configurable.html</a></p>
<p><a href="https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/Path.html">https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/Path.html</a></p>
<h3 id="二、获取hdfs的FileSystem对象"><a href="#二、获取hdfs的FileSystem对象" class="headerlink" title="二、获取hdfs的FileSystem对象"></a>二、获取hdfs的FileSystem对象</h3><p>Hadoop中关于文件操作类基本上全部是在”<strong>org.apache.hadoop.fs</strong>“包中，这些API能够支持的操作包含：打开文件，读写文件，删除文件等。</p>
<p>Hadoop类库中最终面向用户提供的<strong>接口类</strong>是<strong>FileSystem</strong>，该类是个<strong>抽象类</strong>，只能通过来类的get方法得到具体类。get方法存在几个重载版本，常用的是这个：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static FileSystem get(Configuration conf) throws IOException &#123;</span><br><span class="line">    return get(getDefaultUri(conf), conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>获取Configuration对象</p>
<p>我们需要先new一个Configuration对象</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Configuration config = new Configuration();//获取的是hadoop默认配置文件</span><br></pre></td></tr></table></figure>

<p>（生产上一般不需要额外设置）如果需要设置，则调用Configuration对象的set方法，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hostname:9000&quot;);</span><br><span class="line">config.set(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;);//还要到hdfs-site.xml里添加dfs.datanode.use.datanode.hostname:true</span><br><span class="line">config.set(&quot;dfs.replication&quot;, &quot;1&quot;);//不设置的话，默认副本数是3</span><br><span class="line">//系统更改hadoop用户名称</span><br><span class="line">//System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br></pre></td></tr></table></figure></li>
<li><p>获取FileSystem对象</p>
<p>把Configuration对象conf传给FileSystem类的get()方法获得FileSystem类对象hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FileSystem hdfs = FileSystem.get(config);</span><br></pre></td></tr></table></figure></li>
<li><p>进行文件操作</p>
<p>操作过程中有关路径的需要使用<code>org.apache.hadoop.fs.Path</code>类，常用的是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path src = new Path(&quot;pathString&quot;);</span><br></pre></td></tr></table></figure></li>
<li><p>释放资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(null != hdfs) &#123;</span><br><span class="line">    hdfs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="三、利用api进行操作"><a href="#三、利用api进行操作" class="headerlink" title="三、利用api进行操作"></a>三、利用api进行操作</h3><h4 id="获得fs对象hdfs"><a href="#获得fs对象hdfs" class="headerlink" title="获得fs对象hdfs"></a>获得fs对象hdfs</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Configuration config = new Configuration();</span><br><span class="line">FileSystem hdfs = FileSystem.get(config);</span><br></pre></td></tr></table></figure>

<h4 id="mkdir：创建目录"><a href="#mkdir：创建目录" class="headerlink" title="mkdir：创建目录"></a>mkdir：创建目录</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path path = new Path(&quot;/pathString&quot;);</span><br><span class="line">hdfs.mkdir(path)</span><br></pre></td></tr></table></figure>

<h4 id="copyFromLocalFile：从本地复制文件到hdfs"><a href="#copyFromLocalFile：从本地复制文件到hdfs" class="headerlink" title="copyFromLocalFile：从本地复制文件到hdfs"></a>copyFromLocalFile：从本地复制文件到hdfs</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path src = new Path(&quot;srcFile&quot;);</span><br><span class="line">Path dst = new Path(&quot;dstFile&quot;);</span><br><span class="line">hdfs.copyFromLocalFile(src, dst);</span><br></pre></td></tr></table></figure>

<h4 id="copyToLocalFile：从hdfs复制文件到本地"><a href="#copyToLocalFile：从hdfs复制文件到本地" class="headerlink" title="copyToLocalFile：从hdfs复制文件到本地"></a>copyToLocalFile：从hdfs复制文件到本地</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path src = new Path(&quot;srcFile&quot;);</span><br><span class="line">Path dst = new Path(&quot;dstFile&quot;);</span><br><span class="line">hdfs.copyToLocalFile(src, dst);</span><br><span class="line">//hdfs.copyToLocalFile(true, src, dst);//true:delSrc;一般不用</span><br></pre></td></tr></table></figure>

<h4 id="rename：移动文件"><a href="#rename：移动文件" class="headerlink" title="rename：移动文件"></a>rename：移动文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path src = new Path(&quot;path1&quot;);</span><br><span class="line">Path dst = new Path(&quot;path2&quot;);</span><br><span class="line">fileSystem.rename(src, dst);</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>Deprecated.</strong> </p>
<p>Renames Path src to Path dst</p>
<ul>
<li>Fails if src is a file and dst is a directory.</li>
<li>Fails if src is a directory and dst is a file.</li>
<li>Fails if the parent of dst does not exist or is a file.</li>
</ul>
<p>If OVERWRITE option is not passed as an argument, rename fails if the dst already exists.</p>
<p>If OVERWRITE option is passed as an argument, rename overwrites the dst if it is a file or an empty directory. Rename fails if dst is a non-empty directory.</p>
<p>Note that atomicity of rename is dependent on the file system implementation. Please refer to the file system documentation for details. This default implementation is non atomic.</p>
<p>This method is deprecated since it is a temporary method added to support the transition from FileSystem to FileContext for user applications.</p>
<ul>
<li><strong>Parameters:</strong></li>
</ul>
<p> <code>src</code> - path to be renamed</p>
<p> <code>dst</code> - new path after rename</p>
<ul>
<li><strong>Throws:</strong></li>
</ul>
<p> <code>FileNotFoundException</code> - src path does not exist, or the parent path of dst does not exist.</p>
<p> <code>FileAlreadyExistsException</code> - dest path exists and is a file</p>
<p> <code>ParentNotDirectoryException</code> - if the parent path of dest is not a directory</p>
<p> <code>IOException</code> - on failure</p>
</blockquote>
<h4 id="listFiles：文件列表"><a href="#listFiles：文件列表" class="headerlink" title="listFiles：文件列表"></a>listFiles：文件列表</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(new Path(&quot;/user/hadoop&quot;), true);</span><br><span class="line"></span><br><span class="line">while(files.hasNext()) &#123;</span><br><span class="line">    LocatedFileStatus fileStatus = files.next();</span><br><span class="line">    String isDir = fileStatus.isDirectory() ? &quot;d&quot; : &quot;-&quot;;</span><br><span class="line">    String permission = fileStatus.getPermission().toString();</span><br><span class="line">    short replication = fileStatus.getReplication();</span><br><span class="line">    long len = fileStatus.getLen();</span><br><span class="line">    String path = fileStatus.getPath().toString();</span><br><span class="line"></span><br><span class="line">    System.out.println(isDir + permission + &quot;\t&quot; + replication + &quot;\t&quot; + len + &quot;\t&quot; + path);</span><br><span class="line"></span><br><span class="line">    BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">    //for(BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">    //    String[] hosts = blockLocation.getHosts();</span><br><span class="line">    //    for(String host: hosts) &#123;</span><br><span class="line">    //        System.out.println(host);</span><br><span class="line">    //    &#125;</span><br><span class="line">    //&#125;</span><br><span class="line">    int blockLen = blockLocations.length;</span><br><span class="line">	for(int i=0;i&lt;blockLen;i++)&#123;</span><br><span class="line">		String[] hosts = blockLocations[i].getHosts();</span><br><span class="line">		System.out.println(&quot;block_&quot;+i+&quot;_location:&quot;+hosts[0]);</span><br><span class="line">	&#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="delete：删除文件"><a href="#delete：删除文件" class="headerlink" title="delete：删除文件"></a>delete：删除文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path path = new Path(&quot;deleteFilePath&quot;);</span><br><span class="line">fileSystem.delete(path,false);</span><br><span class="line">//fileSystem.delete(new Path(&quot;deleteFilePath&quot;),true);//true:递归删除</span><br></pre></td></tr></table></figure>

<h4 id="exists：查看文件是否存在"><a href="#exists：查看文件是否存在" class="headerlink" title="exists：查看文件是否存在"></a>exists：查看文件是否存在</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path findFile = new Path(&quot;filePath&quot;);</span><br><span class="line">boolean isExists = hdfs.exists(findFile);</span><br></pre></td></tr></table></figure>

<h4 id="FileStatus-查看HDFS文件的最后修改时间"><a href="#FileStatus-查看HDFS文件的最后修改时间" class="headerlink" title="FileStatus:查看HDFS文件的最后修改时间"></a>FileStatus:查看HDFS文件的最后修改时间</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Path path = new Path(&quot;fileName&quot;);</span><br><span class="line">FileStatus fileStatus = hdfs.getFileStatus(path);</span><br><span class="line">long modificationTime = fileStatus.getModificationTime</span><br></pre></td></tr></table></figure>

<h4 id="其他：用读写IO拷贝文件"><a href="#其他：用读写IO拷贝文件" class="headerlink" title="其他：用读写IO拷贝文件"></a>其他：用读写IO拷贝文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.*;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line">import java.io.BufferedInputStream;</span><br><span class="line">import java.io.File;</span><br><span class="line">import java.io.FileInputStream;</span><br><span class="line">import java.io.FileOutputStream;</span><br></pre></td></tr></table></figure>

<ul>
<li>从本地文件拷贝到服务器上去  put<br>读本地文件(读io)  写到服务器上去(写io)</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BufferedInputStream in = new BufferedInputStream(new FileInputStream(new File(&quot;data/wc.data&quot;)));</span><br><span class="line">FSDataOutputStream out = fileSystem.create(new Path(&quot;/input/wc-io.txt&quot;));</span><br><span class="line"></span><br><span class="line">IOUtils.copyBytes(in, out, 4096);</span><br><span class="line"></span><br><span class="line">IOUtils.closeStream(out);</span><br><span class="line">IOUtils.closeStream(in);</span><br></pre></td></tr></table></figure>

<ul>
<li><p>下载服务器的文件到本地<br>读服务器的数据(读io)  写入到本地(写io)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FSDataInputStream in = fileSystem.open(new Path(&quot;/input/wc-io.txt&quot;));</span><br><span class="line">FileOutputStream out = new FileOutputStream(new File(&quot;output/b.txt&quot;));</span><br><span class="line"></span><br><span class="line">IOUtils.copyBytes(in, out, 4096);</span><br><span class="line"></span><br><span class="line">IOUtils.closeStream(out);</span><br><span class="line">IOUtils.closeStream(in);</span><br></pre></td></tr></table></figure></li>
</ul>
<p>更多的hdfs api接口方法可参考：<a href="https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/FileSystem.html">https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/fs/FileSystem.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>HDFS文件读写流程与副本放置策略</title>
    <url>/2021/12/03/HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="HDFS文件写流程"><a href="#HDFS文件写流程" class="headerlink" title="HDFS文件写流程"></a>HDFS文件写流程</h3><p><img src="/2021/12/03/HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/%E5%86%99.png" alt="HDFS文件写流程"></p>
<ol>
<li><p>客户端调用FileSystem.create(filePath)方法新建文件，但此时文件中还没有相应的数据块。</p>
</li>
<li><p>DFS和NN进行【RPC】通信，NN会去检查这个文件是否已经存在、是否有权限创建这个文件等一系列校验操作；</p>
<p>如果校验通过，就会为创建新文件记录一条记录，告知DFS向客户端返回一个【FsDataOutputStream】对象</p>
<p>如果校验失败，文件创建失败并向客户端抛出一个IOException异常。</p>
</li>
<li><p>Client 调用【FsDataOutputStream】对象的write方法，将数据分成一个个的数据包，并写入【数据队列】。</p>
<p>【DataStreamer】处理数据队列,根据文件的大小、当前集群的块大小、副本数和当前的DN节点情况计算出这个文件要上传多少个块(包含副本)和块上传到哪些DN节点，要求NN分配新的数据块。这一组选定的DN构成【管线】。</p>
</li>
<li><p>根据【副本放置策略】，【DataStreamer】处理数据队列,将数据包传输到【管线】中DN1，DN1存储并将它发送到DN2，DN2存储并将它发送到DN3。</p>
</li>
<li><p>【FsDataOutputStream】也维护一个【确认队列】等待确认回执，当三个副本写完的时候，DN3就返回一个ack package确认包给DN2，DN2接收到并加上自己的确认信息到ack package确认包DN1，DN1接收到并加上自己的确认信息到ack package确认包给【FsDataOutputStream】，告诉它三个副本都写完了，数据包才会从【确认队列】删除。</p>
</li>
<li><p>当所有的块全部写完，Client调用【FsDataOutputStream】对象的close方法，关闭输出流。</p>
</li>
<li><p>再次调用FileSystem.complete方法，告诉NN文件写成功。</p>
</li>
</ol>
<h3 id="HDFS文件读流程"><a href="#HDFS文件读流程" class="headerlink" title="HDFS文件读流程"></a>HDFS文件读流程</h3><p><img src="/2021/12/03/HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/%E8%AF%BB.png" alt="HDFS文件读流程"></p>
<ol>
<li><p>Client调用FileSystem的open(filePath)方法，打开希望读取的文件</p>
</li>
<li><p>DFS与NN进行【RPC】通信，确定文件的起始位置。NN返回这个文件的部分或者全部的block列表（DN会根据他们与客户端的距离排序，如果客户端本身就是一个DN，则会从本地DN读取数据）</p>
</li>
<li><p>DFS给Client返回一个【FSDataInputStream】对象。</p>
</li>
<li><p>Client调用【FSDataInputStream】对象的read方法，开始读取数据</p>
</li>
<li><p>连接最近的存储要读取文件中第一个块的DN进行读取，读取完成后会校验是否完整</p>
<ul>
<li>假如ok就关闭与DN通信。</li>
<li>假如不ok，就记录块和DN的信息，通知NN，保证以后不会反复读取该节点后续的块，会尝试从这个块的另一个邻近节点读取。</li>
</ul>
<p>然后连接最近的第二个块所在的DN进行读取，以此类推。</p>
<p>假如当block的列表全部读取完成，文件还没结束，再去NN请求下一个批次的block列表。</p>
<p>（整个过程对于客户端都是透明的，在客户端看来它一直读取一个连续的流）</p>
</li>
<li><p>一旦Client完成读取，调用【FSDataInputStram】对象的close方法，关闭输入流。</p>
</li>
</ol>
<h3 id="HDFS副本放置策略"><a href="#HDFS副本放置策略" class="headerlink" title="HDFS副本放置策略"></a>HDFS副本放置策略</h3><blockquote>
<p>Hadoop的默认布局策略是在运行客户端的节点上放第1个复本（如果客户端运行在集群之外，就随机的选择一个节点，但是系统会避免挑选那些存储太满或太忙的节点）。第2个复本放在与第1个不同且是随机选择的另外的机架中的节点上。第3个复本与第2个复本放在同一个机架上面，且是随机选择的一个节点，其他的复本放在集群中随机选择的节点上面，但是系统会尽量避免在相同的机架上面放太多的复本。</p>
<p>一旦选定了复本的放置的位置，就会根据网络拓扑创建一个管线，如下图为一个典型的复本管线：<br><img src="/2021/12/03/HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/datacenter.png" alt="HDFS副本放置策略"></p>
<p>总的来说，这一方法不仅提供了很好的稳定性，数据不容易丢失（数据块存储在两个机架中）同时实现了很好的负载均衡，包括写入宽带（写入操作只要遍历一个交换机），读取性能（可以从两个机架中进行选择读取）和集群中块的均匀分布（客户端只在本地机架写入一个块）。</p>
</blockquote>
<p>生产上进行读写，尽量自己选取DN节点。（减少网络IO）</p>
<p>第一个副本：放在Client所处的节点上。如果客户端在集群外，则放在随机调选的一台不太忙的DN上。</p>
<p>第二个副：放置在和第一个副本不相同的机架的随机节点上。</p>
<p>第三个副本：放置在和第二个副本位于相同机架的不同节点上。</p>
<p>假如还有更多副本：随机放。</p>
<p>但是，生产上真的是这样的吗？这样会带来 权限问题，比如一不小心把Linux文件删除了怎么办</p>
<p>所以生产上真正的是，有个单点的客户端节点，不是NN也不是DN进程在。</p>
<p>其实网络IO只是小问题，一般生产上集群内部都是万兆带宽，光纤的。忽略不计。</p>
]]></content>
  </entry>
  <entry>
    <title>HIVE UDF 与 HIVE源码编译</title>
    <url>/2021/12/29/HIVE-UDF/</url>
    <content><![CDATA[<h2 id="一、实现UDF"><a href="#一、实现UDF" class="headerlink" title="一、实现UDF"></a>一、实现UDF</h2><p>需求：添加随机数<code>add_random</code>、去除随机数<code>remove_random</code></p>
<p>UDF函数中实现evaluate方法。</p>
<p>UDFAddRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">import java.util.Random;</span><br><span class="line"></span><br><span class="line">public class UDFAddRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		int num = new Random().nextInt(10);</span><br><span class="line">		return s+&quot;_&quot;+num;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFAddRandom input = new UDFAddRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>UDFRemoveRandom.java</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package org.apache.hadoop.hive.ql.udf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line">public class UDFRemoveRandom extends UDF &#123;</span><br><span class="line">	public String evaluate(String s)&#123;</span><br><span class="line">		return s.split(&quot;_&quot;)[0];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		UDFRemoveRandom input = new UDFRemoveRandom();</span><br><span class="line">		System.out.println(input.evaluate(&quot;PK_91&quot;));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="二、在查询中使用函数"><a href="#二、在查询中使用函数" class="headerlink" title="二、在查询中使用函数"></a>二、在查询中使用函数</h2><h3 id="临时函数"><a href="#临时函数" class="headerlink" title="临时函数"></a>临时函数</h3><ol>
<li><p>将包含函数的jar包上传到服务器上，我的存放目录是<code>/home/hadoop/lib</code></p>
</li>
<li><p>开启hive会话，执行以下命令添加jar：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (hive)&gt; add jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br><span class="line">Added [/home/hadoop/lib/ruozedata-hive-1.0.jar] to class path</span><br><span class="line">Added resources: [/home/hadoop/lib/ruozedata-hive-1.0.jar]</span><br></pre></td></tr></table></figure></li>
<li><p>执行以下命令创建名为add_random的临时函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (hive)&gt; create temporary function add_random as &#x27;com.ruozedata.hive.udf.UDFAddRandom&#x27;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.025 seconds</span><br></pre></td></tr></table></figure>

<p>remove_random同理。</p>
</li>
<li><p>使用函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (hive)&gt; select * from emp;</span><br><span class="line">OK</span><br><span class="line">emp.empno	emp.ename	emp.job	emp.mgr	emp.hiredate	emp.sal	emp.comm	emp.deptno</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.0	NULL	20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.0	300.0	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.0	500.0	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.0	NULL	20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.0	1400.0	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.0	NULL	30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	NULL	10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.0	NULL	20</span><br><span class="line">7839	KING	PRESIDENT	NULL	1981-11-17	5000.0	NULL	10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.0	0.0	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.0	NULL	20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.0	NULL	30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.0	NULL	20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	NULL	10</span><br><span class="line">Time taken: 0.331 seconds, Fetched: 14 row(s)</span><br><span class="line">hive (hive)&gt; select ename,add_random(ename) from emp;</span><br><span class="line">OK</span><br><span class="line">ename	_c1</span><br><span class="line">SMITH	SMITH_8</span><br><span class="line">ALLEN	ALLEN_5</span><br><span class="line">WARD	WARD_1</span><br><span class="line">JONES	JONES_0</span><br><span class="line">MARTIN	MARTIN_0</span><br><span class="line">BLAKE	BLAKE_9</span><br><span class="line">CLARK	CLARK_5</span><br><span class="line">SCOTT	SCOTT_7</span><br><span class="line">KING	KING_8</span><br><span class="line">TURNER	TURNER_6</span><br><span class="line">ADAMS	ADAMS_6</span><br><span class="line">JAMES	JAMES_2</span><br><span class="line">FORD	FORD_6</span><br><span class="line">MILLER	MILLER_0</span><br><span class="line">Time taken: 0.882 seconds, Fetched: 14 row(s)</span><br><span class="line">hive (hive)&gt; </span><br></pre></td></tr></table></figure></li>
<li><p>这个UDF只在当前会话窗口生效，当您关闭了窗口此函数就不存在了；</p>
</li>
<li><p>如果您想在当前窗口将这个UDF清理掉，请依次执行以下两个命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop temporary function if exists add_random;</span><br><span class="line">delete jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br></pre></td></tr></table></figure></li>
<li><p>删除后再使用add_random会报错：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (hive)&gt; drop temporary function if exists add_random;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.024 seconds</span><br><span class="line">hive (hive)&gt; delete jar /home/hadoop/lib/ruozedata-hive-1.0.jar;</span><br><span class="line">Deleted [/home/hadoop/lib/ruozedata-hive-1.0.jar] from class path</span><br><span class="line">hive (hive)&gt; select ename,add_random(ename) from emp;</span><br><span class="line">FAILED: SemanticException [Error 10011]: Invalid function add_random</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="永久函数"><a href="#永久函数" class="headerlink" title="永久函数"></a>永久函数</h3><ol>
<li><p>UDF永久生效,并且对所有hive会话都生效</p>
</li>
<li><p>hdfs上创建目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -mkdir /lib/udflib</span><br></pre></td></tr></table></figure></li>
<li><p>将jar文件上传到hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -put /home/hadoop/lib/ruozedata-hive-1.0.jar /lib/udflib</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /lib/udflib</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       6098 2021-12-28 17:25 /lib/udflib/ruozedata-hive-1.0.jar</span><br></pre></td></tr></table></figure></li>
<li><p>开启hive会话，执行以下命令添加jar：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (hive)&gt; create function add_random as &#x27;com.ruozedata.hive.udf.UDFAddRandom&#x27;</span><br><span class="line">           &gt; using jar &#x27;hdfs:///lib/udflib/ruozedata-hive-1.0.jar&#x27;;</span><br><span class="line">Added [/tmp/38a5942b-b210-4c46-b700-9a64ae6090b7_resources/ruozedata-hive-1.0.jar] to class path</span><br><span class="line">Added resources: [hdfs:///lib/udflib/ruozedata-hive-1.0.jar]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.097 seconds</span><br></pre></td></tr></table></figure></li>
<li><p>函数可以使用，新开hive会话也可使用。</p>
</li>
</ol>
<h2 id="三、整合函数到hive源码中，编译hive"><a href="#三、整合函数到hive源码中，编译hive" class="headerlink" title="三、整合函数到hive源码中，编译hive"></a>三、整合函数到hive源码中，编译hive</h2><ol>
<li><p>解压src包到相应目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src</span><br></pre></td></tr></table></figure></li>
<li><p>把函数放到目录<code>ql/src/java/org/apache/hadoop/hive/ql/udf</code>下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd ql/src/java/org/apache/hadoop/hive/ql/udf</span><br><span class="line">[hadoop@hadoop001 udf]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/ql/src/java/org/apache/hadoop/hive/ql/udf</span><br></pre></td></tr></table></figure></li>
<li><p>修改FunctionRegistry.java </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd ql/src/java/org/apache/hadoop/hive/ql/exec</span><br><span class="line">[hadoop@hadoop001 exec]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/ql/src/java/org/apache/hadoop/hive/ql/exec</span><br><span class="line">[hadoop@hadoop001 exec]$ vi FunctionRegistry.java </span><br></pre></td></tr></table></figure>

<p>到相关位置插入添加的函数信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.hive.ql.udf.UDFAddRandom; </span><br><span class="line">import org.apache.hadoop.hive.ql.udf.UDFRemoveRandom; </span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">static &#123;</span><br><span class="line">    system.registerUDF(&quot;add_random&quot;, UDFAddRandom.class, false);</span><br><span class="line">	system.registerUDF(&quot;remove_random&quot;, UDFRemoveRandom.class, false);</span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>编译</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</span><br></pre></td></tr></table></figure>

<p>等待编译</p>
<p><img src="/2021/12/29/HIVE-UDF/hexo\k12blog\source_posts\HIVE-UDF\BUILDSUCCESS.png" alt="img"></p>
<p>目录<code>packaging/target/</code>下的<code>apache-hive-3.1.2-bin.tar.gz</code>就是我们需要的tar包。（不想重新部署的话可以参考第7步）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd packaging/target/</span><br><span class="line">[hadoop@hadoop001 target]$ ll</span><br><span class="line">total 410320</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 antrun</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 apache-hive-3.1.2-bin</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 315855613 Dec 31 06:26 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  77970637 Dec 31 06:27 apache-hive-3.1.2-jdbc.jar</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  26307866 Dec 31 06:27 apache-hive-3.1.2-src.tar.gz</span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop      4096 Dec 31 06:26 archive-tmp</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 maven-shared-archive-resources</span><br><span class="line">drwxrwxr-x. 7 hadoop hadoop      4096 Dec 31 06:26 testconf</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 tmp</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 warehouse</span><br></pre></td></tr></table></figure></li>
<li><p>部署hive（省略）</p>
</li>
<li><p>检查函数</p>
<p>用<code>show functions</code>或者<code>desc function 函数名</code>都可以</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hive</span><br><span class="line">which: no hbase in (/home/hadoop/app/hive/bin:/home/hadoop/app/scala/bin:/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/home/hadoop/app/protobuf/bin:/home/hadoop/app/maven/bin:/usr/local/mysql/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = d35e6d35-1d7b-40ae-b86e-3bf09fc0f5d2</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 5fca1029-8f4a-4c52-9d22-0b2a0942cc85</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc function add_random;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">There is no documentation for function &#x27;add_random&#x27;</span><br><span class="line">Time taken: 0.078 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; desc function remove_random;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">There is no documentation for function &#x27;remove_random&#x27;</span><br><span class="line">Time taken: 0.049 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>替换jar包</p>
<p>找到我们需要替换的<code>hive-exec-3.1.2.jar</code>包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ pwd</span><br><span class="line">/home/hadoop/software/apache-hive-3.1.2-src/packaging/target/apache-hive-3.1.2-bin/apache-hive-3.1.2-bin/lib</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.jar </span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 41063609 Dec 31 06:26 hive-exec-3.1.2.jar</span><br></pre></td></tr></table></figure>

<p>进入到正在使用的hive目录下，找到要被替换的包，改名备份</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ pwd</span><br><span class="line">/home/hadoop/software/hive/lib</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.jar </span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 40623961 Aug 23  2019 hive-exec-3.1.2.jar</span><br><span class="line">[hadoop@hadoop001 lib]$ mv hive-exec-3.1.2.jar hive-exec-3.1.2.jar_bak</span><br></pre></td></tr></table></figure>

<p>替换</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ cp /home/hadoop/software/apache-hive-3.1.2-src/packaging/target/apache-hive-3.1.2-bin/apache-hive-3.1.2-bin/lib/hive-exec-3.1.2.jar ./</span><br><span class="line">[hadoop@hadoop001 lib]$ ll hive-exec-3.1.2.*</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 41063609 Dec 31 06:46 hive-exec-3.1.2.jar</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 40623961 Aug 23  2019 hive-exec-3.1.2.jar_bak</span><br></pre></td></tr></table></figure>

<p>然后重启Hive即可找到函数。</p>
</li>
</ol>
<h2 id="四、Hive源码编译"><a href="#四、Hive源码编译" class="headerlink" title="四、Hive源码编译"></a>四、Hive源码编译</h2><p>个人在上面操作的源码编译上遇到好几个坑，又是改仓库又是修改java文件的。</p>
<p>一开始是直接下载hive官网下载地址<a href="https://dlcdn.apache.org/hive/hive-3.1.2/%E7%9A%84src%E5%8C%85%E3%80%82">https://dlcdn.apache.org/hive/hive-3.1.2/的src包。</a></p>
<p>在目录下执行命令开始编译</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</span><br></pre></td></tr></table></figure>

<p>问题一：<strong>pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar</strong>缺失</p>
<p>在maven的conf/settings.xml 中添加阿里云仓库地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ vi app/maven/conf/settings.xml </span><br></pre></td></tr></table></figure>

<p>注意：<code>&lt;mirror&gt;&lt;/mirror&gt;</code>标签要在<code>&lt;mirrors&gt;&lt;/mirrors&gt;</code>内,我最开始没注意，原来文本下面有个<code>&lt;mirrors&gt;</code>没注释掉</p>
<p><img src="file:///C:\Users\K-12\AppData\Roaming\Tencent\Users\945862485\QQ\WinTemp\RichOle\7]E9`A7%CWY{TP3YC}VZEM6.png" alt="img"></p>
<p>我这里注释掉了，因为后来使用maven仓库下载的。</p>
<p>问题二：添加阿里云仓库后，重新编译，依然报错</p>
<p><img src="/2021/12/29/HIVE-UDF/hexo\k12blog\source_posts\HIVE-UDF\LLAP.png" alt="image-20211231161907516"></p>
<p>网上搜查发现有同样问题的，然后需要根据错误提示，参考<a href="https://github.com/gitlbo/hive/commits/3.1.2%E4%BF%AE%E6%94%B9%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84%E6%9F%90%E5%87%A0%E4%B8%AA%E7%B1%BB%E3%80%82%E4%BA%8E%E6%98%AF%E6%88%91%E6%8C%89%E6%AD%A5%E9%AA%A4%E4%BF%AE%E6%94%B9%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%BC%96%E8%AF%91%E8%BF%99%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%BA%86%EF%BC%8C%E4%BD%86%E5%8F%88%E6%9C%89%E5%85%B6%E4%BB%96%E5%9C%B0%E6%96%B9%E6%8A%A5%E9%94%99%E3%80%82">https://github.com/gitlbo/hive/commits/3.1.2修改源码中的某几个类。于是我按步骤修改后，可以编译这个组件了，但又有其他地方报错。</a></p>
<p>既然都要按照修改，为什么不直接用最新的src包呢，于是我下载了一个新的src包。</p>
<p><img src="/2021/12/29/HIVE-UDF/hive1.png" alt="image-20211231162607453"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive2.png" alt="image-20211231162633319"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive3.png" alt="image-20211231162658887"></p>
<p><img src="/2021/12/29/HIVE-UDF/hive4.png" alt="image-20211231162727453"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 software]$ wget -O apache-hive-3.1.2-src.zip https://codeload.github.com/gitlbo/hive/zip/c073e71ef43699b7aa68cad7c69a2e8f487089fd</span><br></pre></td></tr></table></figure>

<p>然后解压，修改pom.xml里的hadoop.version为我的版本3.2.2.然后其他根据个人需要修改。</p>
<p>使用命令编译<code>mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true</code>，没有报错，编译成功。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 apache-hive-3.1.2-src]$ cd packaging/target/</span><br><span class="line">[hadoop@hadoop001 target]$ ll</span><br><span class="line">total 410320</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 antrun</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 apache-hive-3.1.2-bin</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 315855613 Dec 31 06:26 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  77970637 Dec 31 06:27 apache-hive-3.1.2-jdbc.jar</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  26307866 Dec 31 06:27 apache-hive-3.1.2-src.tar.gz</span><br><span class="line">drwxrwxr-x. 4 hadoop hadoop      4096 Dec 31 06:26 archive-tmp</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop      4096 Dec 31 06:26 maven-shared-archive-resources</span><br><span class="line">drwxrwxr-x. 7 hadoop hadoop      4096 Dec 31 06:26 testconf</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 tmp</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop      4096 Dec 31 06:26 warehouse</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>HDFS中的数据块(Block)</title>
    <url>/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/</url>
    <content><![CDATA[<p>我们在分布式存储原理总结中了解了分布式存储的三大特点：</p>
<ol>
<li>数据分块，分布式的存储在多台机器上</li>
<li>数据块冗余存储在多台机器以提高数据块的高可用性</li>
<li>遵从主/从(master/slave)结构的分布式存储集群</li>
</ol>
<p>HDFS作为分布式存储的实现，肯定也具有上面3个特点。</p>
<span id="more"></span>

<h2 id="HDFS数据块"><a href="#HDFS数据块" class="headerlink" title="HDFS数据块"></a>HDFS数据块</h2><p>与一般文件系统一样，HDFS也有块（block）的概念，HDFS上的文件也被划分为块大小的多个分块作为独立的存储单元。与通常的磁盘文件系统不同的是：</p>
<p><strong>HDFS中小于一个块大小的文件不会占据整个块的空间（当一个1MB的文件存储在一个128MB的块中时，文件只使用1MB的磁盘空间，而不是128MB）</strong></p>
<p>在Hadoop1当中，文件的block块默认大小是64M，Hadoop2当中，文件的block块大小默认是128M，block块的大小可以通过<code>hdfs-site.xml</code>当中的配置文件（dfs.block.size）进行指定。</p>
<p><strong>设置数据块的好处</strong></p>
<p>（1）一个文件的大小可以大于集群任意节点磁盘的容量</p>
<p>（2）容易对数据进行备份，提高容错能力</p>
<p>（3）使用抽象块概念而非整个文件作为存储单元，大大简化存储子系统的设计</p>
<p><strong>块缓存</strong><br>通常DataNode从磁盘中读取块，但对于访问频繁的文件，其对应的块可能被显示的缓存在DataNode的内存中，以堆外块缓存的形式存在。默认情况下，一个块仅缓存在一个DataNode的内存中，当然可以针对每个文件配置DataNode的数量。作业调度器通过在缓存块的DataNode上运行任务，可以利用块缓存的优势提高读操作的性能。</p>
<h2 id="HDFS分布式存储"><a href="#HDFS分布式存储" class="headerlink" title="HDFS分布式存储"></a>HDFS分布式存储</h2><p>在HDFS中，数据块默认的大小是128M，当我们往HDFS上上传一个300多M的文件的时候，那么这个文件会被分成3个数据块： </p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193738768-2015006415.png" alt="img"></p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193745301-961787885.png" alt="img"></p>
<p> 所有的数据块是分布式的存储在所有的DataNode上：</p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193756443-722084406.png" alt="img"></p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193801770-1600366947.png" alt="img"></p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193806888-1561633390.png" alt="img"></p>
<p>为了提高每一个数据块的高可用性，在HDFS中每一个数据块默认备份存储3份，在这里我们看到的只有1份，是因为我们在<code>hdfs-site.xml</code>中配置了如下的配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;表示数据块的备份数量，不能大于DataNode的数量，默认值是3&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>我们也可以通过如下的命令，将文件<code>/user/hadoop-twq/cmd/big_file.txt</code>的所有的数据块都备份存储3份：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs -setrep 3 /user/hadoop-twq/cmd/big_file.txt</span><br></pre></td></tr></table></figure>

<p>我们可以从如下可以看出：每一个数据块都冗余存储了3个备份</p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193841533-1801289433.png" alt="img"> </p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193847536-1876269159.png" alt="img"></p>
<p> <img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193851762-2045344208.png" alt="img"></p>
<p><strong>在这里，可能会问这里为什么看到的是2个备份呢？这个是因为我们的集群只有2个DataNode，所以最多只有2个备份，即使你设置成3个备份也没用，所以我们设置的备份数一般都是比集群的DataNode的个数相等或者要少</strong></p>
<blockquote>
<p>一定要注意：当我们上传362.4MB的数据到HDFS上后，如果数据块的备份数是3个话，那么在HDFS上真正存储的数据量大小是：362.4MB * 3 = 1087.2MB</p>
</blockquote>
<blockquote>
<p>注意：我们上面是通过HDFS的WEB UI来查看HDFS文件的数据块的信息，除了这种方式查看数据块的信息，我们还可以通过命令fsck来查看</p>
</blockquote>
<h2 id="问题：HDFS里面为什么一般设置块大小为64MB或128MB？"><a href="#问题：HDFS里面为什么一般设置块大小为64MB或128MB？" class="headerlink" title="问题：HDFS里面为什么一般设置块大小为64MB或128MB？"></a>问题：HDFS里面为什么一般设置块大小为64MB或128MB？</h2><ul>
<li><p>为什么不能远少于64MB？</p>
<p>（1）<strong>减少硬盘寻道时间。</strong>HDFS设计前提是应对大数据量操作，若数据块大小设置过小，那需要读取的数据块数量就会增多，从而间接增加底层硬盘的寻道时间</p>
<blockquote>
<p>  “HDFS的块比磁盘块大，其目的是为了最小化寻址开销。如果块设置得足够大，从<strong>磁盘传输数据的时间</strong>可以明显大于<strong>定位这个块开始位置所需的时间</strong>。这样，传输一个由多个块组成的文件的时间就<strong>取决于磁盘传输速率</strong>。”</p>
<p> “我们做一个估计计算，如果寻址时间为10ms左右，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们需要设置块大小为100MB左右。而默认的块大小实际为64MB，但是很多情况下HDFS使用128MB的块设置。<strong>以后随着新一代磁盘驱动器传输速率的提升，块的大小将被设置得更大。</strong>”</p>
</blockquote>
<p>（2）<strong>减少NameNode内存消耗。</strong>由于NameNode记录着DataNode中的数据块信息，若数据块大小设置过小，则数据块数量增多，需要维护的数据块信息就会增多，从而消耗NameNode的内存。</p>
</li>
<li><p>为什么不能远大于64MB？</p>
<p>原因主要从上层的MapReduce框架来寻找。</p>
<p>（1）<strong>Map崩溃问题。</strong>系统需要重新启动，启动过程中需要重新加载数据，数据块越大，数据加载时间越长，系统恢复过程越长</p>
<p>（2）<strong>监管时间问题。</strong>主节点监管其他节点的情况，每个节点会周期性的与主节点进行汇报通信。倘若某一个节点保持沉默的时间超过一个<strong>预设的时间间隔</strong>，主节点会记录这个节点状态为死亡，并将该节点的数据转发给别的节点。而这个“预设时间间隔”是从数据块的角度大致估算的。（加入对64MB的数据块，我可以假设你10分钟之内无论如何也能解决完了吧，超过10分钟还没反应，那我就认为你出故障或已经死了。）64MB大小的数据块，其时间尚可较为精准地估计，如果我将数据块大小设为640MB甚至上G，那这个“预设的时间间隔”便不好估算，估长估短对系统都会造成不必要的损失和资源浪费。</p>
<p>（3）<strong>问题分解问题。</strong>数据量的大小与问题解决的复杂度呈线性关系。对于同一个算法，处理的数据量越大，时间复杂度越高。</p>
<p>（4）<strong>约束Map输出。</strong>在Map Reduce框架里，Map之后的数据是要经过排序才执行Reduce操作的。这通常涉及到归并排序，而归并排序的算法思想便是“对小文件进行排序，然后将小文件归并成大文件”，因此“小文件”不宜过大。</p>
<blockquote>
<p>“<strong>但是，该参数也不会设置得过大。MapReduce中的map任务通常一次处理一个块中的数据，因此，如果任务数太少（少于集群中的节点数量），作业的运行速度就会变慢。</strong>”</p>
</blockquote>
</li>
</ul>
<h2 id="数据块的实现"><a href="#数据块的实现" class="headerlink" title="数据块的实现"></a>数据块的实现</h2><p>在HDFS的实现中，数据块被抽象成类<code>org.apache.hadoop.hdfs.protocol.Block(我们以下简称Block)</code>。在Block类中有如下几个属性字段：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Block</span> <span class="keyword">implements</span> <span class="title">Writable</span>, <span class="title">Comparable</span>&lt;<span class="title">Block</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> blockId; <span class="comment">// 标识一个Block的唯一Id</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> numBytes; <span class="comment">// Block的大小(单位是字节)</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> generationStamp; <span class="comment">// Block的生成时间戳</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们从WEB UI上的数据块信息也可以看到：</p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193921505-989550889.png" alt="img"></p>
<p> 一个Block除了存储上面的3个字段信息，还需要知道这个Block含有多少个备份，每一个备份分别存储在哪一个DataNode上，为了存储这些信息，HDFS中有一个名为</p>
<p><code>org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous</code>(下面我们简称为BlockInfo)</p>
<p>的类来存储这些信息，这个BlockInfo类继承Block类，如下：</p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193930783-1441983482.png" alt="img"></p>
<p>BlockInfo类中只有一个非常核心的属性，就是名为triplets的数组，这个数组的长度是<code>3*replication</code>，<code>replication</code>表示数据块的备份数。这个数组中存储了该数据块所有的备份数据块对应的DataNode信息，我们现在假设备份数是<code>3</code>，那么这个数组的长度是<code>3*3=9</code>，这个数组存储的数据如下： </p>
<p><img src="/2021/12/03/HDFS%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9D%97-Block/1598893-20190908193941684-2013773685.png" alt="img"></p>
<p>也就是说，triplets包含的信息：</p>
<ul>
<li>triplets[i]：Block所在的DataNode；</li>
<li>triplets[i+1]：该DataNode上前一个Block；</li>
<li>triplets[i+2]：该DataNode上后一个Block；</li>
</ul>
<p>其中i表示的是Block的第i个副本，i取值[0,replication)。</p>
<p>我们在HDFS的NameNode中的Namespace管理中讲到了，一个HDFS文件包含一个BlockInfo数组，表示这个文件分成的若干个数据块，这个BlockInfo数组实际上就是我们这里说的<code>BlockInfoContiguous</code>数组。以下是INodeFile的属性：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">INodeFile</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> header = <span class="number">0L</span>; <span class="comment">// 用于标识存储策略ID、副本数和数据块大小的信息</span></span><br><span class="line">    <span class="keyword">private</span> BlockInfoContiguous[] blocks; <span class="comment">// 该文件包含的数据块数组</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那么，到现在为止，我们了解到了这些信息：文件包含了哪些Block，这些Block分别被实际存储在哪些DataNode上，DataNode上所有Block前后链表关系。</p>
<p>如果从信息完整度来看，以上信息数据足够支持所有关于HDFS文件系统的正常操作，但还存在一个使用场景较多的问题：怎样通过blockId快速定位BlockInfo？</p>
<p>我们其实可以在NameNode上用一个HashMap来维护blockId到Block的映射，也就是说我们可以使用<code>HashMap&lt;Block, BlockInfo&gt;</code>来维护，这样的话我们就可以快速的根据blockId定位BlockInfo，但是由于在内存使用、碰撞冲突解决和性能等方面存在问题，Hadoop团队之后使用重新实现的LightWeightGSet代替HashMap，该数据结构本质上也是利用链表解决碰撞冲突的<a href="https://issues.apache.org/jira/browse/HDFS-1114">HashTable</a>，但是在易用性、内存占用和性能等方面表现更好。</p>
<p>HDFS为了解决通过blockId快速定位BlockInfo的问题，所以引入了BlocksMap，BlocksMap底层通过LightWeightGSet实现。</p>
<p>在HDFS集群启动过程，DataNode会进行BR（BlockReport，其实就是将DataNode自身存储的数据块上报给NameNode），根据BR的每一个Block计算其HashCode，之后将对应的BlockInfo插入到相应位置逐渐构建起来巨大的BlocksMap。前面在INodeFile里也提到的BlockInfo集合，如果我们将BlocksMap里的BlockInfo与所有INodeFile里的BlockInfo分别收集起来，可以发现两个集合完全相同，事实上BlocksMap里所有的BlockInfo就是INodeFile中对应BlockInfo的引用；通过Block查找对应BlockInfo时，也是先对Block计算HashCode，根据结果快速定位到对应的BlockInfo信息。至此涉及到HDFS文件系统本身元数据的问题基本上已经解决了。</p>
<h2 id="BlocksMap内存估算"><a href="#BlocksMap内存估算" class="headerlink" title="BlocksMap内存估算"></a>BlocksMap内存估算</h2><p>HDFS将文件按照一定的大小切成多个Block，为了保证数据可靠性，每个Block对应多个副本，存储在不同DataNode上。NameNode除需要维护Block本身的信息外，还需要维护从Block到DataNode列表的对应关系，用于描述每一个Block副本实际存储的物理位置，BlocksMap结构即用于Block到DataNode列表的映射关系，BlocksMap是常驻在内存中，而且占用内存非常大，所以对BlocksMap进行内存的估算是非常有必要的。</p>
<p><strong>BlocksMap</strong>的内部结构：</p>
<blockquote>
<p>以下的内存估算是在64位操作系统上且没有开启指针压缩功能场景下</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class BlocksMap &#123;</span><br><span class="line">    private final int capacity; // 占 4 字节</span><br><span class="line">    // 我们使用GSet的实现者：LightWeightGSet</span><br><span class="line">    private GSet&lt;Block, BlockInfoContiguous&gt; blocks;  // 引用类型占8字节</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以得出BlocksMap的直接内存大小是：<strong>对象头16字节 + 4字节 + 8字节 = 28字节</strong></p>
<p><strong>Block</strong>的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class Block implements Writable, Comparable&lt;Block&gt; &#123;</span><br><span class="line">    private long blockId; // 标识一个Block的唯一Id     占 8字节</span><br><span class="line">    private long numBytes; // Block的大小(单位是字节)   占 8字节</span><br><span class="line">    private long generationStamp; // Block的生成时间戳   占 8字节</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以得出Block的直接内存大小是：<strong>对象头16字节 + 8字节 + 8字节 + 8字节 = 40字节</strong></p>
<p><strong>BlockInfoContiguous</strong>的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class BlockInfoContiguous extends Block &#123;</span><br><span class="line">    private BlockCollection bc;   // 引用类型占8字节</span><br><span class="line">    private LightWeightGSet.LinkedElement nextLinkedElement;  // 引用类型占8字节</span><br><span class="line">    private Object[] triplets;  // 引用类型 8字节 + 数组对象头24字节 + 3*3(备份数假设为3)*8 = 104字节</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以得出BlockInfoContiguous的直接内存大小是：<strong>对象头16字节 + 8字节 + 8字节 + 104字节 = 136字节</strong></p>
<p><strong>LightWeightGSet</strong>的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class LightWeightGSet&lt;K, E extends K&gt; implements GSet&lt;K, E&gt; &#123;</span><br><span class="line">    private final LinkedElement[] entries; // 引用类型 8字节 + 数组对象头24字节 = 32字节</span><br><span class="line">    private final int hash_mask; // 4字节</span><br><span class="line">    private int size = 0; // 4字节</span><br><span class="line">    private int modification = 0; // 4字节</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　LightWeightGSet本质是一个链式解决冲突的哈希表，为了避免rehash过程带来的性能开销，初始化时，LightWeightGSet的索引空间直接给到了整个JVM可用内存的2%，并且不再变化。 所以LightWeightGSet的直接内存大小为：**对象头16字节 + 32字节 + 4字节 + 4字节 + 4字节 + (2%<em>JVM可用内存) = 60字节 + (2%<em>JVM可用内存)</em></em></p>
<p>假设集群中共1亿Block，NameNode可用内存空间固定大小128GB，则BlocksMap占用内存情况：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BlocksMap直接内存大小 + (Block直接内存大小 + BlockInfoContiguous直接内存大小) * 100M + LightWeightGSet直接内存大小</span><br><span class="line">即：</span><br><span class="line">28字节 + (40字节 + 136字节) * 100M + 60字节 + (2%*128G) = 19.7475GB</span><br></pre></td></tr></table></figure>

<blockquote>
<p>上面为什么是乘以100M呢？ 因为100M = 100 * 1024 * 1024 bytes = 104857600 bytes，约等于1亿字节，而上面的内存的单位都是字节的，我们乘以100M，就相当于1亿Block</p>
</blockquote>
<p>BlocksMap数据在NameNode整个生命周期内常驻内存，随着数据规模的增加，对应Block数会随之增多，BlocksMap所占用的JVM堆内存空间也会基本保持线性同步增加。</p>
<hr>
<p>参考：</p>
<p><a href="https://blog.csdn.net/wjn19921104/article/details/80742655">https://blog.csdn.net/wjn19921104/article/details/80742655</a></p>
<p><a href="https://www.cnblogs.com/tesla-turing/p/11488035.html">https://www.cnblogs.com/tesla-turing/p/11488035.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>HIVE的部署</title>
    <url>/2021/12/28/HIVE%E7%9A%84%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h3 id="0-前期准备"><a href="#0-前期准备" class="headerlink" title="0.前期准备"></a>0.前期准备</h3><p>启动mysql和hadoop</p>
<h3 id="1-下载hive的tar包"><a href="#1-下载hive的tar包" class="headerlink" title="1.下载hive的tar包"></a>1.下载hive的tar包</h3><p>到<a href="https://dlcdn.apache.org/hive/%E9%80%89%E6%8B%A9%E9%9C%80%E8%A6%81%E7%9A%84%E7%89%88%E6%9C%AC%EF%BC%8C%E6%88%91%E8%BF%99%E9%87%8C%E9%83%A8%E7%BD%B2hive-3.1.2%E7%89%88%E6%9C%AC%EF%BC%9A">https://dlcdn.apache.org/hive/选择需要的版本，我这里部署hive-3.1.2版本：</a></p>
<p><a href="https://dlcdn.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz">apache-hive-3.1.2-bin.tar.gz</a></p>
<h3 id="2-通过rz命令上传到服务器并解压，"><a href="#2-通过rz命令上传到服务器并解压，" class="headerlink" title="2.通过rz命令上传到服务器并解压，"></a>2.通过rz命令上传到服务器并解压，</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd software/</span><br><span class="line">[hadoop@hadoop001 software]$ rz</span><br><span class="line">[hadoop@hadoop001 software]$ tar -zvxf apache-hive-3.1.2-bin.tar.gz </span><br><span class="line">[hadoop@hadoop001 software]$ ll</span><br><span class="line">total 1109404</span><br><span class="line">drwxrwxr-x. 10 hadoop hadoop      4096 Dec 27 00:09 apache-hive-3.1.2-bin</span><br><span class="line">[hadoop@hadoop001 software]$ cd ~/app</span><br><span class="line">[hadoop@hadoop001 app]$ ln -s hive /home/hadoop/software/apache-hive-3.1.2-bin</span><br></pre></td></tr></table></figure>

<h3 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ vi ~/.bash_profile   </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export HIVE_HOME=/home/hadoop/app/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH </span><br></pre></td></tr></table></figure>

<h3 id="4-拷贝mysql的驱动到lib下"><a href="#4-拷贝mysql的驱动到lib下" class="headerlink" title="4.拷贝mysql的驱动到lib下"></a>4.拷贝mysql的驱动到lib下</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ mv mysql-connector-java-5.1.47.jar app/hive/lib/</span><br></pre></td></tr></table></figure>

<h3 id="5-配置hive-site-xml"><a href="#5-配置hive-site-xml" class="headerlink" title="5.配置hive-site.xml"></a>5.配置hive-site.xml</h3><p>hive-site.xml配置mysql相关信息（hive-site.xml这个配置文件是配置元数据的相关信息，元数据存放在mysql中）</p>
<p>hive-site.xml所在目录 <code>/home/hadoop/app/hive/conf/</code>,如果没有vi创建。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--</span><br><span class="line">   Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">   contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">   this work for additional information regarding copyright ownership.</span><br><span class="line">   The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">   (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line">   the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">   Unless required by applicable law or agreed to in writing, software</span><br><span class="line">   distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">   See the License for the specific language governing permissions and</span><br><span class="line">   limitations under the License.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;jdbc:mysql://hadoop001:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">		&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h3 id="6-初始化"><a href="#6-初始化" class="headerlink" title="6.初始化"></a>6.初始化</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure>

<h3 id="7-启动hive"><a href="#7-启动hive" class="headerlink" title="7.启动hive"></a>7.启动hive</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$ hive</span><br><span class="line">which: no hbase in (/home/hadoop/app/hive/bin:/home/hadoop/app/scala/bin:/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/home/hadoop/app/protobuf/bin:/home/hadoop/app/maven/bin:/home/hadoop/app/scala/bin:/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/home/hadoop/app/protobuf/bin:/home/hadoop/app/maven/bin:/usr/local/mysql/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/software/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = 70f578b4-d4d6-4236-a82f-580ff0ca44a3</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/software/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = c1629998-6455-4976-a8c2-c97d2f94104c</span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">Time taken: 0.779 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="8-在mysql中查看hive的元信息"><a href="#8-在mysql中查看hive的元信息" class="headerlink" title="8.在mysql中查看hive的元信息"></a>8.在mysql中查看hive的元信息</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| hive               |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| ruozedata          |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">6 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; use hive;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+-------------------------------+</span><br><span class="line">| Tables_in_hive                |</span><br><span class="line">+-------------------------------+</span><br><span class="line">| aux_table                     |</span><br><span class="line">| bucketing_cols                |</span><br><span class="line">| cds                           |</span><br><span class="line">| columns_v2                    |</span><br><span class="line">| compaction_queue              |</span><br><span class="line">| completed_compactions         |</span><br><span class="line">| completed_txn_components      |</span><br><span class="line">| ctlgs                         |</span><br><span class="line">| database_params               |</span><br><span class="line">| db_privs                      |</span><br><span class="line">| dbs                           |</span><br><span class="line">| delegation_tokens             |</span><br><span class="line">| func_ru                       |</span><br><span class="line">| funcs                         |</span><br><span class="line">| global_privs                  |</span><br><span class="line">| hive_locks                    |</span><br><span class="line">| i_schema                      |</span><br><span class="line">| idxs                          |</span><br><span class="line">| index_params                  |</span><br><span class="line">| key_constraints               |</span><br><span class="line">| master_keys                   |</span><br><span class="line">| materialization_rebuild_locks |</span><br><span class="line">| metastore_db_properties       |</span><br><span class="line">| min_history_level             |</span><br><span class="line">| mv_creation_metadata          |</span><br><span class="line">| mv_tables_used                |</span><br><span class="line">| next_compaction_queue_id      |</span><br><span class="line">| next_lock_id                  |</span><br><span class="line">| next_txn_id                   |</span><br><span class="line">| next_write_id                 |</span><br><span class="line">| notification_log              |</span><br><span class="line">| notification_sequence         |</span><br><span class="line">| nucleus_tables                |</span><br><span class="line">| part_col_privs                |</span><br><span class="line">| part_col_stats                |</span><br><span class="line">| part_privs                    |</span><br><span class="line">| partition_events              |</span><br><span class="line">| partition_key_vals            |</span><br><span class="line">| partition_keys                |</span><br><span class="line">| partition_params              |</span><br><span class="line">| partitions                    |</span><br><span class="line">| repl_txn_map                  |</span><br><span class="line">| role_map                      |</span><br><span class="line">| roles                         |</span><br><span class="line">| runtime_stats                 |</span><br><span class="line">| schema_version                |</span><br><span class="line">| sd_params                     |</span><br><span class="line">| sds                           |</span><br><span class="line">| sequence_table                |</span><br><span class="line">| serde_params                  |</span><br><span class="line">| serdes                        |</span><br><span class="line">| skewed_col_names              |</span><br><span class="line">| skewed_col_value_loc_map      |</span><br><span class="line">| skewed_string_list            |</span><br><span class="line">| skewed_string_list_values     |</span><br><span class="line">| skewed_values                 |</span><br><span class="line">| sort_cols                     |</span><br><span class="line">| tab_col_stats                 |</span><br><span class="line">| table_params                  |</span><br><span class="line">| tbl_col_privs                 |</span><br><span class="line">| tbl_privs                     |</span><br><span class="line">| tbls                          |</span><br><span class="line">| txn_components                |</span><br><span class="line">| txn_to_write_id               |</span><br><span class="line">| txns                          |</span><br><span class="line">| type_fields                   |</span><br><span class="line">| types                         |</span><br><span class="line">| version                       |</span><br><span class="line">| wm_mapping                    |</span><br><span class="line">| wm_pool                       |</span><br><span class="line">| wm_pool_to_trigger            |</span><br><span class="line">| wm_resourceplan               |</span><br><span class="line">| wm_trigger                    |</span><br><span class="line">| write_set                     |</span><br><span class="line">+-------------------------------+</span><br><span class="line">74 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br></pre></td></tr></table></figure>

<h3 id="9-其他（部署过程中遇到的问题）"><a href="#9-其他（部署过程中遇到的问题）" class="headerlink" title="9.其他（部署过程中遇到的问题）"></a>9.其他（部署过程中遇到的问题）</h3><ul>
<li><p>Hive启动报错：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</p>
<p>错误原因：系统找不到这个类所在的jar包或者jar包的版本不一样系统不知道使用哪个。hive启动报错的原因是后者</p>
<p>解决办法：</p>
<p>1、com.google.common.base.Preconditions.checkArgument这个类所在的jar包为：guava.jar</p>
<p>2、hadoop-3.2.1（路径：hadoop\share\hadoop\common\lib）中该jar包为 guava-27.0-jre.jar；而hive-3.1.2(路径：hive/lib)中该jar包为guava-19.0.1.jar</p>
<p>3、将jar包变成一致的版本：删除hive中低版本jar包，将hadoop中高版本的复制到hive的lib中。</p>
<p>再次启动问题得到解决！</p>
</li>
<li><p>FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.me</p>
<p>原因分析：<br>是由于没有初始化数据库导致，执行名称初始化数据库即可。</p>
<p>解决办法：<br>执行命令：<code>schematool -dbType mysql -initSchema</code></p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Hadoop Shell命令</title>
    <url>/2021/11/26/Hadoop%20Shell%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="Hadoop-Shell命令"><a href="#Hadoop-Shell命令" class="headerlink" title="Hadoop Shell命令"></a>Hadoop Shell命令</h1><h2 id="FS-Shell"><a href="#FS-Shell" class="headerlink" title="FS Shell"></a>FS Shell</h2><p>调用文件系统(FS)Shell命令应使用 bin/hadoop fs &lt;args&gt;的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是<em>scheme://authority/path</em>。对HDFS文件系统，scheme是<em>hdfs</em>，对本地文件系统，scheme是<em>file</em>。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如*/parent/child<em>可以表示成</em>hdfs://namenode:namenodeport/parent/child<em>，或者更简单的</em>/parent/child<em>（假设你配置文件中的默认值是</em>namenode:namenodeport<em>）。大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到</em>stderr<em>，其他信息输出到</em>stdout*。</p>
<span id="more"></span>

<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>使用方法：<code>hadoop fs -cat URI [URI …]</code></p>
<p>将路径指定文件的内容输出到<em>stdout</em>。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2</code></li>
<li><code>hadoop fs -cat file:///file3 /user/hadoop/file4</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h3><p>使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] </p>
<p>改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>使用方法：<code>hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI …]</code></p>
<p>改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>使用方法：<code>hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</code></p>
<p>改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">HDFS Permissions Guide</a>。</p>
<h3 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h3><p>使用方法：<code>hadoop fs -copyFromLocal &lt;localsrc&gt; URI</code></p>
<p>除了限定源路径是一个本地文件外，和<a href="#put"><strong>put</strong></a>命令相似。</p>
<h3 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a>copyToLocal</h3><p>使用方法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</code></p>
<p>除了限定目标路径是一个本地文件外，和<a href="#get"><strong>get</strong></a>命令类似。</p>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>使用方法：<code>hadoop fs -cp URI [URI …] &lt;dest&gt;</code></p>
<p>将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>示例：</p>
<ul>
<li><code>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2</code></li>
<li><code>hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="du"><a href="#du" class="headerlink" title="du"></a>du</h3><p>使用方法：<code>hadoop fs -du URI [URI …]</code></p>
<p>显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：<br><code>hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1</code><br>返回值：成功返回0，失败返回-1。</p>
<h3 id="dus"><a href="#dus" class="headerlink" title="dus"></a>dus</h3><p>使用方法：<code>hadoop fs -dus &lt;args&gt;</code></p>
<p>显示文件的大小。</p>
<h3 id="expunge"><a href="#expunge" class="headerlink" title="expunge"></a>expunge</h3><p>使用方法：<code>hadoop fs -expunge</code></p>
<p>清空回收站。请参考 <a href="http://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes">HDFS Architecture guide</a> 文档以获取更多关于回收站特性的信息。</p>
<h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><p>使用方法：<code>hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;</code></p>
<p>复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -get /user/hadoop/file localfile</code></li>
<li><code>hadoop fs -get hdfs://host:port/user/hadoop/file localfile</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="getmerge"><a href="#getmerge" class="headerlink" title="getmerge"></a>getmerge</h3><p>使用方法：<code>hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</code></p>
<p>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</p>
<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>使用方法：<code>hadoop fs -ls &lt;args&gt;</code></p>
<p>如果是文件，则按照如下格式返回文件信息：<br>文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>目录名 &lt;dir&gt; 修改日期 修改时间 权限 用户ID 组ID<br>示例：</p>
<ul>
<li><code>hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="lsr"><a href="#lsr" class="headerlink" title="lsr"></a>lsr</h3><p>使用方法：<code>hadoop fs -lsr &lt;args&gt;</code><br>ls命令的递归版本。类似于Unix中的ls -R。</p>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>使用方法：<code>hadoop fs -mkdir &lt;paths&gt;</code></p>
<p>接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2</code></li>
<li><code>hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="movefromLocal"><a href="#movefromLocal" class="headerlink" title="movefromLocal"></a>movefromLocal</h3><p>使用方法：<code>dfs -moveFromLocal &lt;src&gt; &lt;dst&gt;</code></p>
<p>输出一个”not implemented“信息。</p>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>使用方法：<code>hadoop fs -mv URI [URI …] &lt;dest&gt;</code></p>
<p>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p>
<ul>
<li><code>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</code></li>
<li><code>hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><p>使用方法：<code>hadoop fs -put &lt;localsrc&gt; ... &lt;dst&gt;</code></p>
<p>从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p>
<ul>
<li><code>hadoop fs -put localfile /user/hadoop/hadoopfile</code></li>
<li><code>hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</code></li>
<li><code>hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile</code></li>
<li><code>hadoop fs -put - hdfs://host:port/hadoop/hadoopfile</code><br>从标准输入中读取输入。</li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>使用方法：<code>hadoop fs -rm URI [URI …]</code></p>
<p>删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p>
<ul>
<li><code>hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h3><p>使用方法：<code>hadoop fs -rmr URI [URI …]</code></p>
<p>delete的递归版本。<br>示例：</p>
<ul>
<li><code>hadoop fs -rmr /user/hadoop/dir</code></li>
<li><code>hadoop fs -rmr hdfs://host:port/user/hadoop/dir</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a>setrep</h3><p>使用方法：<code>hadoop fs -setrep [-R] &lt;path&gt;</code></p>
<p>改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h3><p>使用方法：<code>hadoop fs -stat URI [URI …]</code></p>
<p>返回指定路径的统计信息。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -stat path</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>使用方法：<code>hadoop fs -tail [-f] URI</code></p>
<p>将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -tail pathname</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>使用方法：<code>hadoop fs -test -[ezd] URI</code></p>
<p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p>
<p>示例：</p>
<ul>
<li><code>hadoop fs -test -e filename</code></li>
</ul>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><p>使用方法：<code>hadoop fs -text &lt;src&gt;</code></p>
<p>将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p>
<h3 id="touchz"><a href="#touchz" class="headerlink" title="touchz"></a>touchz</h3><p>使用方法：<code>hadoop fs -touchz URI [URI …]</code></p>
<p>创建一个0字节的空文件。</p>
<p>示例：</p>
<ul>
<li><code>hadoop -touchz pathname</code></li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
<h2 id="hdfs-其他命令"><a href="#hdfs-其他命令" class="headerlink" title="hdfs 其他命令"></a>hdfs 其他命令</h2><ul>
<li><p>安全模式</p>
<p>hdfs dfsadmin     [-safemode &lt;enter | leave | get | wait&gt;]</p>
<p>安全模式关闭：读写正常</p>
<p>log看到safemode:on，必然是集群有问题的，可以手动退出，就能正常对外提供服务</p>
<p>启动安全模式：<code>hdfs dfsadmin -safemode enter</code></p>
<p>启动后可读不可写</p>
</li>
<li><p>hdfs fsck /</p>
<p>检查系统问题</p>
</li>
<li><p>集群平衡</p>
<p>dfs.disk.balancer.enabled:  true</p>
<p>执行命令：先生成计划再执行</p>
<p><code>hdfs balancer</code> </p>
<p>DN1 DN2节点和节点之前的平衡 2.X</p>
<p><code>hdfs diskbalancer</code></p>
<p>单个节点多盘的平衡 3.X</p>
</li>
<li><p>回收站</p>
<p>linux有回收站吗？没有。要做，怎么办？：</p>
<ul>
<li><p>狸猫换太子</p>
<p>写脚本封装</p>
</li>
</ul>
<p><code>etc/hadoop/core-default.xml</code></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>fs.trash.interval</td>
<td>0</td>
<td>Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.</td>
</tr>
</tbody></table>
</li>
</ul>
<p>hadoop命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN applications, not</span><br><span class="line">              this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br></pre></td></tr></table></figure>

<p>查看当前版本压缩情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop checknative</span><br><span class="line">2021-11-28 14:06:21,011 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  false </span><br><span class="line">zlib:    false </span><br><span class="line">zstd  :  false </span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     false </span><br><span class="line">bzip2:   false </span><br><span class="line">openssl: false </span><br><span class="line">ISA-L:   false </span><br><span class="line">PMDK:    false </span><br><span class="line">2021-11-28 14:06:21,370 INFO util.ExitUtil: Exiting with status 1: ExitException</span><br><span class="line">[hadoop@hadoop001 ~]$ </span><br></pre></td></tr></table></figure>

<p>打印类的路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop classpath</span><br><span class="line">/home/hadoop/app/hadoop/etc/hadoop:/home/hadoop/app/hadoop/share/hadoop/common/lib/*:/home/hadoop/app/hadoop/share/hadoop/common/*:/home/hadoop/app/hadoop/share/hadoop/hdfs:/home/hadoop/app/hadoop/share/hadoop/hdfs/lib/*:/home/hadoop/app/hadoop/share/hadoop/hdfs/*:/home/hadoop/app/hadoop/share/hadoop/mapreduce/lib/*:/home/hadoop/app/hadoop/share/hadoop/mapreduce/*:/home/hadoop/app/hadoop/share/hadoop/yarn:/home/hadoop/app/hadoop/share/hadoop/yarn/lib/*:/home/hadoop/app/hadoop/share/hadoop/yarn/*</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>Hadoop Archives</title>
    <url>/2021/12/20/Hadoop-Archives/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Hadoop Archives就是指Hadoop存档。Hadoop Archives是特殊格式的存档，它会映射一个文件系统目录。一个Hadoop Archives文件总是带有<code>.har</code>扩展名</p>
<p>Hadoop存档(har文件)目录包含</p>
<ul>
<li><p>元数据（采用_index和_masterindex形式）</p>
</li>
<li><p>数据部分data（part- *）文件。</p>
</li>
</ul>
<p>_index文件包含归档文件的名称和部分文件中的位置。</p>
<p><img src="/2021/12/20/Hadoop-Archives/arcvhives1" alt="img"></p>
<span id="more"></span>

<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>​    hdfs并不擅长存储小文件，因为每个文件最少占用一个block，每个block的元数据都会在namenode节点占用内存，如果存在这样大量的小文件，它们会吃掉namenode节点的大量内存。<br>​    hadoop Archives可以有效的处理以上问题，他可以把多个文件归档成为一个文件，归档成一个文件后还可以透明的访问每一个文件，并且可以做为mapreduce任务的输入。（但对于MapReduce 来说起不到任何作用，因为har文件就相当一个目录，仍然不能讲小文件合并到一个split中去，一个小文件一个split）</p>
<h2 id="创建档案文件"><a href="#创建档案文件" class="headerlink" title="创建档案文件"></a>创建档案文件</h2><p>创建档案文件是一个Map/Reduce job，所以需要一个map reduce集群来运行它（启动YARN）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Usage: hadoop archive -archiveName name -p &lt;parent&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt;</span><br><span class="line">用法：hadoop archive -archiveName  归档名称 -p 父目录 [-r &lt;复制因子&gt;]  原路径（可以多个）  目的路径</span><br></pre></td></tr></table></figure>

<p><strong>参数说明</strong></p>
<ul>
<li>-archiveName 档案名.har:以<code>.har</code>为扩展名结尾的档案文件名字</li>
<li>-p 父目录:指定归档文件基于的相对路径</li>
<li>-r 副本数：所需的复制因子，不设置的话默认为3</li>
<li>&lt;src&gt;*:要归档的文件源路径，可多个</li>
<li>&lt;dest&gt;:har文件保存到的目标路径</li>
</ul>
<p><strong>Example:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop archive -archiveName foo.har -p /foo/bar -r 3 dir1 dir2 /user/hadoop</span><br></pre></td></tr></table></figure>

<p><code>/foo/bar</code>是<code>dir1</code>，<code>dir2</code>两个src路径的父目录，所以以上命令是归档<code>/foo/bar/dir1</code>，<code>/foo/bar/dir2</code>到 <code>/user/hadoop/foo.bar</code>中</p>
<p>如果想归档目录 /foo/bar，可以省略src：</p>
<p><code>hadoop archive -archiveName zoo.har -p /foo/bar -r 3 /outputdir</code></p>
<p><strong>补充说明</strong></p>
<ol>
<li>创建档案文件是一个Map/Reduce job，所以需要一个map reduce集群来运行它（启动YARN）。</li>
<li>归档文件后，不会删除源文件。如果需要删除源文件（来减少namespace），需要自己手动删除。</li>
<li>如果您指定加密区域中的源文件，它们将被解密并写入存档。如果har文件不在加密区中，则它们将以解密的形式存储。如果har文件位于加密区域，它们将以加密形式存储。</li>
</ol>
<h2 id="查看归档中的文件"><a href="#查看归档中的文件" class="headerlink" title="查看归档中的文件"></a>查看归档中的文件</h2><p>档案将自己公开为文件系统层。因此，档案中的所有fs shell命令都可以工作，但使用不同的URI。</p>
<p>Hadoop Archives的URI是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HAR：//方案-主机名：端口/ archivepath / fileinarchive</span><br></pre></td></tr></table></figure>

<p>如果没有提供方案，它假定底层文件系统。在这种情况下，URI看起来像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HAR：/// archivepath / fileinarchive</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>档案是不可变的。所以，重命名，删除并创建返回一个错误。</p>
<h2 id="如何解除归档"><a href="#如何解除归档" class="headerlink" title="如何解除归档"></a>如何解除归档</h2><p>由于档案中的所有fs shell命令都是透明的，因此取消存档只是复制的问题。</p>
<p>依次取消存档：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfs -cp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir</span><br></pre></td></tr></table></figure>

<p>要并行解压缩，请使用DistCp：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop distcp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir</span><br></pre></td></tr></table></figure>

<h2 id="Hadoop-Archives-and-MapReduce"><a href="#Hadoop-Archives-and-MapReduce" class="headerlink" title="Hadoop Archives and MapReduce"></a>Hadoop Archives and MapReduce</h2><p>​    在MapReduce中，与输入数据 使用默认文件系统一样，也可以使用Hadoop Archives(归档)文件作为输入文件系统。如果你有存储在HDFS目录下<code>/user/zoo/foo.har</code>的Hadoop Archives(归档)文件 ，然后你在MapReduce程序中就可以使用如下路径<code>har:///user/zoo/foo.har</code>作为输入文件。<br>由于Hadoop Archives(归档)文件是作为一种文件类型，MapReduce将能够使用Hadoop Archives(归档)文件中的所有逻辑输入文件作为输入源。</p>
<h2 id="个人示例"><a href="#个人示例" class="headerlink" title="个人示例"></a>个人示例</h2><ol>
<li><p>准备文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls -R /user/hadoop/input</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         11 2021-12-19 15:54 /user/hadoop/input/a.log</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         18 2021-12-19 15:54 /user/hadoop/input/b.log</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         11 2021-12-19 15:54 /user/hadoop/input/c.log</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-19 15:54 /user/hadoop/input/d</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          4 2021-12-19 15:54 /user/hadoop/input/d/e.log</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>创建har文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hadoop archive -archiveName input.har -p /user/hadoop/input /user/hadoop</span><br><span class="line">2021-12-19 15:56:44,393 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-12-19 15:56:45,593 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-12-19 15:56:46,217 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-12-19 15:56:46,258 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-12-19 15:56:46,685 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1639763497373_0008</span><br><span class="line">2021-12-19 15:56:47,302 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">2021-12-19 15:56:47,571 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639763497373_0008</span><br><span class="line">2021-12-19 15:56:47,578 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2021-12-19 15:56:47,895 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2021-12-19 15:56:47,895 INFO resource.ResourceUtils: Unable to find &#x27;resource-types.xml&#x27;.</span><br><span class="line">2021-12-19 15:56:48,044 INFO impl.YarnClientImpl: Submitted application application_1639763497373_0008</span><br><span class="line">2021-12-19 15:56:48,119 INFO mapreduce.Job: The url to track the job: http://hadoop001:8088/proxy/application_1639763497373_0008/</span><br><span class="line">2021-12-19 15:56:48,124 INFO mapreduce.Job: Running job: job_1639763497373_0008</span><br><span class="line">2021-12-19 15:56:58,359 INFO mapreduce.Job: Job job_1639763497373_0008 running in uber mode : false</span><br><span class="line">2021-12-19 15:56:58,361 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2021-12-19 15:57:05,437 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2021-12-19 15:57:12,484 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2021-12-19 15:57:13,506 INFO mapreduce.Job: Job job_1639763497373_0008 completed successfully</span><br><span class="line">2021-12-19 15:57:13,611 INFO mapreduce.Job: Counters: 54</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=425</span><br><span class="line">		FILE: Number of bytes written=473491</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=581</span><br><span class="line">		HDFS: Number of bytes written=450</span><br><span class="line">		HDFS: Number of read operations=24</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=12</span><br><span class="line">		HDFS: Number of bytes read erasure-coded=0</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Other local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=4796</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4103</span><br><span class="line">		Total time spent by all map tasks (ms)=4796</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4103</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=4796</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=4103</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=4911104</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=4201472</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=6</span><br><span class="line">		Map output records=6</span><br><span class="line">		Map output bytes=407</span><br><span class="line">		Map output materialized bytes=425</span><br><span class="line">		Input split bytes=118</span><br><span class="line">		Combine input records=0</span><br><span class="line">		Combine output records=0</span><br><span class="line">		Reduce input groups=6</span><br><span class="line">		Reduce shuffle bytes=425</span><br><span class="line">		Reduce input records=6</span><br><span class="line">		Reduce output records=0</span><br><span class="line">		Spilled Records=12</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=181</span><br><span class="line">		CPU time spent (ms)=1520</span><br><span class="line">		Physical memory (bytes) snapshot=322760704</span><br><span class="line">		Virtual memory (bytes) snapshot=5437816832</span><br><span class="line">		Total committed heap usage (bytes)=170004480</span><br><span class="line">		Peak Map Physical memory (bytes)=212164608</span><br><span class="line">		Peak Map Virtual memory (bytes)=2717405184</span><br><span class="line">		Peak Reduce Physical memory (bytes)=110596096</span><br><span class="line">		Peak Reduce Virtual memory (bytes)=2720411648</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=419</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=0</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls /user/hadoop/</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-19 15:54 /user/hadoop/input</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-19 15:57 /user/hadoop/input.har</span><br></pre></td></tr></table></figure></li>
<li><p>查看文件组成结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -cat /user/hadoop/input.har</span><br><span class="line">cat: `/user/hadoop/input.har&#x27;: Is a directory</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls /user/hadoop/input.har</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-12-19 15:57 /user/hadoop/input.har/_SUCCESS</span><br><span class="line">-rw-r--r--   3 hadoop supergroup        383 2021-12-19 15:57 /user/hadoop/input.har/_index</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         23 2021-12-19 15:57 /user/hadoop/input.har/_masterindex</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         44 2021-12-19 15:57 /user/hadoop/input.har/part-0</span><br></pre></td></tr></table></figure></li>
<li><p>使用hdfs文件系统查看har文件目录内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls har:///user/hadoop/input.har</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         11 2021-12-19 15:54 har:///user/hadoop/input.har/a.log</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         18 2021-12-19 15:54 har:///user/hadoop/input.har/b.log</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         11 2021-12-19 15:54 har:///user/hadoop/input.har/c.log</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-19 15:54 har:///user/hadoop/input.har/d</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls -R har:///user/hadoop/input.har</span><br><span class="line">2021-12-19 16:03:48,906 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         11 2021-12-19 15:54 har:///user/hadoop/input.har/a.log</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         18 2021-12-19 15:54 har:///user/hadoop/input.har/b.log</span><br><span class="line">-rw-r--r--   3 hadoop supergroup         11 2021-12-19 15:54 har:///user/hadoop/input.har/c.log</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-19 15:54 har:///user/hadoop/input.har/d</span><br><span class="line">-rw-r--r--   3 hadoop supergroup          4 2021-12-19 15:54 har:///user/hadoop/input.har/d/e.log</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Hadoop基础知识</title>
    <url>/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="一、Hadoop"><a href="#一、Hadoop" class="headerlink" title="一、Hadoop"></a>一、Hadoop</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>狭义上：以Hadoop软件本身（hadoop.apache.org），指一个用于大数据分布式存储(HDFS)，分布式计算(MapReduce)和资源调度(YARN)的平台，这三样只能用来做离线批处理，不能用于实时处理，因此才需要生态系统的其他的组件。</p>
<p>广义上：指的是hadoop的生态系统，即其他各种组件在内的一整套软件（sqoop，flume，spark，flink，hbase，kafka，cdh环境等）。hadoop生态系统是一个很庞大的概念，hadoop只是其中最重要最基础的部分，生态系统的每一个子系统只结局的某一个特定的问题域。不是一个全能系统，而是多个小而精的系统。</p>
<h2 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h2><p>Hadoop common：提供一些通用的功能支持其他hadoop模块。</p>
<p><strong>Hadoop Distributed File System</strong>：即分布式文件系统，简称HDFS。主要用来做数据存储，并提供对应用数据高吞吐量的访问。</p>
<p><strong>Hadoop MapReduce</strong>：基于yarn的，能用来并行处理大数据集的计算框架。</p>
<p><strong>Hadoop Yarn</strong>：用于作业调度和集群资源管理的框架。</p>
<h1 id="二、HDFS概述"><a href="#二、HDFS概述" class="headerlink" title="二、HDFS概述"></a>二、HDFS概述</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>Hdfs（hadoop distribute file system），他是一个文件系统，用于存储文件，通过目录树来定位文件：其次，他是分布式的，有很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>Hdfs的使用场景，适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据存放。</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li><p>高容错性</p>
<p>数据自动保存多个副本。 （默认是三分）通过增加副本的形式，提高容错性</p>
<p>某一个副本丢失以后，它可以自动恢复</p>
</li>
<li><p>适合大数据处理</p>
<p>数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据</p>
<p>文件规模：能够处理百万规模以上的文件数量，</p>
</li>
<li><p>可构建在廉价机器上，通过多副本机制，提高可靠性</p>
</li>
</ul>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>不适合低延时数据访问，比如毫秒级的存储数据，是做不到的</li>
<li>无法高效的对大量小文件进行存储<ul>
<li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的</li>
<li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li>
</ul>
</li>
<li>不支持并发写入、文件随机修改<ul>
<li>一个文件只能有一个写，不允许多个线程同时写（重点）</li>
<li>仅支持数据append（追加），不支持文件的随机修改</li>
</ul>
</li>
</ul>
<h2 id="hdfs支持的三种模式"><a href="#hdfs支持的三种模式" class="headerlink" title="hdfs支持的三种模式"></a>hdfs支持的三种模式</h2><ul>
<li>Local (Standalone) Mode ：本地模式，不启动进程，实际工作中从来没用过</li>
<li>Pseudo-Distributed Mode：伪分布式，启动单个进程（1大 小），应用场景：学习</li>
<li>Fully-Distributed Mode    集群模式，启动多个进程（2个大多个小），应用场景：生产（CDH,按量付费）</li>
</ul>
<h1 id="三、HDFS架构"><a href="#三、HDFS架构" class="headerlink" title="三、HDFS架构"></a>三、HDFS架构</h1><p> <img src="/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1598893-20191129134655673-2083411338.png" alt="img"></p>
<ol>
<li><strong>NameNode（nn）</strong>：Master，它是一个主管、管理者。</li>
</ol>
<ul>
<li><p>管理HDFS的名称空间</p>
<ul>
<li>文件的名称、目录结构、权限、大小、所属用户用户组  时间</li>
</ul>
</li>
<li><p>处理客户端读写请求</p>
</li>
<li><p>配置副本策略</p>
</li>
<li><p>管理数据块（Block）映射信息</p>
<ul>
<li><p>文件被切割哪些块、块(块本身+2副本=3个块)分布在哪些DN节点上，blockmap 块映射。</p>
</li>
<li><p>不会持久化存储这种映射关系，是通过集群<strong>启动</strong>和<strong>运行</strong>时候，DN定期给NN汇报blockreport（BR），然后NN在内存中动态维护这种映射关系；</p>
</li>
</ul>
</li>
</ul>
<p>2）<strong>DataNode</strong>：Slave。NameNode下达命令，DataNode执行实际的操作</p>
<ul>
<li><p>存储实际的数据块和块的校验和</p>
</li>
<li><p>执行数据块的读/写操作</p>
</li>
<li><p>定期给NN发送块报告</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dfs.blockreport.intervalMsec  21600000=6h</span><br><span class="line">dfs.datanode.directoryscan.interval  21600s=6h</span><br></pre></td></tr></table></figure></li>
</ul>
<p>3）<strong>Client</strong>：客户端</p>
<ul>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ul>
<p>4）<strong>Secondary NameNode</strong>：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<ul>
<li><p>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</p>
<ul>
<li><p>edits 编辑日志文件</p>
</li>
<li><p>fsimage 镜像文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NN:</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      42 Nov 28 08:07 edits_0000000000000000256-0000000000000000257</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      42 Nov 28 09:07 edits_0000000000000000258-0000000000000000259</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze 1048576 Nov 28 09:07 edits_inprogress_0000000000000000260</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze    2874 Nov 28 08:07 fsimage_0000000000000000257</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      62 Nov 28 08:07 fsimage_0000000000000000257.md5</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze    2874 Nov 28 09:07 fsimage_0000000000000000259</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      62 Nov 28 09:07 fsimage_0000000000000000259.md5</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze       4 Nov 28 09:07 seen_txid</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze     219 Nov 26 22:01 VERSION</span><br><span class="line">[ruoze@ruozedata001 current]$ pwd</span><br><span class="line">/home/ruoze/tmp/hadoop-ruoze/dfs/name/current</span><br><span class="line"></span><br><span class="line">SNN:</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      42 Nov 28 08:07 edits_0000000000000000256-0000000000000000257</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      42 Nov 28 09:07 edits_0000000000000000258-0000000000000000259</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze    2874 Nov 28 08:07 fsimage_0000000000000000257</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      62 Nov 28 08:07 fsimage_0000000000000000257.md5</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze    2874 Nov 28 09:07 fsimage_0000000000000000259</span><br><span class="line">-rw-rw-r-- 1 ruoze ruoze      62 Nov 28 09:07 fsimage_0000000000000000259.md5</span><br><span class="line"></span><br><span class="line">将NN的 </span><br><span class="line">fsimage_0000000000000000257</span><br><span class="line">edits_0000000000000000258-0000000000000000259</span><br><span class="line">拿到SNN，进行【合并】，生成fsimage_0000000000000000259文件，然后将此文件【推送】给NN；</span><br><span class="line">同时，NN在新的编辑日志文件edits_inprogress_0000000000000000260</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>在紧急情况下，可辅助恢复NameNode</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">关于NN的补充：在大数据早期的时候，只有NN一个，假如挂了就真的挂了。</span><br><span class="line">中期的时候，新增SNN来定期来合并、 备份 、推送，但是这样的也就是满足一定条件，如1小时，备份1次。例如，12点合并备份，但是12点半挂了，从SNN恢复到NN，只能恢复12点的时刻的元数据，丢了12点-12点半期间的元数据。</span><br><span class="line"></span><br><span class="line">后期就取消SNN，新建一个实时NN，作为高可靠 HA。</span><br><span class="line">NN Active</span><br><span class="line">NN Standby 实时的等待active NN挂了，瞬间启动Standby--&gt;Active，对外提供读写服务。</span><br></pre></td></tr></table></figure>



<p>HDFS中的文件在物理上是分块存储，块的大小可以通过配置参数（dfs.Blocksize）来规定，默认大小在hadoop2.x版本中是128M,老版本是64M</p>
<p><strong>思考：为什么块的大小不能设置太小，也不能设置太大？</strong></p>
<p>（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置</p>
<p>（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<p><strong>总结：HDFS块的大小设置主要取决于磁盘传输速率</strong></p>
<h1 id="四、MapReduce-on-Yarn-Yarn的工作流程"><a href="#四、MapReduce-on-Yarn-Yarn的工作流程" class="headerlink" title="四、MapReduce on Yarn/Yarn的工作流程"></a>四、MapReduce on Yarn/Yarn的工作流程</h1><p><img src="/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/mronyarn.jpg" alt="MapReduce on Yarn"></p>
<p>1.客户端client向ResourceManager提交应用程序/Application/作业JOB，包含application master程序，启动application master的命令等，并请求一个ApplicationMaster实例<br>2.RM为该job分配第一个container,与对应的NM通信，要求它在这个container启动作业的application master<br>3.application master向applications manager注册，这样用户就可以通过RM Web查看job的状态,一直到最后<br>4.application master采用轮询的方式通过【RPC】协议向resource scheduler申请和领取资源（哪台DN机器，领取多少内存 CPU）<br>5.一旦application master申请到资源后，与对应的NM通信，要求启动task<br>6.NM为任务设置好运行环境后，将任务的启动命令写到一个脚本中，并通过该脚本启动任务，运行任务<br>7.各个任务 task 通过【RPC】协议汇报自己的状态和进度，以让application master随时掌握各个任务的运行状态，从而在任务失败时，重启启动任务。<br>8.job运行完成后，application master向applications manager注销并关闭自己。</p>
<p>总结:<br>启动主程序，领取资源；1-4<br>运行任务，直到完成；  5-8</p>
<p>客户端提交job给 Applications Manager 连接Node Manager去申请一个Container的容器，这个容器运行作业的App Mstr的主程序，启动后向App Manager进行注册，然后可以访问URL界面，然后App Mastr向 Resource Scheduler申请资源，拿到一个资源的列表，和对应的NodeManager进行通信，去启动对应的Container容器，去运行 Reduce Task 和 Map Task （两个先后运行顺序随机运行），它们是向App Mstr进行汇报它们的运行状态， 当所有作业运行完成后还需要向Applications Manager进行汇报并注销和关闭</p>
<p>yarn中，它按照实际资源需求为每个任务分配资源，比如一个任务需要1GB内存，1个CPU，则为其分配对应的资源，而资源是用container表示的，container是一个抽象概念，它实际上是一个JAVA对象，里面有资源描述（资源所在节点，资源优先级，资源量，比如CPU量，内存量等）。当一个applicationmaster向RM申请资源时，RM会以container的形式将资源发送给对应的applicationmaster，applicationmaster收到container后，与对应的nodemanager通信，告诉它我要利用这个container运行某个任务。</p>
]]></content>
  </entry>
  <entry>
    <title>MR Chain（ChainMapper与ChainReducer）</title>
    <url>/2021/12/21/MR-Chain/</url>
    <content><![CDATA[<h2 id="ChainMapper-ChainReducer的实现原理"><a href="#ChainMapper-ChainReducer的实现原理" class="headerlink" title="ChainMapper/ChainReducer的实现原理"></a>ChainMapper/ChainReducer的实现原理</h2><p>​    ChainMapper/ChainReducer主要为了解决线性链式Mapper而提出的。也就是说，在Map或者Reduce阶段存在多个Mapper，这些Mapper像linux管道一样，前一个Mapper的输出结果直接重定向到下一个Mapper的输入，形成一个流水线，形式类似于[MAP + REDUCE MAP*]。下图展示了一个典型的ChainMapper/ChainReducer的应用场景。</p>
<p>​    在Map阶段，数据依次经过Mapper1和Mapper2处理；在Reducer阶段，数据经过shuffle和sort排序后，交给对应的Reduce处理，但Reducer处理之后还可以交给其它的Mapper进行处理，最终产生的结果写入到hdfs输出目录上。</p>
<p><strong>注意</strong>：对于任意一个MapReduce作业，Map和Reduce阶段可以有无限多个Mapper，但是<strong>Reducer只能有一个</strong>。</p>
<p>​    通过链式MapReducer模式可以有效的减少网络间传输数据的带宽，因为大量的计算基本都是在本地进行的。如果通过迭代作业的方式实现多个MapReduce作业组合的话就会在网络间传输大量的数据，这样会非常的耗时。(所以这里只是一个MR作业，MR作业的迭代实现用JobControl：)</p>
<p><img src="/2021/12/21/MR-Chain/hexo\k12blog\source_posts\MR-Chain\Chain.jpg" alt="Chain"></p>
<span id="more"></span>

<h2 id="ChainMapper"><a href="#ChainMapper" class="headerlink" title="ChainMapper"></a>ChainMapper</h2><h3 id="官方说明"><a href="#官方说明" class="headerlink" title="官方说明"></a>官方说明</h3><p>​    ChainMapper类允许使用多个Map子类作为一个Map任务。</p>
<p>​    这些map子类的执行与liunx的管道命令十分相似，第一个map的输出会成为第二个map的输入，第二个map的输出也会变成第三个map的输入，以此类推，直到最后一个map的输出会变成整个mapTask的输出。</p>
<p>​    该特性的关键功能是链中的Mappers不需要知道它们是在链中执行的。这使具有可重用的专门的映射器可以组合起来，在单个任务中执行组合操作。</p>
<p><strong>注意</strong>：在创建链式是每个Mapper的键/值的输出是链中下一个Mapper或Reducer的输入。它假定所有的映射器和链中的Reduce都使用匹配输出和输入键和值类，因为没有对链接代码进行转换。</p>
<h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">Configuration mapAConf = new Configuration(false);</span><br><span class="line">...</span><br><span class="line">ChainMapper.addMapper(job, AMap.class, LongWritable.class, Text.class,</span><br><span class="line"> Text.class, Text.class, true, mapAConf);</span><br><span class="line"></span><br><span class="line">Configuration mapBConf = new Configuration(false);</span><br><span class="line">...</span><br><span class="line">ChainMapper.addMapper(job, BMap.class, Text.class, Text.class,</span><br><span class="line"> LongWritable.class, Text.class, false, mapBConf);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">job.waitForComplettion(true);</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>

<h3 id="addMapper函数的参数说明"><a href="#addMapper函数的参数说明" class="headerlink" title="addMapper函数的参数说明"></a>addMapper函数的参数说明</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void addMapper(Job job, Class&lt;? extends Mapper&gt; klass,</span><br><span class="line">  Class&lt;?&gt; inputKeyClass, Class&lt;?&gt; inputValueClass,</span><br><span class="line">  Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span><br><span class="line">  Configuration mapperConf)</span><br><span class="line">## 参数的含义如下</span><br><span class="line"># 1. job</span><br><span class="line"># 2. 此map的class</span><br><span class="line"># 3. 此map的输入的key类型</span><br><span class="line"># 4. 此map的输入的value类型</span><br><span class="line"># 5. 此map的输出的key类型</span><br><span class="line"># 6. 此map的输出的value类型</span><br><span class="line"># 7. 此map的配置文件类conf</span><br></pre></td></tr></table></figure>



<h2 id="ChainReducer"><a href="#ChainReducer" class="headerlink" title="ChainReducer"></a>ChainReducer</h2><h3 id="官方说明-1"><a href="#官方说明-1" class="headerlink" title="官方说明"></a>官方说明</h3><p>ChainReducer类允许多个map在reduce执行完之后执行在一个reducerTask中，<br>reducer的每一条输出，都被作为输入给ChainReducer类设置的第一个map，然后第一个map的输出作为第二个map的输入，以此类推，最后一个map的输出会作为整个reducerTask的输出，写到磁盘上。</p>
<h3 id="使用方法-1"><a href="#使用方法-1" class="headerlink" title="使用方法"></a>使用方法</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Job = new Job(conf);</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">Configuration reduceConf = new Configuration(false);</span><br><span class="line">...</span><br><span class="line">ChainReducer.setReducer(job, XReduce.class, LongWritable.class, Text.class,</span><br><span class="line">  Text.class, Text.class, true, reduceConf);</span><br><span class="line"></span><br><span class="line">ChainReducer.addMapper(job, CMap.class, Text.class, Text.class,</span><br><span class="line">  LongWritable.class, Text.class, false, null);</span><br><span class="line"></span><br><span class="line">ChainReducer.addMapper(job, DMap.class, LongWritable.class, Text.class,</span><br><span class="line">  LongWritable.class, LongWritable.class, true, null);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">job.waitForCompletion(true);</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="setReducer函数的参数说明"><a href="#setReducer函数的参数说明" class="headerlink" title="setReducer函数的参数说明"></a>setReducer函数的参数说明</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void setReducer(Job job, Class&lt;? extends Reducer&gt; klass,</span><br><span class="line"> Class&lt;?&gt; inputKeyClass, Class&lt;?&gt; inputValueClass,</span><br><span class="line">  Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span><br><span class="line">   Configuration reducerConf)</span><br><span class="line">## 参数的含义如下</span><br><span class="line"># 1. job</span><br><span class="line"># 2. 此reducer的class</span><br><span class="line"># 3. 此reducer的输入的key类型</span><br><span class="line"># 4. 此reducer的输入的value类型</span><br><span class="line"># 5. 此reducer的输出的key类型</span><br><span class="line"># 6. 此reducer的输出的value类型</span><br><span class="line"># 7. 此reducer的配置文件类conf</span><br></pre></td></tr></table></figure>



<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="案例描述"><a href="#案例描述" class="headerlink" title="案例描述"></a>案例描述</h3><p>统计出一篇文章的高频词汇（只收集出现次数大于3的单词），去除谓词，并且过滤掉敏感词汇。</p>
<h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><p>在MapTask中有三个子Mapper，分别命名为M1,M2,M3，在ReduceTask阶段有一个Reduce命名为R1和一个Mpaaer命名为RM1。</p>
<h4 id="MapTask阶段"><a href="#MapTask阶段" class="headerlink" title="MapTask阶段"></a>MapTask阶段</h4><p>M1负责将文本内容按行切分每个单词，M2负责将M1输出的单词进行谓词过滤，M3将M2输出的内容进行敏感词过滤。</p>
<h4 id="ReduceTask阶段"><a href="#ReduceTask阶段" class="headerlink" title="ReduceTask阶段"></a>ReduceTask阶段</h4><p>Reduce过程中R1负责将shuffle阶段中的单词进行统计，统计好之后将结果交给RM1处理，RM1主要是将单词数量大于5的单词进行输出。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by yanzhe on 2017/8/18.</span><br><span class="line"> */</span><br><span class="line">public class App &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        args = new String[]&#123;&quot;d:/java/mr/data/data.txt&quot;, &quot;d:/java/mr/out&quot;&#125; ;</span><br><span class="line"></span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line"></span><br><span class="line">        FileSystem fs = FileSystem.get(conf) ;</span><br><span class="line"></span><br><span class="line">        Path outPath = new Path(args[1]) ;</span><br><span class="line">        if (fs.exists(outPath))&#123;</span><br><span class="line">            fs.delete(outPath,true) ;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf) ;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job,Mapper1.class, LongWritable.class, Text.class, Text.class, IntWritable.class,job.getConfiguration());</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job,Mapper2.class, Text.class,IntWritable.class, Text.class, IntWritable.class,job.getConfiguration());</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job,Mapper3.class, Text.class,IntWritable.class, Text.class, IntWritable.class,job.getConfiguration());</span><br><span class="line"></span><br><span class="line">        ChainReducer.setReducer(job, Reducer1.class, Text.class, IntWritable.class, Text.class, IntWritable.class,job.getConfiguration());</span><br><span class="line"></span><br><span class="line">        ChainReducer.addMapper(job, ReducerMapper1.class, Text.class,</span><br><span class="line">                IntWritable.class, Text.class, IntWritable.class, job.getConfiguration());</span><br><span class="line"></span><br><span class="line">        FileInputFormat.addInputPath(job,new Path(args[0]));</span><br><span class="line"></span><br><span class="line">        FileOutputFormat.setOutputPath(job,outPath);</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(2);</span><br><span class="line">        job.setCombinerClass(Combiner1.class);</span><br><span class="line">        job.setPartitionerClass(MyPartitioner.class);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(true) ;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public class Mapper1 extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            System.out.println(&quot;map1===========&quot; + value.toString());</span><br><span class="line">            String line = value.toString() ;</span><br><span class="line">            String[] strArr = line.split(&quot; &quot;) ;</span><br><span class="line"></span><br><span class="line">            for (String w: strArr) &#123;</span><br><span class="line">                context.write(new Text(w), new IntWritable(1));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class Mapper2 extends Mapper&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            System.out.println(&quot;map2==================&quot; + key.toString() + &quot;:&quot; + value.toString());</span><br><span class="line">            //过滤单词&#x27;of&#x27;</span><br><span class="line">            if (! key.toString().equals(&quot;of&quot;))&#123;</span><br><span class="line">                context.write(key, value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    public static class Mapper3 extends Mapper&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    </span><br><span class="line">        @Override</span><br><span class="line">        protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            System.out.println(&quot;map3==================&quot; + key.toString() + &quot;:&quot; + value.toString());</span><br><span class="line">            //过滤单词&#x27;google&#x27;</span><br><span class="line">            if (! key.toString().equals(&quot;xxx&quot;))&#123;</span><br><span class="line">                context.write(key, value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class Reducer1 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) </span><br><span class="line">        throws IOException, InterruptedException &#123;</span><br><span class="line">            int count = 0 ;</span><br><span class="line">            for (IntWritable iw: values) &#123;</span><br><span class="line">                count += iw.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, new IntWritable(count));</span><br><span class="line">            System.out.println(&quot;reduce=========&quot; + key.toString() + &quot;:&quot; + count);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    public static class ReducerMapper1 extends Mapper&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    </span><br><span class="line">        @Override</span><br><span class="line">        protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            if (value.get() &gt; 5)</span><br><span class="line">                context.write(key, value);</span><br><span class="line"></span><br><span class="line">            System.out.println(&quot;reduceMap======&quot; + key.toString() + &quot;:&quot; + value.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>原文链接：<a href="https://blog.csdn.net/u010521842/article/details/77413104">https://blog.csdn.net/u010521842/article/details/77413104</a></p>
]]></content>
  </entry>
  <entry>
    <title>MR作业的迭代：JobControl设计及用法</title>
    <url>/2021/12/21/MR%E4%BD%9C%E4%B8%9A%E7%9A%84%E8%BF%AD%E4%BB%A3%EF%BC%9AJobControl%E8%AE%BE%E8%AE%A1%E5%8F%8A%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h2 id="JobControl设计原理分析"><a href="#JobControl设计原理分析" class="headerlink" title="JobControl设计原理分析"></a>JobControl设计原理分析</h2><p>​    JobControl 由两个类组成：Job 和 JobControl。其中，Job 类封装了一个 MapReduce 作业及其对应的依赖关系，主要负责监控各个依赖作业的运行状态，以此更新自己的状态，其状态转移图如图所示。作业刚开始处于 WAITING 状态。如果没有依赖作业或者所有依赖作业均已运行完成，则进入READY 状态。一旦进入 READY 状态，则作业可被提交到 Hadoop 集群上运行，并进入 RUNNING 状态。在 RUNNING 状态下，根据作业运行情况，可能进入 SUCCESS 或者 FAILED 状态。需要注意的是，如果一个作业的依赖作业失败，则该作业也会失败，于是形成“多米诺骨牌效应”， 后续所有作业均会失败。</p>
<p><img src="/2021/12/21/MR%E4%BD%9C%E4%B8%9A%E7%9A%84%E8%BF%AD%E4%BB%A3%EF%BC%9AJobControl%E8%AE%BE%E8%AE%A1%E5%8F%8A%E7%94%A8%E6%B3%95/hexo\k12blog\source_posts\MR作业的迭代：JobControl设计及用法\JobControl.jpg" alt="img"></p>
<p>​    JobControl 封装了一系列 MapReduce 作业及其对应的依赖关系。 它将处于不同状态的作业放入不同的哈希表中，并按照图所示的状态转移作业，直到所有作业运行完成。在实现的时候，JobControl 包含一个线程用于周期性地监控和更新各个作业的运行状态，调度依赖作业运行完成的作业，提交处于 READY 状态的作业等。同时，它还提供了一些API 用于挂起、恢复和暂停该线程。</p>
<h2 id="JobControl代码实现"><a href="#JobControl代码实现" class="headerlink" title="JobControl代码实现"></a>JobControl代码实现</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import java.io.File;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.HashSet;</span><br><span class="line"> </span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"> </span><br><span class="line">import mapreduce.SegmentUtil;</span><br><span class="line"> </span><br><span class="line">public class JobControlDemo &#123;</span><br><span class="line">	public static int main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">		Configuration conf = new Configuration();</span><br><span class="line">		String[] otherargs = new GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">		if (otherargs.length != 3) &#123;</span><br><span class="line">			System.err.println(&quot;Usage JobControlDemo &lt;InputPath1&gt; &lt;InputPath1&gt; &lt;OutPath&gt;&quot;);</span><br><span class="line">			System.exit(2);</span><br><span class="line">		&#125;</span><br><span class="line"> </span><br><span class="line">		// 创建基础作业</span><br><span class="line">		Job job1 = Job.getInstance(conf, JobControlDemo.class.getSimpleName() + &quot;1&quot;);</span><br><span class="line">		Job job2 = Job.getInstance(conf, JobControlDemo.class.getSimpleName() + &quot;2&quot;);</span><br><span class="line">		Job job3 = Job.getInstance(conf, JobControlDemo.class.getSimpleName() + &quot;3&quot;);</span><br><span class="line"> </span><br><span class="line">		// Job1作业参数配置</span><br><span class="line">		job1.setJarByClass(JobControlDemo.class);</span><br><span class="line">		job1.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job1.setMapOutputValueClass(Text.class);</span><br><span class="line">		job1.setOutputKeyClass(Text.class);</span><br><span class="line">		job1.setOutputValueClass(Text.class);</span><br><span class="line">		job1.setMapperClass(MyMapper1.class);</span><br><span class="line">		job1.setReducerClass(MyReducer1.class);</span><br><span class="line">		job1.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">		job1.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">		FileInputFormat.addInputPath(job1, new Path(otherargs[0]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job1, new Path(otherargs[2]+File.separator+&quot;mid1&quot;));</span><br><span class="line"> </span><br><span class="line">		// Job2作业参数配置</span><br><span class="line">		job2.setJarByClass(JobControlDemo.class);</span><br><span class="line">		job2.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job2.setMapOutputValueClass(Text.class);</span><br><span class="line">		job2.setOutputKeyClass(Text.class);</span><br><span class="line">		job2.setOutputValueClass(Text.class);</span><br><span class="line">		job2.setMapperClass(MyMapper2.class);</span><br><span class="line">		job2.setReducerClass(MyReducer2.class);</span><br><span class="line">		job2.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">		job2.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">		FileInputFormat.addInputPath(job2, new Path(otherargs[1]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job2, new Path(otherargs[2]+File.separator+&quot;mid2&quot;));</span><br><span class="line"> </span><br><span class="line">		// Job3作业参数配置</span><br><span class="line">		job3.setJarByClass(JobControlDemo.class);</span><br><span class="line">		job3.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job3.setMapOutputValueClass(Text.class);</span><br><span class="line">		job3.setOutputKeyClass(Text.class);</span><br><span class="line">		job3.setOutputValueClass(Text.class);</span><br><span class="line">		job3.setMapperClass(MyMapper3.class);</span><br><span class="line">		job3.setReducerClass(MyReducer3.class);</span><br><span class="line">		job3.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line">		job3.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">		FileInputFormat.addInputPath(job3, new Path(otherargs[2]+File.separator+&quot;mid1&quot;));</span><br><span class="line">		FileInputFormat.addInputPath(job3, new Path(otherargs[2]+File.separator+&quot;mid2&quot;));</span><br><span class="line">		FileOutputFormat.setOutputPath(job3, new Path(otherargs[2]+File.separator+&quot;result&quot;));</span><br><span class="line"> </span><br><span class="line">		// 创建受控作业</span><br><span class="line">		ControlledJob cjob1 = new ControlledJob(conf);</span><br><span class="line">		ControlledJob cjob2 = new ControlledJob(conf);</span><br><span class="line">		ControlledJob cjob3 = new ControlledJob(conf);</span><br><span class="line"> </span><br><span class="line">		// 将普通作业包装成受控作业</span><br><span class="line">		cjob1.setJob(job1);</span><br><span class="line">		cjob2.setJob(job2);</span><br><span class="line">		cjob3.setJob(job3);</span><br><span class="line"> </span><br><span class="line">		// 设置依赖关系</span><br><span class="line">		//cjob2.addDependingJob(cjob1);</span><br><span class="line">		cjob3.addDependingJob(cjob1);</span><br><span class="line">		cjob3.addDependingJob(cjob2);</span><br><span class="line"> </span><br><span class="line">		// 新建作业控制器</span><br><span class="line">		JobControl jc = new JobControl(&quot;My control job&quot;);</span><br><span class="line"> </span><br><span class="line">		// 将受控作业添加到控制器中</span><br><span class="line">		jc.addJob(cjob1);</span><br><span class="line">		jc.addJob(cjob2);</span><br><span class="line">		jc.addJob(cjob3);</span><br><span class="line"> </span><br><span class="line">		/**</span><br><span class="line">		 * hadoop的JobControl类实现了线程Runnable接口。我们需要实例化一个线程来让它启动。直接调用JobControl的run()方法，线程将无法结束。</span><br><span class="line">		 */</span><br><span class="line">		//jc.run();</span><br><span class="line">		</span><br><span class="line">        Thread jcThread = new Thread(jc);  </span><br><span class="line">        jcThread.start();  </span><br><span class="line">        while(true)&#123;  </span><br><span class="line">            if(jc.allFinished())&#123;  </span><br><span class="line">                System.out.println(jc.getSuccessfulJobList());  </span><br><span class="line">                jc.stop();  </span><br><span class="line">                return 0;  </span><br><span class="line">            &#125;  </span><br><span class="line">            if(jc.getFailedJobList().size() &gt; 0)&#123;  </span><br><span class="line">                System.out.println(jc.getFailedJobList());  </span><br><span class="line">                jc.stop();  </span><br><span class="line">                return 1;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125; </span><br><span class="line">	&#125;</span><br><span class="line">	/**</span><br><span class="line">	 * 第一个Job</span><br><span class="line">	 */</span><br><span class="line">	public static class MyMapper1 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			String[] spl1=value.toString().split(&quot;\t&quot;);</span><br><span class="line">			if(spl1.length==2)&#123;</span><br><span class="line">				context.write(new Text(spl1[0].trim()), new Text(spl1[1].trim()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	public static class MyReducer1 extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void reduce(Text k2, Iterable&lt;Text&gt; v2s, Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			for (Text v2 : v2s) &#123;</span><br><span class="line">				context.write(k2, v2);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	/**</span><br><span class="line">	 * 第二个Job</span><br><span class="line">	 */</span><br><span class="line">	public static class MyMapper2 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			String[] spl2=value.toString().split(&quot;\t&quot;);</span><br><span class="line">			if(spl2.length==2)&#123;</span><br><span class="line">				context.write(new Text(spl2[0].trim()), new Text(spl2[1].trim()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	public static class MyReducer2 extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void reduce(Text k3, Iterable&lt;Text&gt; v3s, Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			for (Text v3 : v3s) &#123;</span><br><span class="line">				context.write(k3, v3);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	/**</span><br><span class="line">	 * 第三个Job</span><br><span class="line">	 */</span><br><span class="line">	public static class MyMapper3 extends Mapper&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void map(Text key, Text value, Mapper&lt;Text, Text, Text, Text&gt;.Context context)</span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			context.write(key, value);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	public static class MyReducer3 extends Reducer&lt;Text,Text, Text, Text&gt;&#123;</span><br><span class="line">		@Override</span><br><span class="line">		protected void reduce(Text k4, Iterable&lt;Text&gt; v4s,Reducer&lt;Text, Text, Text, Text&gt;.Context context) </span><br><span class="line">				throws IOException, InterruptedException &#123;</span><br><span class="line">			HashSet&lt;String&gt; hashSet=new HashSet&lt;String&gt;();</span><br><span class="line">			for (Text v4 : v4s) &#123;</span><br><span class="line">				hashSet.add(v4.toString().trim());</span><br><span class="line">			&#125;</span><br><span class="line">			if(hashSet.size()&gt;=2)&#123;</span><br><span class="line">				context.write(k4, new Text(&quot;OK&quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试输入数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfs -text /user/jiuqian/libin/input/inputpath1.txt</span><br><span class="line">hadoop  a</span><br><span class="line">spark   a</span><br><span class="line">hive    a</span><br><span class="line">hbase   a</span><br><span class="line">tachyon a</span><br><span class="line">storm   a</span><br><span class="line">redis   a</span><br><span class="line">hdfs dfs -text /user/jiuqian/libin/input/inputpath2.txt</span><br><span class="line">hadoop  b</span><br><span class="line">spark   b</span><br><span class="line">kafka   b</span><br><span class="line">tachyon b</span><br><span class="line">oozie   b</span><br><span class="line">flume   b</span><br><span class="line">sqoop   b</span><br><span class="line">solr    b</span><br></pre></td></tr></table></figure>

<p>测试输出数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfs -text /user/jiuqian/libin/input/inputpathmerge2.txt/result/*</span><br><span class="line">hadoop  OK</span><br><span class="line">spark   OK</span><br><span class="line">tachyon OK</span><br></pre></td></tr></table></figure>

<p>运行输出信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[sshexec] cmd : bash -c &#x27;source  /home/jiuqian/.bashrc; /home/hduser/hadoop/bin/hadoop jar  /home/jiuqian/blb/JobControlDemo.jar -D mapreduce.map.java.opts=-Xmx2048m -D mapreduce.input.fileinputformat.split.minsize=1 -Dmapreduce.input.fileinputformat.split.maxsize=512000000 -D mapred.linerecordreader.maxlength=32768 /user/jiuqian/libin/input/inputpath1.txt /user/jiuqian/libin/input/inputpath2.txt /user/jiuqian/libin/input/inputpathmerge2.txt&#x27;</span><br><span class="line">16/02/27 12:37:45 INFO client.RMProxy: Connecting to ResourceManager at sh-rslog1/192.168.1.2:8032</span><br><span class="line">16/02/27 12:37:46 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">16/02/27 12:37:46 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">16/02/27 12:37:46 INFO Configuration.deprecation: mapred.linerecordreader.maxlength is deprecated. Instead, use mapreduce.input.linerecordreader.line.maxlength</span><br><span class="line">16/02/27 12:37:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446086163035_17037</span><br><span class="line">16/02/27 12:37:47 INFO impl.YarnClientImpl: Submitted application application_1446086163035_17037</span><br><span class="line">16/02/27 12:37:47 INFO mapreduce.Job: The url to track the job: http://sh-rslog1:8088/proxy/application_1446086163035_17037/</span><br><span class="line">16/02/27 12:37:47 INFO client.RMProxy: Connecting to ResourceManager at sh-rslog1/27.115.29.102:8032</span><br><span class="line">16/02/27 12:37:47 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">16/02/27 12:37:47 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">16/02/27 12:37:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446086163035_17038</span><br><span class="line">16/02/27 12:37:47 INFO impl.YarnClientImpl: Submitted application application_1446086163035_17038</span><br><span class="line">16/02/27 12:37:47 INFO mapreduce.Job: The url to track the job: http://sh-rslog1:8088/proxy/application_1446086163035_17038/</span><br><span class="line">16/02/27 12:38:13 INFO client.RMProxy: Connecting to ResourceManager at sh-rslog1/27.115.29.102:8032</span><br><span class="line">16/02/27 12:38:13 INFO input.FileInputFormat: Total input paths to process : 2</span><br><span class="line">16/02/27 12:38:13 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">16/02/27 12:38:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446086163035_17039</span><br><span class="line">16/02/27 12:38:13 INFO impl.YarnClientImpl: Submitted application application_1446086163035_17039</span><br><span class="line">16/02/27 12:38:13 INFO mapreduce.Job: The url to track the job: http://sh-rslog1:8088/proxy/application_1446086163035_17039/</span><br><span class="line">[job name:	JobControlDemo1</span><br><span class="line">job id:	My control job0</span><br><span class="line">job state:	SUCCESS</span><br><span class="line">job mapred id:	job_1446086163035_17037</span><br><span class="line">job message:	just initialized</span><br><span class="line">job has no depending job:	</span><br><span class="line">, job name:	JobControlDemo2</span><br><span class="line">job id:	My control job1</span><br><span class="line">job state:	SUCCESS</span><br><span class="line">job mapred id:	job_1446086163035_17038</span><br><span class="line">job message:	just initialized</span><br><span class="line">job has no depending job:	</span><br><span class="line">, job name:	JobControlDemo3</span><br><span class="line">job id:	My control job2</span><br><span class="line">job state:	SUCCESS</span><br><span class="line">job mapred id:	job_1446086163035_17039</span><br><span class="line">job message:	just initialized</span><br><span class="line">job has 2 dependeng jobs:</span><br><span class="line">	 depending job 0:	JobControlDemo1</span><br><span class="line">	 depending job 1:	JobControlDemo2</span><br><span class="line">]</span><br><span class="line">[INFO] Executed tasks</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>



<p>参考链接：</p>
<p><a href="https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/mapreduce/lib/jobcontrol/JobControl.html">https://hadoop.apache.org/docs/r3.2.2/api/org/apache/hadoop/mapreduce/lib/jobcontrol/JobControl.html</a></p>
<p><a href="https://blog.csdn.net/baolibin528/article/details/50754753">https://blog.csdn.net/baolibin528/article/details/50754753</a></p>
<p><a href="https://www.cnblogs.com/wuyudong/p/hadoop-jobcontrol.html">https://www.cnblogs.com/wuyudong/p/hadoop-jobcontrol.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>MapReduce支持递归子目录作为输入</title>
    <url>/2021/12/30/MapReduce%E6%94%AF%E6%8C%81%E9%80%92%E5%BD%92%E5%AD%90%E7%9B%AE%E5%BD%95%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5/</url>
    <content><![CDATA[<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>执行MapReduce程序时，input path中包含子目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Error: java.io.FileNotFoundException: Path is not a file: /data/hive/mulit_file/sub_dir</span><br></pre></td></tr></table></figure>

<p>解决办法：mr中或者在mapred-site.xml中设置：mapreduce.input.fileinputformat.input.dir.recursive=true</p>
<ul>
<li><p>mr中设置configuration:<code>conf.set(&quot;mapreduce.input.fileinputformat.input.dir.recursive&quot;,true)</code></p>
</li>
<li><p>etc/hadoop/mapred-site.xml添加属性:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><p>在hive-cli中设置参数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set hive.mapred.supports.subdirectories=true;</span><br><span class="line">set mapred.input.dir.recursive=true;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Windows环境下JDK1.8.0安装与环境变量配置</title>
    <url>/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="一、准备工具"><a href="#一、准备工具" class="headerlink" title="一、准备工具"></a>一、准备工具</h3><h4 id="1-JDK"><a href="#1-JDK" class="headerlink" title="1.JDK"></a>1.JDK</h4><p>JDK安装包：jdk-8u202-windows-x64.exe<br><a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html">https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html</a><br>链接：<a href="https://pan.baidu.com/s/1_xEszoPjFIyic61FbFo2cg">https://pan.baidu.com/s/1_xEszoPjFIyic61FbFo2cg</a><br>提取码：x6v8</p>
<h4 id="2-安装前："><a href="#2-安装前：" class="headerlink" title="2.安装前："></a>2.安装前：</h4><p>检验是否配置jdk ctrl+R 运行cmd 分别输入java，javac， java -version （java 和 -version 之间有空格）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Windows\System32&gt;javac</span><br><span class="line">&#x27;javac&#x27; 不是内部或外部命令，也不是可运行的程序</span><br><span class="line">或批处理文件。</span><br><span class="line">C:\Windows\System32&gt;java -version</span><br><span class="line">&#x27;java&#x27; 不是内部或外部命令，也不是可运行的程序</span><br><span class="line">或批处理文件。</span><br></pre></td></tr></table></figure>



<h3 id="二、方法-步骤"><a href="#二、方法-步骤" class="headerlink" title="二、方法/步骤"></a>二、方法/步骤</h3><span id="more"></span>
<h4 id="1-安装JDK，JRE，-选择安装目录"><a href="#1-安装JDK，JRE，-选择安装目录" class="headerlink" title="1. 安装JDK，JRE， 选择安装目录"></a>1. 安装JDK，JRE， 选择安装目录</h4><p>安装过程中会出现两次 安装提示 。第一次是安装 jdk ，第二次是安装 jre 。建议两个都安装在同一个java文件夹中的不同文件夹中。（不能都安装在java文件夹的根目录下，jdk和jre安装在同一文件夹会出错）。</p>
<p>（1）双击jdk-8u202-windows-x64.exe 进行安装。</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-1.png"></p>
<p>（2）点击“下一步”继续。</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-2.png"></p>
<p>（3）选择安装路径，然后点击下一步。</p>
<p>默认是在C盘。我这里选择的是E盘。路径为：E:\Java\jdk1.8.0_202\</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-3.png"></p>
<p>（4）中途会进行JRE的安装。选择JRE安装的路径，点击下一步。默认会选择C盘。</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-4.png"> </p>
<p>因为在选择的时候不能新建。自己新建一个文件夹：jre1.8.0_202文件夹。更改路径：E:\Java\jre1.8.0_202\</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-5.png"> </p>
<p>（5）点击下一步，等待安装完成。</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-6.png"> </p>
<p>（6）安装完成，点击关闭。</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-7.png"></p>
<h4 id="2-配置系统环境"><a href="#2-配置系统环境" class="headerlink" title="2.配置系统环境"></a>2.配置系统环境</h4><p>配置环境变量：右击“我的电脑”–&gt;”属性”–&gt;”高级系统设置”–&gt;”环境变量”。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CLASSPATH</span><br><span class="line">.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar</span><br><span class="line"></span><br><span class="line">JAVA_HOME</span><br><span class="line">E:\Java\jdk1.8.0_202</span><br><span class="line"></span><br><span class="line">PATH</span><br><span class="line">%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin</span><br></pre></td></tr></table></figure>

<p><strong>（1）JAVA_HOME环境变量。</strong></p>
<p>作用：它指向jdk的安装目录，Eclipse/NetBeans/Tomcat等软件就是通过搜索JAVA_HOME变量来找到并使用安装好的jdk。<br>配置方法：在系统变量里点击新建，变量名填写JAVA_HOME，变量值填写JDK的安装路径。（根据自己的安装路径填写）</p>
<p>JAVA_HOME：E:\Java\jdk1.8.0_202</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-8.png"></p>
<p><strong>（2）CLASSPATH环境变量。</strong></p>
<p>作用：是指定类搜索路径，要使用已经编写好的类，前提当然是能够找到它们了，JVM就是通过CLASSPTH来寻找类的。我们需要把jdk安装目录下的lib子目录中的dt.jar和tools.jar设置到CLASSPATH中，当然，当前目录“.”也必须加入到该变量中。<br>配置方法：<br>新建CLASSPATH变量，变量值为：.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\tools.jar 。CLASSPATH变量名字，可以大写也可以小写。注意不要忘记前面的点和中间的分号。且要在英文输入的状态下的分号和逗号。</p>
<p>CLASSPATH ：.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-9.png"> </p>
<p><strong>（3）path环境变量</strong></p>
<p>作用：指定命令搜索路径，在i命令行下面执行命令如javac编译java程序时，它会到PATH变量所指定的路径中查找看是否能找到相应的命令程序。我们需要把jdk安装目录下的bin目录增加到现有的PATH变量中，bin目录中包含经常要用到的可执行文件如javac/java/javadoc等待，设置好PATH变量后，就可以在任何目录下执行javac/java等工具了。</p>
<p>在系统变量里找到Path变量，这是系统自带的，不用新建。双击Path，由于原来的变量值已经存在，故应在已有的<strong>变量前追加</strong>上“;%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin”。注意前面的分号。</p>
<p>Path：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-10.png"> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-11.png"> </p>
<p>或者：</p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-12.png"> </p>
<p>然后点击确定完成。</p>
<h3 id="三、-测试环境。"><a href="#三、-测试环境。" class="headerlink" title="三、 测试环境。"></a>三、 测试环境。</h3><p>检验是否配置成功 ctrl+R 运行cmd 分别输入<code>java</code>，<code>javac</code>， <code>java -version</code>。</p>
<p><strong>1.Java</strong></p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-13.png"> </p>
<p><strong>2.Javac</strong></p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-14.png"></p>
<p><strong>3.java –version</strong></p>
<p> <img src="/2021/11/17/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BJDK%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE/image-15.png"> </p>
<p>若如图所示 显示版本信息 则说明安装和配置成功!</p>
]]></content>
  </entry>
  <entry>
    <title>MapReduceClass extends Configured implements Tool代码</title>
    <url>/2021/12/18/hadoop-Interface-Tool/</url>
    <content><![CDATA[<p>在hdfs上运行jar包执行MapReduce程序时，要实现Tool接口，记录实现接口的MR程序代码，方便自己使用：</p>
<ol>
<li>extends Configured implements Tool</li>
<li>run()放置任务代码.以下为mapreduce代码</li>
<li>main方法中调用run()方法</li>
</ol>
<span id="more"></span>

<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//标出大概流程，具体实现代码省略</span><br><span class="line">//Tool接口可以支持处理通用的命令行选项，它是所有Map-Reduce程序的都可用的一个标准接口</span><br><span class="line"></span><br><span class="line">public class WordCount extends Configured implements Tool &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">    </span><br><span class="line">    	// 让ToolRunner执行</span><br><span class="line">    	System.exit(ToolRunner.run(new Configuration(), new WordCount(),args));</span><br><span class="line">    				//ToolRunner.run(new Configuration(),new ClassName(),参数args)</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">    public int run(String[] strings) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        //ToolRunner要处理的Configuration，Tool通过ToolRunner调用ToolRunner.run时，传入参数Configuration</span><br><span class="line">        Configuration conf = getConf();</span><br><span class="line">        //根据需要设置configuration</span><br><span class="line">        //conf.set(key,value);</span><br><span class="line">        </span><br><span class="line">        //获取job</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;word count&quot;);</span><br><span class="line"></span><br><span class="line">        // 2 设置jar</span><br><span class="line">        job.setJarByClass(WordCount.class);</span><br><span class="line"></span><br><span class="line">        // 3 设置mapper和reducer</span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">        // 4 设置map的输出类型</span><br><span class="line">        job.setMapOutputKeyClass(Access.class);//key1</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);//value1</span><br><span class="line"></span><br><span class="line">        // 5 设置reduce的输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);//key2</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);//value2</span><br><span class="line"></span><br><span class="line">        //设置分区</span><br><span class="line">        //job.setPartitionerClass(MyPartitioner.class);</span><br><span class="line">        //job.setNumReduceTasks(4);</span><br><span class="line"></span><br><span class="line">        // 6 设置输入输出</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(input));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(output));</span><br><span class="line"></span><br><span class="line">        // 7 提交作业</span><br><span class="line">        boolean result = job.waitForCompletion(true);</span><br><span class="line">        return result?0:1;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class MyMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">                                                                    //key1,	value1</span><br><span class="line">    @Override			</span><br><span class="line">    public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    	.</span><br><span class="line">    	.</span><br><span class="line">    	.</span><br><span class="line">        context.write(word, one);							//context.write(key1,value1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static class MyReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    												&lt;key1,	value1, key2, value2&gt;</span><br><span class="line">    @Override	//reduce(key1,value1,key2,value2)</span><br><span class="line">    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    	.</span><br><span class="line">    	.</span><br><span class="line">    	.</span><br><span class="line">      context.write(key, result);//context.write(key2,value2)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>补充：自定义序列化类</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">自定义序列化类的实现步骤：</span><br><span class="line">* 1）implements Writable</span><br><span class="line">* 2）必须要有无参构造</span><br><span class="line">* 3）实现write和readFields方法</span><br><span class="line">* 4）这两个方法中的字段顺序一定要一致</span><br><span class="line">* 5) optional:  toString</span><br></pre></td></tr></table></figure></li>
</ol>
<p>参考链接：</p>
<p><a href="https://dzone.com/articles/using-libjars-option-hadoop">https://dzone.com/articles/using-libjars-option-hadoop</a></p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/09/18/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>什么是尾递归</title>
    <url>/2021/11/19/%E4%BB%80%E4%B9%88%E6%98%AF%E5%B0%BE%E9%80%92%E5%BD%92/</url>
    <content><![CDATA[<h3 id="尾递归"><a href="#尾递归" class="headerlink" title="尾递归"></a>尾递归</h3><h4 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h4><p>​    如果一个函数中所有递归形式的调用都出现在函数的末尾，我们称这个递归函数是尾递归的。当递归调用是整个函数体中最后执行的语句且<strong>它的返回值不属于表达式的一部分</strong>时，这个递归调用就是尾递归。尾递归函数的特点是在<strong>回归过程中不用做任何操作</strong>，这个特性很重要，因为大多数现代的编译器会利用这种特点自动生成优化的代码。</p>
<span id="more"></span>

<h4 id="二、实例"><a href="#二、实例" class="headerlink" title="二、实例"></a>二、实例</h4><p>以递归的形式计算阶乘：</p>
<p>线性递归:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">Rescuvie</span><span class="params">( <span class="keyword">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (n == <span class="number">1</span>) ? <span class="number">1</span> : n * Rescuvie(n - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>尾递归:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">TailRescuvie</span><span class="params">( <span class="keyword">long</span> n, <span class="keyword">long</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (n == <span class="number">1</span>) ? a : TailRescuvie(n - <span class="number">1</span>, a * n); </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">TailRescuvie</span><span class="params">( <span class="keyword">long</span> n)</span> </span>&#123;<span class="comment">//封装用的</span></span><br><span class="line">    <span class="keyword">return</span> (n == <span class="number">0</span>) ? <span class="number">1</span> : TailRescuvie(n, <span class="number">1</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当n = 5时<br>对于传统线性递归, 他的递归过程如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Rescuvie(5)</span><br><span class="line"></span><br><span class="line">&#123;5 * Rescuvie(4)&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * Rescuvie(3)&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * &#123;3 * Rescuvie(2)&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * &#123;3 * &#123;2 * Rescuvie(1)&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * &#123;3 * &#123;2 * 1&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * &#123;3 * 2&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * &#123;4 * 6&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;5 * 24&#125;</span><br><span class="line"></span><br><span class="line">120</span><br></pre></td></tr></table></figure>

<p>对于尾递归, 他的递归过程如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TailRescuvie(5)                  // 所以在运算上和内存占用上节省了很多,直接传回结果</span><br><span class="line"></span><br><span class="line">TailRescuvie(5, 1)                         return 120</span><br><span class="line">                                                 ↑</span><br><span class="line">TailRescuvie(4, 5)                         return 120</span><br><span class="line">                                                 ↑</span><br><span class="line">TailRescuvie(3, 20)                        return 120</span><br><span class="line">                                                 ↑</span><br><span class="line">TailRescuvie(2, 60)                        return 120</span><br><span class="line">                                                 ↑</span><br><span class="line">TailRescuvie(1, 120)                       return 120</span><br><span class="line">                                                 ↑</span><br><span class="line">120                                //当运行到最后时,return a =&gt; return 120 ,将120返回上一级</span><br></pre></td></tr></table></figure>

<p>说明：其实尾递归也需要下层往上层返回结果，但在返回的过程中不用再做计算，依次返回结果即可。从上可以看到尾递归把返回结果放到了调用的参数里。这个细小的变化导致，TailRescuvie(n)不必像以前一样，非要等到拿到了TailRescuvie(n-1)的返回值，才能计算它自己的返回结果,它完全就等于TailRescuvie(n-1)的返回值。因此理论上：TailRescuvie(n)在调用tailTailRescuvie(n-1)前，完全就可以先销毁自己放在栈上的东西。</p>
<h4 id="三、优势"><a href="#三、优势" class="headerlink" title="三、优势"></a>三、优势</h4><p>​    与普通递归相比，由于尾递归的调用处于方法的最后，因此方法之前所积累下的各种状态对于递归调用结果已经没有任何意义，因此每一个函数在调用下一个函数之前，都能做到先把当前自己占用的栈给先释放了，尾递归的调用链上可以做到只有一个函数在使用栈，因此可以无限地调用！</p>
<p>​    但是，上述的优化是在<strong>某些语言</strong>编译器的优化支持上实现的，尾递归本身并不能消除函数调用栈过长的问题。在一般递归函数func()中，func(n)是依赖于 func(n-1) 的，func(n) 只有在得到 func(n-1) 的结果之后，才能计算它自己的返回值，因此理论上，在 func(n-1) 返回之前，func(n)，不能结束返回。因此func(n)就必须保留它在栈上的数据，直到func(n-1)先返回，而尾递归的实现则可以在编译器的帮助下，消除这个限制。</p>
<h4 id="四、尾递归的调用栈优化特性"><a href="#四、尾递归的调用栈优化特性" class="headerlink" title="四、尾递归的调用栈优化特性"></a>四、尾递归的调用栈优化特性</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int tail_func(int n, int res)&#123;</span><br><span class="line">     if (n &lt;= 1) return res;</span><br><span class="line">     return tail_func(n - 1, n * res);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    int dummy[1024*1024]; // 尽可能占用栈。</span><br><span class="line">    tail_func(2048*2048, 1);</span><br><span class="line">    return 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​    上面这个程序在开了编译优化和没开编译优化的情况下编出来的结果是<strong>不一样</strong>的，如果不开启优化，直接 <code>gcc -o tr func_tail.c</code> 编译然后运行的话，程序会爆栈崩溃，但如果开优化的话：<code>gcc -o tr -O2 func_tail.c</code>，上面的程序最后就能正常运行。 这里面的原因就在于，尾递归的写法只是具备了使当前函数在调用下一个函数前把当前占有的栈销毁，但是会不会真的这样做，是要具体看编译器是否最终这样做，如果在语言层面上，没有规定要优化这种尾调用，那编译器就可以有自己的选择来做不同的实现，在这种情况下，尾递归就不一定能解决一般递归的问题。</p>
<p>参考链接:</p>
<p><a href="https://blog.csdn.net/h330531987/article/details/76218956">什么是尾递归,尾递归的优势以及语言支持情况说明</a></p>
<p><a href="https://www.cnblogs.com/catch/p/3495450.html">说说尾递归</a></p>
]]></content>
  </entry>
  <entry>
    <title>将Hexo部署到GitHub</title>
    <url>/2021/10/24/%E5%B0%86Hexo%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/</url>
    <content><![CDATA[<p>参考<a href="https://hexo.io/zh-cn/docs">Hexo文档</a> </p>
<span id="more"></span>

<h2 id="一、安装Git"><a href="#一、安装Git" class="headerlink" title="一、安装Git"></a>一、安装Git</h2><ol>
<li><p><a href="https://gitforwindows.org/">gitforwindows</a> 下载安装</p>
</li>
<li><p>安装完查看git版本：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git --version</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="二、安装Node-js"><a href="#二、安装Node-js" class="headerlink" title="二、安装Node.js"></a>二、安装Node.js</h2><ol>
<li><p><a href="https://nodejs.org/en/download/">node.js</a> 选择LTS的window版本，下载安装</p>
</li>
<li><p>安装完查看node.js版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure></li>
<li><p>更改npm镜像源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org/</span><br></pre></td></tr></table></figure></li>
<li><p>查看npm镜像源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm get registry https://registry.npm.taobao.org/</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="三、安装Hexo"><a href="#三、安装Hexo" class="headerlink" title="三、安装Hexo"></a>三、安装Hexo</h2><ol>
<li><p>安装Hexo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="四、建站"><a href="#四、建站" class="headerlink" title="四、建站"></a>四、建站</h2><ol>
<li><p>初始化博客目录</p>
<p>安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure></li>
<li><p>常用命令：</p>
<ul>
<li>清理缓存： <code>hexo clean</code></li>
<li>生成静态文件： <code>hexo g/generate</code></li>
<li>生成静态文件： <code>hexo s/server</code></li>
<li>组合版：<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</code></li>
<li>发布到github：<code>hexo d</code></li>
</ul>
<p>本地访问地址：<a href="http://localhost:4000/">http://localhost:4000</a></p>
</li>
<li><p>修改网站基本配置信息参考<a href="https://hexo.io/zh-cn/docs/configuration">配置</a></p>
</li>
</ol>
<h2 id="五、将hexo部署到GitHub"><a href="#五、将hexo部署到GitHub" class="headerlink" title="五、将hexo部署到GitHub"></a>五、将hexo部署到GitHub</h2><ol>
<li><p>生成SSH添加到GitHub</p>
<p>生成key，可以git部署网站</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</span><br></pre></td></tr></table></figure>

<p>然后需要配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.email “you@example.com”</span><br><span class="line">git config --global user.name “Your Name”</span><br></pre></td></tr></table></figure>

<p>将这个文件拷贝到git的<a href="https://github.com/settings/keys">https://github.com/settings/keys</a><br>查看是否配置成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure></li>
<li><p>GitHub创建个人仓库<br>新建一个 repository。如果你希望你的站点能通过域名 <code>&lt;你的 GitHub 用户名&gt;.github.io</code> 访问，你的 repository 应该直接命名为 <code>&lt;你的 GitHub 用户名&gt;.github.io</code>。</p>
</li>
<li><p>安装 hexo-deployer-git.：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li>
<li><p>在 _config.yml（如果有已存在的请删除）添加如下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:  </span><br><span class="line">    type: git  </span><br><span class="line">    repo: https://github.com/&lt;username&gt;/&lt;project&gt; </span><br></pre></td></tr></table></figure></li>
<li><p>运行 hexo </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clean &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure></li>
<li><p>查看 <code>username.github.io</code> 上的网页是否部署成功。</p>
</li>
</ol>
<h2 id="六、发布文章"><a href="#六、发布文章" class="headerlink" title="六、发布文章"></a>六、发布文章</h2><ol>
<li><p>创建文章</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new post &quot;title&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>然后用编辑器修改好文本，发布</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</span><br><span class="line">hexo clean &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>画图详解yarn的资源调度流程</title>
    <url>/2021/12/08/%E7%94%BB%E5%9B%BE%E8%AF%A6%E8%A7%A3yarn%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p><img src="/2021/12/08/%E7%94%BB%E5%9B%BE%E8%AF%A6%E8%A7%A3yarn%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B/mr_on_yarn.png" alt="mr_on_yarn"></p>
<h3 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h3><ul>
<li>Client调用job.waitForCompletion⽅法，向整个集群提交MapReduce作业。</li>
<li>Client向RM申请一个作业id。</li>
<li>RM给Client返回该job资源的提交路径和作业id。</li>
<li>Client提交jar包、切⽚信息和配置文件到指定的资源提交路径。</li>
<li>Client提交完资源后，向RM申请运行MrAppMaster。</li>
</ul>
<h3 id="作业初始化"><a href="#作业初始化" class="headerlink" title="作业初始化"></a>作业初始化</h3><ul>
<li>当RM收到Client的请求后，将该job添加到容量调度器中。</li>
<li>某⼀个空闲的NM领取到该Job。</li>
<li>该NM创建Container，并产生MRAppmaster。</li>
<li>下载Client提交的资源到本地。</li>
</ul>
<h3 id="任务分配"><a href="#任务分配" class="headerlink" title="任务分配"></a>任务分配</h3><ul>
<li>MrAppMaster向RM申请运行多个MapTask任务资源。</li>
<li>RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</li>
</ul>
<h3 id="任务运行"><a href="#任务运行" class="headerlink" title="任务运行"></a>任务运行</h3><ul>
<li>MrAppMaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</li>
<li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</li>
<li>ReduceTask向MapTask获取相应分区的数据。</li>
<li>程序运行完毕后，MrAppMaster会向RM申请注销⾃己。</li>
</ul>
<h3 id="进度和状态更新"><a href="#进度和状态更新" class="headerlink" title="进度和状态更新"></a>进度和状态更新</h3><p>YARN中的任务将其进度和状态返回给应⽤管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应⽤管理器请求进度更新, 展示给用户。</p>
<h3 id="作业完成"><a href="#作业完成" class="headerlink" title="作业完成"></a>作业完成</h3><p>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<p>原文连接：<a href="https://www.cnblogs.com/kyle-blog/p/14222496.html">https://www.cnblogs.com/kyle-blog/p/14222496.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>简明 VIM 练级攻略（转载）</title>
    <url>/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
    <content><![CDATA[<p>下面的文章翻译自《<a href="http://yannesposito.com/Scratch/en/blog/Learn-Vim-Progressively/">Learn Vim Progressively</a>》，我觉得这是给新手最好的VIM的升级教程了，没有列举所有的命令，只是列举了那些最有用的命令。非常不错。</p>
<hr>
<p>你想以最快的速度学习人类史上最好的文本编辑器VIM吗？你先得懂得如何在VIM幸存下来，然后一点一点地学习各种戏法。</p>
<p><a href="https://www.vim.org/">Vim</a> the Six Billion Dollar editor</p>
<blockquote>
<p>Better, Stronger, Faster.</p>
</blockquote>
<p>学习 <a href="https://www.vim.org/">vim</a> 并且其会成为你最后一个使用的文本编辑器。没有比这个更好的文本编辑器了，非常地难学，但是却不可思议地好用。</p>
<p>我建议下面这四个步骤：</p>
<ol>
<li>存活</li>
<li>感觉良好</li>
<li>觉得更好，更强，更快</li>
<li>使用VIM的超能力</li>
</ol>
<p>当你走完这篇文章，你会成为一个vim的 superstar。</p>
<p>在开始学习以前，我需要给你一些警告：</p>
<ul>
<li>学习vim在开始时是痛苦的。</li>
<li>需要时间</li>
<li>需要不断地练习，就像你学习一个乐器一样。</li>
<li>不要期望你能在3天内把vim练得比别的编辑器更有效率。</li>
<li>事实上，你需要2周时间的苦练，而不是3天。</li>
</ul>
<span id="more"></span>

<h4 id="第一级-–-存活"><a href="#第一级-–-存活" class="headerlink" title="第一级 – 存活"></a>第一级 – 存活</h4><ol>
<li>安装 <a href="https://www.vim.org/">vim</a></li>
<li>启动 vim</li>
<li><strong>什么也别干！</strong>请先阅读</li>
</ol>
<p>当你安装好一个编辑器后，你一定会想在其中输入点什么东西，然后看看这个编辑器是什么样子。但vim不是这样的，请按照下面的命令操作：</p>
<ul>
<li>启 动Vim后，vim在 <em>Normal</em> 模式下。</li>
<li>让我们进入 <em>Insert</em> 模式，请按下键 i 。(陈皓注：你会看到vim左下角有一个–insert–字样，表示，你可以以插入的方式输入了）</li>
<li>此时，你可以输入文本了，就像你用“记事本”一样。</li>
<li>如果你想返回 <em>Normal</em> 模式，请按 <code>ESC</code> 键。</li>
</ul>
<p>现在，你知道如何在 <em>Insert</em> 和 <em>Normal</em> 模式下切换了。下面是一些命令，可以让你在 <em>Normal</em> 模式下幸存下来：</p>
<blockquote>
<ul>
<li><code>i</code> → <em>Insert</em> 模式，按 <code>ESC</code> 回到 <em>Normal</em> 模式.</li>
<li><code>x</code> → 删当前光标所在的一个字符。</li>
<li><code>:wq</code> → 存盘 + 退出 (<code>:w</code> 存盘, <code>:q</code> 退出)  （陈皓注：:w 后可以跟文件名）</li>
<li><code>dd</code> → 删除当前行，并把删除的行存到剪贴板里</li>
<li><code>p</code> → 粘贴剪贴板</li>
</ul>
<p><strong>推荐</strong>:</p>
<ul>
<li><code>hjkl</code> (强例推荐使用其移动光标，但不必需) →你也可以使用光标键 (←↓↑→). 注: <code>j</code> 就像下箭头。</li>
<li><code>:help &lt;command&gt;</code> → 显示相关命令的帮助。你也可以就输入 <code>:help</code> 而不跟命令。（陈皓注：退出帮助需要输入:q）</li>
</ul>
</blockquote>
<p>你能在vim幸存下来只需要上述的那5个命令，你就可以编辑文本了，你一定要把这些命令练成一种下意识的状态。于是你就可以开始进阶到第二级了。</p>
<p>当是，在你进入第二级时，需要再说一下 <em>Normal</em> 模式。在一般的编辑器下，当你需要copy一段文字的时候，你需要使用 <code>Ctrl</code> 键，比如：<code>Ctrl-C</code>。也就是说，Ctrl键就好像功能键一样，当你按下了功能键Ctrl后，C就不在是C了，而且就是一个命令或是一个快键键了，<strong>在VIM的Normal模式下，所有的键就是功能键了</strong>。这个你需要知道。</p>
<p>标记:</p>
<ul>
<li>下面的文字中，如果是 <code>Ctrl-λ</code>我会写成 <code>&lt;C-λ&gt;</code>.</li>
<li>以 <code>:</code> 开始的命令你需要输入 <code>&lt;enter&gt;</code>回车，例如 — 如果我写成 <code>:q</code> 也就是说你要输入 <code>:q&lt;enter&gt;</code>.</li>
</ul>
<h4 id="第二级-–-感觉良好"><a href="#第二级-–-感觉良好" class="headerlink" title="第二级 – 感觉良好"></a>第二级 – 感觉良好</h4><p>上面的那些命令只能让你存活下来，现在是时候学习一些更多的命令了，下面是我的建议：（陈皓注：所有的命令都需要在Normal模式下使用，如果你不知道现在在什么样的模式，你就狂按几次ESC键）</p>
<ol>
<li><h5 id="各种插入模式"><a href="#各种插入模式" class="headerlink" title="各种插入模式"></a>各种插入模式</h5><blockquote>
<ul>
<li><code>a</code> → 在光标后插入</li>
<li><code>o</code> → 在当前行后插入一个新行</li>
<li><code>O</code> → 在当前行前插入一个新行</li>
<li><code>cw</code> → 替换从光标所在位置后到一个单词结尾的字符</li>
</ul>
</blockquote>
</li>
<li><h5 id="简单的移动光标"><a href="#简单的移动光标" class="headerlink" title="简单的移动光标"></a>简单的移动光标</h5><blockquote>
<ul>
<li><code>0</code> → 数字零，到行头</li>
<li><code>^</code> → 到本行第一个不是blank字符的位置（所谓blank字符就是空格，tab，换行，回车等）</li>
<li><code>$</code> → 到本行行尾</li>
<li><code>g_</code> → 到本行最后一个不是blank字符的位置。</li>
<li><code>/pattern</code> → 搜索 <code>pattern</code> 的字符串（陈皓注：如果搜索出多个匹配，可按n键到下一个）</li>
</ul>
</blockquote>
</li>
<li><h5 id="拷贝-粘贴"><a href="#拷贝-粘贴" class="headerlink" title="拷贝/粘贴"></a>拷贝/粘贴</h5><p>（陈皓注：p/P都可以，p是表示在当前位置之后，P表示在当前位置之前）</p>
<blockquote>
<ul>
<li><code>P</code> → 粘贴</li>
<li><code>yy</code> → 拷贝当前行当行于 <code>ddP</code></li>
</ul>
</blockquote>
</li>
<li><h5 id="Undo-Redo"><a href="#Undo-Redo" class="headerlink" title="Undo/Redo"></a>Undo/Redo</h5><blockquote>
<ul>
<li><code>u</code> → undo</li>
<li><code>&lt;C-r&gt;</code> → redo</li>
</ul>
</blockquote>
</li>
<li><h5 id="打开-保存-退出-改变文件-Buffer"><a href="#打开-保存-退出-改变文件-Buffer" class="headerlink" title="打开/保存/退出/改变文件(Buffer)"></a>打开/保存/退出/改变文件(Buffer)</h5><blockquote>
<ul>
<li><code>:e &lt;path/to/file&gt;</code> → 打开一个文件</li>
<li><code>:w</code> → 存盘</li>
<li><code>:saveas &lt;path/to/file&gt;</code> → 另存为 <code>&lt;path/to/file&gt;</code></li>
<li><code>:x</code>， <code>ZZ</code> 或 <code>:wq</code> → 保存并退出 (<code>:x</code> 表示仅在需要时保存，ZZ不需要输入冒号并回车)</li>
<li><code>:q!</code> → 退出不保存 <code>:qa!</code> 强行退出所有的正在编辑的文件，就算别的文件有更改。</li>
<li><code>:bn</code> 和 <code>:bp</code> → 你可以同时打开很多文件，使用这两个命令来切换下一个或上一个文件。（陈皓注：我喜欢使用:n到下一个文件）</li>
</ul>
</blockquote>
</li>
</ol>
<p>花点时间熟悉一下上面的命令，一旦你掌握他们了，你就几乎可以干其它编辑器都能干的事了。但是到现在为止，你还是觉得使用vim还是有点笨拙，不过没关系，你可以进阶到第三级了。</p>
<h4 id="第三级-–-更好，更强，更快"><a href="#第三级-–-更好，更强，更快" class="headerlink" title="第三级 – 更好，更强，更快"></a>第三级 – 更好，更强，更快</h4><p>先恭喜你！你干的很不错。我们可以开始一些更为有趣的事了。在第三级，我们只谈那些和vi可以兼容的命令。</p>
<h5 id="更好"><a href="#更好" class="headerlink" title="更好"></a>更好</h5><p>下面，让我们看一下vim是怎么重复自己的：</p>
<ol>
<li><code>.</code> → (小数点) 可以重复上一次的命令</li>
<li>N<command> → 重复某个命令N次</li>
</ol>
<p>下面是一个示例，找开一个文件你可以试试下面的命令：</p>
<blockquote>
<ul>
<li><code>2dd</code> → 删除2行</li>
<li><code>3p</code> → 粘贴文本3次</li>
<li><code>100idesu [ESC]</code> → 会写下 “desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu “</li>
<li><code>.</code> → 重复上一个命令—— 100 “desu “.</li>
<li><code>3.</code> → 重复 3 次 “desu” (注意：不是 300，你看，VIM多聪明啊).</li>
</ul>
</blockquote>
<h5 id="更强"><a href="#更强" class="headerlink" title="更强"></a>更强</h5><p>你要让你的光标移动更有效率，你一定要了解下面的这些命令，<strong>千万别跳过</strong>。</p>
<ol>
<li><p>N<code>G</code> → 到第 N 行 （陈皓注：注意命令中的G是大写的，另我一般使用 : N 到第N行，如 :137 到第137行）</p>
</li>
<li><p><code>gg</code> → 到第一行。（陈皓注：相当于1G，或 :1）</p>
</li>
<li><p><code>G</code> → 到最后一行。</p>
</li>
<li><p>按单词移动：</p>
<blockquote>
<ol>
<li><code>w</code> → 到下一个单词的开头。</li>
<li><code>e</code> → 到下一个单词的结尾。</li>
</ol>
<p>&gt; 如果你认为单词是由默认方式，那么就用小写的e和w。默认上来说，一个单词由字母，数字和下划线组成（陈皓注：程序变量）</p>
<p>&gt; 如果你认为单词是由blank字符分隔符，那么你需要使用大写的E和W。（陈皓注：程序语句）</p>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/word_moves.jpg" alt="Word moves example"></p>
</blockquote>
</li>
</ol>
<p>下面，让我来说说最强的光标移动：</p>
<blockquote>
<ul>
<li><code>%</code> : 匹配括号移动，包括 <code>(</code>, <code>&#123;</code>, <code>[</code>. （陈皓注：你需要把光标先移到括号上）</li>
<li><code>*</code> 和 <code>#</code>:  匹配光标当前所在的单词，移动光标到下一个（或上一个）匹配单词（*是下一个，#是上一个）</li>
</ul>
</blockquote>
<p>相信我，上面这三个命令对程序员来说是相当强大的。</p>
<h5 id="更快"><a href="#更快" class="headerlink" title="更快"></a>更快</h5><p>你一定要记住光标的移动，因为很多命令都可以和这些移动光标的命令连动。很多命令都可以如下来干：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;start position&gt;&lt;command&gt;&lt;end position&gt;</span><br></pre></td></tr></table></figure>

<p>例如 <code>0y$</code> 命令意味着：</p>
<ul>
<li><code>0</code> → 先到行头</li>
<li><code>y</code> → 从这里开始拷贝</li>
<li><code>$</code> → 拷贝到本行最后一个字符</li>
</ul>
<p>你可可以输入 <code>ye</code>，从当前位置拷贝到本单词的最后一个字符。</p>
<p>你也可以输入 <code>y2/foo</code> 来拷贝2个 “foo” 之间的字符串。</p>
<p>还有很多时间并不一定你就一定要按y才会拷贝，下面的命令也会被拷贝：</p>
<ul>
<li><code>d</code> (删除 )</li>
<li><code>v</code> (可视化的选择)</li>
<li><code>gU</code> (变大写)</li>
<li><code>gu</code> (变小写)</li>
<li>等等</li>
</ul>
<p>（陈皓注：可视化选择是一个很有意思的命令，你可以先按v，然后移动光标，你就会看到文本被选择，然后，你可能d，也可y，也可以变大写等）</p>
<h4 id="第四级-–-Vim-超能力"><a href="#第四级-–-Vim-超能力" class="headerlink" title="第四级 – Vim 超能力"></a>第四级 – Vim 超能力</h4><p>你只需要掌握前面的命令，你就可以很舒服的使用VIM了。但是，现在，我们向你介绍的是VIM杀手级的功能。下面这些功能是我只用vim的原因。</p>
<h5 id="在当前行上移动光标-0-f-F-t-T"><a href="#在当前行上移动光标-0-f-F-t-T" class="headerlink" title="在当前行上移动光标: 0 ^ $ f F t T , ;"></a>在当前行上移动光标: <code>0</code> <code>^</code> <code>$</code> <code>f</code> <code>F</code> <code>t</code> <code>T</code> <code>,</code> <code>;</code></h5><blockquote>
<ul>
<li><code>0</code> → 到行头</li>
<li><code>^</code> → 到本行的第一个非blank字符</li>
<li><code>$</code> → 到行尾</li>
<li><code>g_</code> → 到本行最后一个不是blank字符的位置。</li>
<li><code>fa</code> → 到下一个为a的字符处，你也可以fs到下一个为s的字符。</li>
<li><code>t,</code> → 到逗号前的第一个字符。逗号可以变成其它字符。</li>
<li><code>3fa</code> → 在当前行查找第三个出现的a。</li>
<li><code>F</code> 和 <code>T</code> → 和 <code>f</code> 和 <code>t</code> 一样，只不过是相反方向。<br><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/line_moves.jpg" alt="Line moves"></li>
</ul>
</blockquote>
<p>还有一个很有用的命令是 <code>dt&quot;</code> → 删除所有的内容，直到遇到双引号—— <code>&quot;。</code></p>
<h5 id="区域选择-lt-action-gt-a-lt-object-gt-或-lt-action-gt-i-lt-object-gt"><a href="#区域选择-lt-action-gt-a-lt-object-gt-或-lt-action-gt-i-lt-object-gt" class="headerlink" title="区域选择 &lt;action&gt;a&lt;object&gt; 或 &lt;action&gt;i&lt;object&gt;"></a>区域选择 <code>&lt;action&gt;a&lt;object&gt;</code> 或 <code>&lt;action&gt;i&lt;object&gt;</code></h5><p>在visual 模式下，这些命令很强大，其命令格式为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;action&gt;a&lt;object&gt;` 和 `&lt;action&gt;i&lt;object&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>action可以是任何的命令，如 <code>d</code> (删除), <code>y</code> (拷贝), <code>v</code> (可以视模式选择)。</li>
<li>object 可能是： <code>w</code> 一个单词， <code>W</code> 一个以空格为分隔的单词， <code>s</code> 一个句字， <code>p</code> 一个段落。也可以是一个特别的字符：<code>&quot;、</code> <code>&#39;、</code> <code>)、</code> <code>&#125;、</code> <code>]。</code></li>
</ul>
<p>假设你有一个字符串 <code>(map (+) (&quot;foo&quot;))</code>.而光标键在第一个 <code>o </code>的位置。</p>
<blockquote>
<ul>
<li><code>vi&quot;</code> → 会选择 <code>foo</code>.</li>
<li><code>va&quot;</code> → 会选择 <code>&quot;foo&quot;</code>.</li>
<li><code>vi)</code> → 会选择 <code>&quot;foo&quot;</code>.</li>
<li><code>va)</code> → 会选择<code>(&quot;foo&quot;)</code>.</li>
<li><code>v2i)</code> → 会选择 <code>map (+) (&quot;foo&quot;)</code></li>
<li><code>v2a)</code> → 会选择 <code>(map (+) (&quot;foo&quot;))</code></li>
</ul>
</blockquote>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/textobjects.png" alt="Text objects selection"></p>
<h5 id="块操作-lt-C-v-gt"><a href="#块操作-lt-C-v-gt" class="headerlink" title="块操作: &lt;C-v&gt;"></a>块操作: <code>&lt;C-v&gt;</code></h5><p>块操作，典型的操作： <code>0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC]</code></p>
<ul>
<li><code>^</code> → 到行头</li>
<li><code>&lt;C-v&gt;</code> → 开始块操作</li>
<li><code>&lt;C-d&gt;</code> → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的)</li>
<li><code>I-- [ESC]</code> → I是插入，插入“<code>--</code>”，按ESC键来为每一行生效。</li>
</ul>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/rectangular-blocks.gif" alt="Rectangular blocks"></p>
<p>在Windows下的vim，你需要使用 <code>&lt;C-q&gt;</code> 而不是 <code>&lt;C-v&gt;</code> ，<code>&lt;C-v&gt;</code> 是拷贝剪贴板。</p>
<h5 id="自动提示：-lt-C-n-gt-和-lt-C-p-gt"><a href="#自动提示：-lt-C-n-gt-和-lt-C-p-gt" class="headerlink" title="自动提示： &lt;C-n&gt; 和 &lt;C-p&gt;"></a>自动提示： <code>&lt;C-n&gt;</code> 和 <code>&lt;C-p&gt;</code></h5><p>在 Insert 模式下，你可以输入一个词的开头，然后按 <code>&lt;C-p&gt;或是&lt;C-n&gt;，自动补齐功能就出现了……</code></p>
<p>``<img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/completion.gif" alt="Completion"></p>
<h5 id="宏录制：-qa-操作序列-q-a"><a href="#宏录制：-qa-操作序列-q-a" class="headerlink" title="宏录制： qa 操作序列 q, @a, @@"></a>宏录制： <code>qa</code> 操作序列 <code>q</code>, <code>@a</code>, <code>@@</code></h5><ul>
<li><code>qa</code> 把你的操作记录在寄存器 <code>a。</code></li>
<li>于是 <code>@a</code> 会replay被录制的宏。</li>
<li><code>@@</code> 是一个快捷键用来replay最新录制的宏。</li>
</ul>
<blockquote>
<p>*<strong>示例*</strong></p>
<p>在一个只有一行且这一行只有“1”的文本中，键入如下命令：</p>
<ul>
<li><pre><code>qaYp&lt;C-a&gt;q
</code></pre>
<p>→</p>
<ul>
<li><code>qa</code> 开始录制</li>
<li><code>Yp</code> 复制行.</li>
<li><code>&lt;C-a&gt;</code> 增加1.</li>
<li><code>q</code> 停止录制.</li>
</ul>
</li>
<li><p><code>@a</code> → 在1下面写下 2</p>
</li>
<li><p><code>@@</code> → 在2 正面写下3</p>
</li>
<li><p>现在做 <code>100@@</code> 会创建新的100行，并把数据增加到 103.</p>
</li>
</ul>
</blockquote>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/macros.gif" alt="Macros"></p>
<h5 id="可视化选择：-v-V-lt-C-v-gt"><a href="#可视化选择：-v-V-lt-C-v-gt" class="headerlink" title="可视化选择： v,V,&lt;C-v&gt;"></a>可视化选择： <code>v</code>,<code>V</code>,<code>&lt;C-v&gt;</code></h5><p>前面，我们看到了 <code>&lt;C-v&gt;</code>的示例 （在Windows下应该是<C-q>），我们可以使用 <code>v</code> 和 <code>V</code>。一但被选好了，你可以做下面的事：</C-q></p>
<ul>
<li><code>J</code> → 把所有的行连接起来（变成一行）</li>
<li><code>&lt;</code> 或 <code>&gt;</code> → 左右缩进</li>
<li><code>=</code> → 自动给缩进 （陈皓注：这个功能相当强大，我太喜欢了）</li>
</ul>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/autoindent.gif" alt="Autoindent"></p>
<p>在所有被选择的行后加上点东西：</p>
<ul>
<li><code>&lt;C-v&gt;</code></li>
<li>选中相关的行 (可使用 <code>j</code> 或 <code>&lt;C-d&gt;</code> 或是 <code>/pattern</code> 或是 <code>%</code> 等……)</li>
<li><code>$</code> 到行最后</li>
<li><code>A</code>, 输入字符串，按 <code>ESC。</code></li>
</ul>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/append-to-many-lines.gif" alt="Append to many lines"></p>
<h5 id="分屏-split-和-vsplit"><a href="#分屏-split-和-vsplit" class="headerlink" title="分屏: :split 和 vsplit."></a>分屏: <code>:split</code> 和 <code>vsplit</code>.</h5><p>下面是主要的命令，你可以使用VIM的帮助 <code>:help split</code>. 你可以参考本站以前的一篇文章<a href="https://coolshell.cn/articles/1679.html">VIM分屏</a>。</p>
<blockquote>
<ul>
<li><code>:split</code> → 创建分屏 (<code>:vsplit</code>创建垂直分屏)</li>
<li><code>&lt;C-w&gt;&lt;dir&gt;</code> : dir就是方向，可以是 <code>hjkl</code> 或是 ←↓↑→ 中的一个，其用来切换分屏。</li>
<li><code>&lt;C-w&gt;_</code> (或 <code>&lt;C-w&gt;|</code>) : 最大化尺寸 (<C-w>| 垂直分屏)</C-w></li>
<li><code>&lt;C-w&gt;+</code> (或 <code>&lt;C-w&gt;-</code>) : 增加尺寸</li>
</ul>
</blockquote>
<p><img src="/2021/12/02/%E7%AE%80%E6%98%8E-VIM-%E7%BB%83%E7%BA%A7%E6%94%BB%E7%95%A5%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/split.gif" alt="Split"></p>
<h4 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h4><ul>
<li><p>上面是作者最常用的90%的命令。</p>
</li>
<li><p>我建议你每天都学1到2个新的命令。</p>
</li>
<li><p>在两到三周后，你会感到vim的强大的。</p>
</li>
<li><p>有时候，学习VIM就像是在死背一些东西。</p>
</li>
<li><p>幸运的是，vim有很多很不错的工具和优秀的文档。</p>
</li>
<li><p>运行vimtutor直到你熟悉了那些基本命令。</p>
</li>
<li><p>其在线帮助文档中你应该要仔细阅读的是 <code>:help usr_02.txt</code>.</p>
</li>
<li><p>你会学习到诸如 <code>!，</code> 目录，寄存器，插件等很多其它的功能。</p>
</li>
</ul>
<p>学习vim就像学弹钢琴一样，一旦学会，受益无穷。</p>
<p>原文地址：<a href="https://coolshell.cn/articles/5426.html">https://coolshell.cn/articles/5426.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>（WINDOWS）解决IDEA运行MapReduce程序没有日志问题</title>
    <url>/2021/12/16/%E8%A7%A3%E5%86%B3IDEA%E8%BF%90%E8%A1%8CMapReduce%E7%A8%8B%E5%BA%8F%E6%B2%A1%E6%9C%89%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><strong>问题</strong>：MapReduce项目 可以运行成功，但是控制台只有几条信息，说明项目没有配置log4j，在开发的过程中，我们需要更详细的日志信息来定位问题和查看整个过程。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.htrace.core.Tracer).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>

<p>项目打包成jar包，放到hdfs上运行，是有完整的日志信息的，可以看到整个MapRuduce的执行过程。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021-12-16 15:39:45,000 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-12-16 15:39:46,374 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-12-16 15:39:47,293 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.</span><br><span class="line">2021-12-16 15:39:47,317 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1639471863570_0002</span><br><span class="line">Thu Dec 16 15:39:48 CST 2021 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">2021-12-16 15:39:48,301 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">2021-12-16 15:39:48,601 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639471863570_0002</span><br><span class="line">2021-12-16 15:39:48,603 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2021-12-16 15:39:48,910 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2021-12-16 15:39:48,910 INFO resource.ResourceUtils: Unable to find &#x27;resource-types.xml&#x27;.</span><br><span class="line">2021-12-16 15:39:49,056 INFO impl.YarnClientImpl: Submitted application application_1639471863570_0002</span><br><span class="line">2021-12-16 15:39:49,108 INFO mapreduce.Job: The url to track the job: http://hadoop001:8088/proxy/application_1639471863570_0002/</span><br><span class="line">2021-12-16 15:39:49,109 INFO mapreduce.Job: Running job: job_1639471863570_0002</span><br><span class="line">2021-12-16 15:40:00,420 INFO mapreduce.Job: Job job_1639471863570_0002 running in uber mode : false</span><br><span class="line">2021-12-16 15:40:00,424 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2021-12-16 15:40:15,639 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2021-12-16 15:40:23,706 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2021-12-16 15:40:23,717 INFO mapreduce.Job: Job job_1639471863570_0002 completed successfully</span><br><span class="line">2021-12-16 15:40:23,832 INFO mapreduce.Job: Counters: 54</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=106</span><br><span class="line">		FILE: Number of bytes written=705400</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=156</span><br><span class="line">		HDFS: Number of bytes written=80</span><br><span class="line">		HDFS: Number of read operations=9</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">		HDFS: Number of bytes read erasure-coded=0</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=2</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Other local map tasks=2</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=24451</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=5384</span><br><span class="line">		Total time spent by all map tasks (ms)=24451</span><br><span class="line">		Total time spent by all reduce tasks (ms)=5384</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=24451</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=5384</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=25037824</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=5513216</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=4</span><br><span class="line">		Map output records=4</span><br><span class="line">		Map output bytes=92</span><br><span class="line">		Map output materialized bytes=112</span><br><span class="line">		Input split bytes=156</span><br><span class="line">		Combine input records=0</span><br><span class="line">		Combine output records=0</span><br><span class="line">		Reduce input groups=1</span><br><span class="line">		Reduce shuffle bytes=112</span><br><span class="line">		Reduce input records=4</span><br><span class="line">		Reduce output records=4</span><br><span class="line">		Spilled Records=8</span><br><span class="line">		Shuffled Maps =2</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=2</span><br><span class="line">		GC time elapsed (ms)=515</span><br><span class="line">		CPU time spent (ms)=3130</span><br><span class="line">		Physical memory (bytes) snapshot=507174912</span><br><span class="line">		Virtual memory (bytes) snapshot=8157011968</span><br><span class="line">		Total committed heap usage (bytes)=307437568</span><br><span class="line">		Peak Map Physical memory (bytes)=199159808</span><br><span class="line">		Peak Map Virtual memory (bytes)=2718306304</span><br><span class="line">		Peak Reduce Physical memory (bytes)=109441024</span><br><span class="line">		Peak Reduce Virtual memory (bytes)=2720411648</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=0</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=80</span><br></pre></td></tr></table></figure>

<p>在此，我的项目有log4j依赖，但缺少log4j配置文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;log4j-1.2-api&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="添加log4j配置文件"><a href="#添加log4j配置文件" class="headerlink" title="添加log4j配置文件"></a>添加log4j配置文件</h3><p>通过网上搜查得知，hadoop目录下<code>hadoop/etc/hadoop</code>是有自带的log4j.properties配置文件的。把log4j.properties 复制到project-&gt;src-&gt;main-&gt;resources下面，再执行程序。显示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br></pre></td></tr></table></figure>

<p>大概意思就是需要SLF4J依赖，于是搜索如何添加SLF4J依赖：</p>
<h3 id="slf4j打印日志必须的三个依赖包"><a href="#slf4j打印日志必须的三个依赖包" class="headerlink" title="slf4j打印日志必须的三个依赖包"></a>slf4j打印日志必须的三个依赖包</h3><p>slf4j假设使用log4j做为底层日志工具，运行以上程序需要三个包：</p>
<ul>
<li><p><strong>log4j-1.2.xx.jar、</strong></p>
</li>
<li><p><strong>slf4j-api-x.x.x.jar、</strong></p>
</li>
<li><p><strong>slf4j-log4j12-x.x.x.jar</strong></p>
</li>
</ul>
<p>经调试，直接把原来org.apache.logging.log4j依赖，替换成上面3个包的依赖：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;log4j&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;log4j&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.17&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.21&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.21&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>然后运行程序，控制台输出日志与服务器上运行输出的信息的大致一样。</p>
]]></content>
  </entry>
  <entry>
    <title>Linux快速整理</title>
    <url>/2021/11/08/Linux%E5%BF%AB%E9%80%9F%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>环境搭建:</p>
<p>vmware虚拟机+XShell登录虚拟机</p>
<p>Linux快速整理：</p>
<ol>
<li><p><code>[root@hadoop001 ~]</code>表示[登录的用户@机器名称 <del>]，“</del>”表示家目录</p>
</li>
<li><p><code>pwd</code>查看当前所在目录路径地址</p>
<p>有时候2个文件夹同名可能会搞错，需要要查看目录</p>
<span id="more"></span></li>
<li><p>切换目录 <code>cd</code></p>
<p>root用户的家目录：/root</p>
<p>xxx用户的家目录：/home/xxx 默认</p>
<p>家目录的修改：vi /etc/passwd 默认不修改</p>
<p>什么情况会修改？部署mysql时，修改mysqladmin的家目录为/usr/local/mysql，方便操作与规范。行业里比较标准的事情</p>
<p><code>cd 回车</code>  /  <code>cd ~ </code>  / <code>cd /root</code> 回到家目录</p>
<p><code>cd -</code>回到上一次访问的目录</p>
<p><code>cd ../</code> 回退上一层目录</p>
<p><code>cd ../../</code> 回退上两层目录</p>
</li>
<li><p>目录 文件夹</p>
<p><strong>绝对路径</strong>：“/”代表根目录，以根目录开始表示</p>
<p>​                    写shell脚本时，路径要用绝对路径。</p>
<p><strong>相对路径</strong>：不以根目录为开始，以当前光标所在目录（pwd结果）为开始表示</p>
<p>查看当前文件夹下的内容 <code>ls</code></p>
</li>
<li><p>清空屏幕 <code>clear</code></p>
</li>
<li><p>ls查看当前光标所在的目录 文件有哪些</p>
<p><code>ls -l</code>  等价于`ll``</p>
<p>``ls -l -a` 查看当前的文件文件夹+ 隐藏文件、文件夹（以.开头）</p>
<p><code>ll -h</code> 显示文件的大小</p>
<p><code>ll -rt</code> 按时间排序</p>
</li>
<li><p>查询命令帮助 <code>--help</code> /<code>man</code></p>
</li>
<li><p>创建文件夹 <code>mkdir</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir [-mp] 目录名称</span><br></pre></td></tr></table></figure>

<p>选项与参数：</p>
<ul>
<li>-m ：配置文件的权限喔！直接配置，不需要看默认权限 (umask) 的脸色～</li>
<li>-p ：帮助你直接将所需要的目录(包含上一级目录)递归创建起来！</li>
</ul>
<p>例如 </p>
<p><code>mkdir -p a/b/c/d</code>递归创建多层目录</p>
<p><code>mkdir a b c d</code>在当前文件夹下创建4个文件夹</p>
<p>删除空的目录：<code>rmdir</code></p>
<p>选项与参数：</p>
<ul>
<li><strong>-p ：</strong>从该目录起，一次删除多级空目录</li>
</ul>
</li>
<li><p>复制文件或目录： <code>cp [-adfilprsu] source destination</code></p>
<p>移动文件与目录，或修改名称： <code>mv [-fiu] source destination</code></p>
<p>移除文件或目录： <code>rm [-fir] 文件或目录</code></p>
<p><strong>思考：mv和cp哪个执行快？</strong></p>
<ul>
<li>同一个文件系统（在同一个分区）内，mv的速度是瞬间的，因为它所有需要的是重命名的目录的文件路径。除了目录条目之外，没有必要更改任何数据。</li>
<li>在文件系统之间移动目录将涉及将数据复制到目标并将其从源中删除。这将与在单个文件系统中复制（复制）数据一样长的时间。</li>
<li>都可修改名称。</li>
</ul>
</li>
<li><p>如何创建一个空文件 或者把一个文件设置为空</p>
<ul>
<li><p>touch rz.log 如何创建一个空文件</p>
<p><code>touch</code>命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。</p>
</li>
<li><p>echo “” &gt; rz.log1 慎用(不是真正的空，会有1字节的大小)</p>
</li>
<li><p>cat /dev/null &gt; ruoze.log20191113  把一个文件设置为空</p>
</li>
</ul>
</li>
<li><p>查看文件内容</p>
<ul>
<li><p><code>cat</code>命令用于连接文件并打印到标准输出设备上。</p>
<p>-n 或 –number：由 1 开始对所有输出的行数编号。</p>
</li>
<li><p><code>more</code> 命令类似 cat ，不过会以一页一页的形式显示，更方便使用者逐页阅读</p>
<p>空白键（space）- 往下一页显示</p>
<p>b  - 往回（back）一页显示</p>
<p>q  - 退出more</p>
</li>
<li><p><code>less</code> 与 more 类似，less 可以随意浏览文件，支持翻页和搜索，支持向上翻页和向下翻页</p>
<p>/字符串：向下搜索”字符串”的功能</p>
<p>?字符串：向上搜索”字符串”的功能</p>
<p>n：重复前一个搜索（与 / 或 ? 有关）</p>
<p>N：反向重复前一个搜索（与 / 或 ? 有关）</p>
<p>G ：移动到最后一行</p>
<p>g：移动到第一行</p>
<p>q / ZZ ：-退出 less 命令</p>
</li>
</ul>
<p><strong>配置文件，内容较少，可用以上上个命令</strong></p>
<p><strong>log日志，内容较多，用tail命令</strong></p>
<ul>
<li><p><code>tail</code> 可用于查看文件的内容，有一个常用的参数 <strong>-f</strong> 常用于查阅正在改变的日志文件。</p>
<p>实时查看</p>
<p><code>tail -f filename</code> 会把 filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新，只要 filename 更新就可以看到最新的文件内容。</p>
<p><code>tail -F filename</code>等于-f + retry，即使文件丢失后再创建也会更新</p>
<p>补充：flume exec source 切记使用 -F</p>
<p>-n&lt;行数&gt; 显示文件的尾部 n 行内容</p>
<p>-c&lt;数目&gt; 显示的字节数</p>
<p><code>tail -n -5 /test001/text001</code> 与 <code>tail -n 5 /test001/text001</code> 显示的结果相同，均是文件末尾最后 5 行内容。<br><code>tail -n +5 /test001/text001</code> 显示的内容为从第 5 行开始，直到末尾的内容。tail -n 后面的数字有效输入只有单个数字（5）或者加号连接数字（+5）两种。</p>
<p>tail -300f messages 实时查看倒数300行文件<br>tail -300F messages 不能这样写，即不能写成数字+F<br>tail: option used in invalid context – 3</p>
</li>
</ul>
</li>
<li><p>文件上传下载工具</p>
<p>安装： <code>yum install -y lrzsz</code></p>
<p>文件下载到windows：<code>sz</code></p>
<p>windows上传到linux: <code>rz</code></p>
</li>
<li><p>如何定位ERROR</p>
<ul>
<li><p>文件内容很小，几十兆</p>
<p>先下载到windows上，用编辑器打开搜索关键字定位ERROR</p>
</li>
<li><p>文件内容很大，几百兆 2G</p>
<p>cat xxx.log | grep ERROR</p>
<p>| 是管道符，管道符前面的命令结果作为管道符后面命令的输入，grep是过滤命令</p>
</li>
</ul>
</li>
<li><p>grep命令</p>
<p><code>grep</code> 指令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设 grep 指令会把含有范本样式的那一列显示出来。若不指定任何文件名称，或是所给予的文件名为 **-**，则 grep 指令会从标准输入设备读取数据。</p>
<p>-A&lt;显示行数&gt; 或 –after-context=&lt;显示行数&gt; : 除了显示符合范本样式的那一列之外，并显示该行之后的内容。</p>
<p>-B&lt;显示行数&gt; 或 –before-context=&lt;显示行数&gt;: 除了显示符合样式的那一行之外，并显示该行之前的内容。</p>
<p>-C&lt;显示行数&gt; 或 –context=&lt;显示行数&gt;或-&lt;显示行数&gt;: 除了显示符合样式的那一行之外，并显示该行之前后的内容。</p>
<p>cat xxx.log | grep -A 10 ERROR 后10行<br>cat xxx.log | grep -B 10 ERROR 前10行<br>cat xxx.log | grep -C 30 ERROR 前后各30行  经常用  迅速定位ERROR上下文</p>
<p>cat xxx.log | grep -C 30 ERROR &gt; error.log 新建/覆盖<br>cat xxx.log | grep -C 30 ERROR &gt;&gt; error.log 追加</p>
</li>
<li><p>环境变量$PATH</p>
<p>打印环境变量：<code>echo $PATH</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# echo $PATH</span><br><span class="line">/usr/local/mysql/bin:/usr/java/jdk1.7.0_80/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin</span><br></pre></td></tr></table></figure>

<p><code>which</code>命令：依次在环境变量$PATH中以冒号分割，查找目录下是否有要查找的命令目录，返回第一个查找到的目录地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# which ls</span><br><span class="line">alias ls=&#x27;ls --color=auto&#x27;</span><br><span class="line">	/bin/ls</span><br></pre></td></tr></table></figure>

<p>全局环境变量: /etc/profile    所有人都使用<br>个人环境变量: ~/.bash_profile  </p>
<pre><code>                      ~/.bashrc       个人 不给其他人
</code></pre>
<p>注意：配置个人环境变量文件**.bashrc** 优先。当用ssh远程登录时，**.bashrc** 会自动生效， <strong>.bash_profile</strong>则不会，bug。</p>
<p>生效文件: <code>source xxxx</code></p>
<pre><code>              `. ~/.bashrc`
</code></pre>
<p>（补充：安装unzip命令 ：<code>yum install -y unzip</code> 解压：<code>tar -xzvf xxxx.tar.gz</code>）</p>
<p>配置环境变量:<code>vi /etc/profile</code></p>
<p>环境变量是指的什么<br>K=V  等号前后不能有空格<br>使用环境变量K时用$符号，如： $K</p>
<p>export JAVA_HOME=usr/java/jdk1.8.0_121</p>
<p>export PATH=$JAVA_HOME/bin:$PATH</p>
<p>新配置的变量在前面追加</p>
<p>a. 上下键 移动光标<br>b. 按 i键insert 进入 <strong>编辑模式</strong><br>c. 开始编辑<br>d. 按 esc键退出 编辑模式，进入<strong>命令行模式</strong><br>e. 按 ：（shift+；）键，进入<strong>尾行模式</strong><br>f. 输入 wq 保存退出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 java]# source /etc/profile</span><br><span class="line">[root@ruozedata001 java]# echo $PATH</span><br><span class="line">/usr/java/jdk1.8.0_121/bin:/usr/java/jdk1.8.0_12/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br><span class="line">[root@ruozedata001 java]# which java</span><br><span class="line">/usr/java/jdk1.8.0_121/bin/java</span><br><span class="line">[root@ruozedata001 java]# </span><br></pre></td></tr></table></figure>

<p>小结:<br>1.command not found<br>没有部署安装包，部署了没有配置环境变量<br>2.习惯<br>当我们以后部署一个软件，bin目录的可执行文件 比如java<br>习惯 当生效环境变量文件，习惯做 which java</p>
</li>
<li><p>别名alias</p>
<p><code>alias</code> 命令用于设置指令的别名。</p>
<p>用户可利用alias，自定指令的别名。若仅输入alias，则可列出目前所有的别名设置。alias的效力仅及于该次登入的操作。若要每次登入是即自动设好别名，可在.profile或.bashrc中设定指令的别名。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alias[别名]=[指令名称]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# alias</span><br><span class="line">alias cp=&#x27;cp -i&#x27;</span><br><span class="line">alias l.=&#x27;ls -d .* --color=auto&#x27;</span><br><span class="line">alias ll=&#x27;ls -l --color=auto&#x27;</span><br><span class="line">alias ls=&#x27;ls --color=auto&#x27;</span><br><span class="line">alias mv=&#x27;mv -i&#x27;</span><br><span class="line">alias rm=&#x27;rm -i&#x27;</span><br><span class="line">alias which=&#x27;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&#x27;</span><br></pre></td></tr></table></figure>

<p>配置个人环境变量文件**.bashrc** 优先。当用ssh远程登录时，**.bashrc** 会自动生效， <strong>.bash_profile</strong>则不会，bug。</p>
</li>
<li><p>查看历史命令history</p>
<p>当前命令输入状态下：</p>
<pre><code>* 可以按一下**上＼下方向键**，命令行就会显示相对于当前命令的上一条或下一条历史记录．

*  和方向键相同功能的就是组合键**Ctrl+ p** （前面执行过的命令）,**Ctrl +n**（后面执行过的命令）
</code></pre>
<ul>
<li><p>上面两个都是相对于当前命令查询上一条或者下一条命令的历史记录．如果搜索命令历史记录，就用<strong>Ctrl+ r</strong> 组合键进入历史记录搜寻状态，然后，键盘每按一个字母，当前命令行就会搜索出命令历史记录．使用Ctrl+r反向查询历史命令，将匹配的最新一条显示出来</p>
<p>如果还想继续向上查询，继续按<strong>Ctrl+r</strong>。</p>
</li>
</ul>
<p><code>history [n]</code>  n为数字，列出最近的n条命令</p>
<p><code>history -c</code>  将目前shell中的所有history命令消除</p>
<p>使用! 执行历史命令。</p>
<pre><code>* `!  [n]`  n为数字 执行第n条命令
</code></pre>
<ul>
<li><code>! command</code> 从最近的命令查到以command开头的命令执行</li>
<li><code>!!</code> 执行上一条</li>
</ul>
<p>当同一账号，同时登录多个bash时，只有最后一个退出的会写入bash_history,其他的都被覆盖了。</p>
<p>历史命令文件记录在 ~/.bash_history中,要清空历史记录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /dev/null &gt;  ~/.bash_history</span><br><span class="line">history -c </span><br></pre></td></tr></table></figure>

<p>拓展：</p>
<p>当刚进公司进入服务器，第一步应该用history 看看这个账号之前做过哪些操作，有可能发现password</p>
</li>
<li><p>删除<br>生成新文件:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch xxx.log </span><br><span class="line">cat /dev/null &gt; xxx.log //把文件置空</span><br><span class="line">vi xxx.log</span><br></pre></td></tr></table></figure>

<p>创建文件夹: <code>mkdir</code> </p>
<p> <code>rm</code>（英文全拼：remove）命令用于删除一个文件或者目录。</p>
<ul>
<li><p>-i 删除前逐一询问确认。</p>
</li>
<li><p>-f 即使原档案属性设为唯读，亦直接删除，无需逐一确认。</p>
</li>
<li><p>-r 将目录及以下之档案亦逐一删除。</p>
<p>删除文件可以直接使用rm命令，若删除目录则必须配合选项”-r”，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># rm  test.txt </span><br><span class="line">rm：是否删除 一般文件 &quot;test.txt&quot;? y  </span><br><span class="line"># rm  homework  </span><br><span class="line">rm: 无法删除目录&quot;homework&quot;: 是一个目录  </span><br><span class="line"># rm  -r  homework  </span><br><span class="line">rm：是否删除 目录 &quot;homework&quot;? y </span><br></pre></td></tr></table></figure></li>
</ul>
<p>文件一旦通过rm命令删除，则无法恢复，所以必须格外小心地使用该命令。</p>
<p><strong>风险:</strong><br>rm -rf /  高危命令<br>什么场景会发生？ shell脚本:</p>
<p>K=’/home/jepson’</p>
<p>K=’’<br>shell脚本中必须判断 $K命令是否存在<br>rm -rf $K/*</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 6]# K=&#x27;&#x27; //本应该设置K=&#x27;/home/jepson&#x27;</span><br><span class="line">[root@hadoop001 6]# echo $K/</span><br><span class="line">/</span><br><span class="line">[root@hadoop001 6]# ls $K/</span><br><span class="line">6     data  home   lost+found  mnt  proc  selinux  tmp</span><br><span class="line">bin   dev   lib    media       net  root  srv      usr</span><br><span class="line">boot  etc   lib64  misc        opt  sbin  sys      var</span><br></pre></td></tr></table></figure></li>
<li><p>用户 用户组</p>
<p>创建用户：<code>useradd username</code></p>
<p>查看用户信息：<code>id username</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# id ruoze</span><br><span class="line">//uid=501(ruoze) gid=501(ruoze) groups=501(ruoze) 默认值</span><br><span class="line">uid=501(ruoze) gid=502(bigdata) groups=502(bigdata),501(ruoze)</span><br></pre></td></tr></table></figure>

<p>创建一个普通用户，默认创建这个名称的用户组ruoze,<br>且设置这个用户 主组为ruoze，且创建/home/ruoze</p>
<p>查看机器上的用户：<code>cat /etc/passwd</code><br>查看机器上的用户组：<code>cat /etc/group</code></p>
<p>删除用户:<code>userdel username</code></p>
<p>删除用户后，/etc/passwd内会删除用户信息，但是用户home文件夹依然存在。如果组内只有一个成员，成员被删除后，组会自动删除。</p>
<p>模拟切换用户丢失样式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 ruoze]# ll -a</span><br><span class="line">total 12</span><br><span class="line">drwx------  2 ruoze ruoze  59 Nov 16 21:16 .</span><br><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span><br><span class="line">-rw-r--r--  1 ruoze ruoze  18 Apr 11  2018 .bash_logout</span><br><span class="line">-rw-r--r--  1 ruoze ruoze 193 Apr 11  2018 .bash_profile</span><br><span class="line">-rw-r--r--  1 ruoze ruoze 231 Apr 11  2018 .bashrc</span><br><span class="line">[root@ruozedata001 ruoze]# rm -rf .bash*</span><br><span class="line"></span><br><span class="line">[root@ruozedata001 ~]# su  - ruoze</span><br><span class="line">Last login: Sat Nov 16 21:29:09 CST 2019 on pts/1</span><br><span class="line">-bash-4.2$ </span><br><span class="line">-bash-4.2$ </span><br></pre></td></tr></table></figure>

<p>修正样式（从/etc/skel上把隐藏文件复制到用户home目录下即可）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 ruoze]# cp /etc/skel/.* /home/ruoze/</span><br><span class="line">cp: omitting directory ‘/etc/skel/.’</span><br><span class="line">cp: omitting directory ‘/etc/skel/..’</span><br><span class="line">[root@ruozedata001 ruoze]# ll -a</span><br><span class="line">total 12</span><br><span class="line">drwx------  2 ruoze ruoze  59 Nov 16 21:32 .</span><br><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span><br><span class="line">-rw-r--r--  1 root  root   18 Nov 16 21:32 .bash_logout</span><br><span class="line">-rw-r--r--  1 root  root  193 Nov 16 21:32 .bash_profile</span><br><span class="line">-rw-r--r--  1 root  root  231 Nov 16 21:32 .bashrc</span><br><span class="line">[root@ruozedata001 ruoze]# chown ruoze:ruoze .bash*</span><br><span class="line"></span><br><span class="line">[root@ruozedata001 ruoze]# ll -a</span><br><span class="line">total 12</span><br><span class="line">drwx------  2 ruoze ruoze  59 Nov 16 21:32 .</span><br><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span><br><span class="line">-rw-r--r--  1 ruoze ruoze  18 Nov 16 21:32 .bash_logout</span><br><span class="line">-rw-r--r--  1 ruoze ruoze 193 Nov 16 21:32 .bash_profile</span><br><span class="line">-rw-r--r--  1 ruoze ruoze 231 Nov 16 21:32 .bashrc</span><br><span class="line">[root@ruozedata001 ruoze]# </span><br><span class="line">[root@ruozedata001 ~]# su - ruoze</span><br><span class="line">Last login: Sat Nov 16 21:30:23 CST 2019 on pts/2</span><br><span class="line">[ruoze@ruozedata001 ~]$ </span><br></pre></td></tr></table></figure>

<p>创建用户组：<code>groupadd groupname</code></p>
<p>把用户添加到用户组：<code>usermod -a -G bigdata ruoze</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">usermod	-g, --gid GROUP               force use GROUP as new primary group</span><br><span class="line">		-G, --groups GROUPS           new list of supplementary GROUPS</span><br><span class="line">		-a, --append                  append the user to the supplemental GROUPS</span><br></pre></td></tr></table></figure>

<p>给用户设置密码：<code>passwd username</code>，passwd后不加参数，就是设置当前用户的密码</p>
<p>切换用户：<code>su</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">su username</span><br><span class="line">su - username</span><br></pre></td></tr></table></figure>

<p>加”-“会把当前目录切换到用户的家目录，且执行环境变量文件（.bash_profile和.bashrc都执行）</p>
<p>不加“-”，目录不切换，.bashrc中的配置执行，.bash_profile中的配置不执行。所以配置最好写在.bashrc上。</p>
<p>用户权限问题：connection连接拒绝、Permission denied</p>
<p>普通用户获取root的最大权限<code>vi /etc/sudoers</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Allow root to run any commands anywhere </span><br><span class="line">username   ALL=(root)      NOPASSWD:ALL</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ruoze@ruozedata001 root]$ ls -l</span><br><span class="line">ls: cannot open directory .: Permission denied</span><br><span class="line">[ruoze@ruozedata001 root]$ cat rz.log</span><br><span class="line">cat: rz.log: Permission denied</span><br><span class="line">[ruoze@ruozedata001 root]$ sudo cat rz.log</span><br><span class="line">www.ruozedata.com</span><br></pre></td></tr></table></figure>

<p>/etc/passwd:</p>
<p>设置为：ruoze:\x:1002:1003::/home/ruoze:/bin/false，然后su切换失败</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 ~]# su - ruoze</span><br><span class="line">Last login: Sat Nov 16 21:57:32 CST 2019 on pts/0</span><br><span class="line">[root@ruozedata001 ~]# </span><br></pre></td></tr></table></figure>

<p>设置为：ruoze:\x:1002:1003::/home/ruoze:/sbin/nologin,然后不允许登录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 ~]# su - ruoze</span><br><span class="line">Last login: Sat Nov 16 22:08:52 CST 2019 on pts/0</span><br><span class="line">This account is currently not available.</span><br><span class="line">[root@ruozedata001 ~]# </span><br></pre></td></tr></table></figure>

<p>所以以后在CDH中遇到切换用户失败，到/etc/passwd中对应修改为 /bin/bash</p>
</li>
<li><p>权限：chown chmod</p>
<p>错误: Permission denied（permission denied一般用chmod修改文件权限就可，chown不必要）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod -R 777 文件夹/文件路径</span><br><span class="line">chown -R 用户:用户组 文件夹/文件路径</span><br><span class="line">-R, --recursive        operate on files and directories recursively</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 ~]# ll</span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 2 root root  6 Nov 16 22:15 ruozedata</span><br><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 rz.log</span><br></pre></td></tr></table></figure>

<p>第一个字符：d:文件夹；-：文件；l：连接；</p>
<p>r: read  4<br>w: write 2<br>x: 执行  1<br>-: 没权限 0</p>
<p>rwx 第一组 7 代表文件或文件夹的用户root，读写执行<br>r-x 第二组 5 代表文件或文件夹的用户组root，读执行<br>r-x 第三组 5 代表其他组的所属用户对这个文件或文件夹的权限: 读执行</p>
</li>
<li><p>查看大小</p>
<p>文件：</p>
<ul>
<li>ll</li>
<li>du -sh </li>
</ul>
<p>文件夹：</p>
<ul>
<li>du -sh</li>
<li>用ll看到的并不是文件夹大小</li>
</ul>
</li>
<li><p>删除执行中的程序或工作:<code>kill</code></p>
<p><code>kill</code> 可将指定的信息送至程序。预设的信息为 SIGTERM(15)，可将指定程序终止。</p>
<p>若仍无法终止该程序，可使用 SIGKILL(9) 信息尝试强制删除程序。程序或工作的编号(pid)可利用 ps 指令或 jobs 指令查看。</p>
<p>root用户将影响用户的进程，非root用户只能影响自己的进程。</p>
<p>使用 <code>kill -l</code> 命令列出所有可用信号。最常用的信号是：</p>
<ul>
<li>1 (HUP)：重新加载进程。</li>
<li>2（INT）：中断（同 Ctrl + C）</li>
<li>3（QUIT）：退出（同 Ctrl + \）</li>
<li>9 (KILL)：强制杀死一个进程。</li>
<li>15 (TERM)：正常停止一个进程。</li>
</ul>
<p>杀死进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kill pid</span><br></pre></td></tr></table></figure>

<p>强制杀死进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kill -KILL pid</span><br><span class="line"># kill -9 pid</span><br></pre></td></tr></table></figure>

<p>发送SIGHUP信号，可以使用一下信号</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kill -HUP pid</span><br></pre></td></tr></table></figure>

<p>杀死指定用户所有进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#kill -9 $(ps -ef | grep hnlinux) //方法一 过滤出hnlinux用户进程 </span><br><span class="line">#kill -u hnlinux //方法二</span><br></pre></td></tr></table></figure>

<p><strong>扩展：kill pid与kill -9 pid的区别</strong></p>
<p><code>kill pid</code>的作用是向进程号为pid的进程发送SIGTERM（这是kill默认发送的信号），该信号是一个结束进程的信号且可以被应用程序捕获。若应用程序没有捕获并响应该信号的逻辑代码，则该信号的默认动作是kill掉进程。这是终止指定进程的推荐做法。</p>
<p> <code>kill -9 pid</code>则是向进程号为pid的进程发送SIGKILL（该信号的编号为9），SIGKILL既不能被应用程序捕获，也不能被阻塞或忽略，其动作是立即结束指定进程。通俗地说，应用程序根本无法“感知”SIGKILL信号，它在完全无准备的情况下，就被收到SIGKILL信号的操作系统给干掉了，显然，在这种“暴力”情况下，应用程序完全没有释放当前占用资源(善后：关闭socket链接、清理临时文件、将自己将要被销毁的消息通知给子进程、重置自己的终止状态)的机会。事实上，SIGKILL信号是直接发给init进程的，它收到该信号后，负责终止pid指定的进程。在某些情况下（如进程已经hang死，无法响应正常信号），就可以使用kill -9来结束进程。</p>
<p><strong>注意：</strong>kill生产上不能随意杀进程，确认是自己服务不影响其他不丢数据，在杀死前周知运维、部门。发送信号时必须小心，只有在万不得已时，才用kill信号(9)，因为进程不能首先捕获它。</p>
</li>
<li><p>其他命令</p>
<ul>
<li><p>搜索 <code>find</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#find   path   -option   [   -print ]   [ -exec   -ok   command ]   &#123;&#125; \;</span><br></pre></td></tr></table></figure>

<p>将当前目录及其子目录下所有文件后缀为 <strong>.c</strong> 的文件列出来:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#find . -name &quot;*.c&quot;</span><br></pre></td></tr></table></figure>

<p>将当前目录及其子目录中的所有文件列出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#find . -type f</span><br></pre></td></tr></table></figure>

<p>将当前目录及其子目录下所有最近 20 天内更新过的文件列出:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#find . -ctime -20</span><br></pre></td></tr></table></figure></li>
<li><p>查看进程：<code>ps -ef</code> </p>
</li>
<li><p>系统情况：<code>top</code>（load average: 0，0，0反映繁忙不繁忙。超过十就很高。）</p>
</li>
<li><p>查看ip通不通：ping ip地址</p>
</li>
<li><p>测试端口号连通性：telnet ip地址 端口号</p>
</li>
</ul>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>MapReduce Input Split与map task数量</title>
    <url>/2021/12/14/MapReduce-Input-Split%EF%BC%88%E8%BE%93%E5%85%A5%E5%88%86-%E5%88%87%E7%89%87%EF%BC%89%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="Hadoop中的block-Size和split-Size是什么关系"><a href="#Hadoop中的block-Size和split-Size是什么关系" class="headerlink" title="Hadoop中的block Size和split Size是什么关系"></a>Hadoop中的block Size和split Size是什么关系</h2><p><strong>问题</strong></p>
<p>hadoop的split size 和 block size 是什么关系？ 是否 split size 应该 n倍于 block size ?</p>
<span id="more"></span>

<p><strong>概念</strong></p>
<p>在 hdfs 架构中，存在 blocks 的概念。 通常来说，hdfs中的一个block 是 64MB 。 当我们把一个大文件导入hdfs中的时候，文件会按 64MB 每个block来分割（版本不同，默认配置可能不同）。 如果你有1GB的文件要存入HDFS中， 1GB/64MB = 1024MB / 64MB = 16 个blocks 会被分割到不同的datanode上。</p>
<p><strong>目的</strong></p>
<p>数据分割(data splitting )策略是基于文件偏移进行的。文件分割的目的是有利于数据并行处理 ，以及便于数据容灾恢复。</p>
<p><strong>区别</strong></p>
<p>split 是逻辑意义上的split。 输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。</p>
<p>通常在 M/R 程序或者其他数据处理技术上用到。根据你处理的数据量的情况，split size是允许用户自定义的。</p>
<p>split size 定义好了之后，可以控制 M/R 中 Mapper 的数量。如果M/R中没有定义 split size ， 就用默认的HDFS配置作为 input split。<br>其中有俩个配置文件（如下）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--minsize   默认大小为1</span><br><span class="line">mapreduce.input.fileinputformat.split.minsize  </span><br><span class="line"></span><br><span class="line">--maxsize   默认大小为Long.MAXValue </span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize</span><br></pre></td></tr></table></figure>

<p>1.如果blockSize小于maxSize &amp;&amp; blockSize 大于 minSize之间，那么split就是blockSize；</p>
<p>2.如果blockSize小于maxSize &amp;&amp; blockSize 小于 minSize之间，那么split就是minSize；</p>
<p>3.如果blockSize大于maxSize &amp;&amp; blockSize 大于 minSize之间，那么split就是maxSize；</p>
<p><strong>举例</strong></p>
<p>案例1：</p>
<p>你有个100MB的文件，block size 是 64MB ， 那么就会被split成 2 块。这时如果你你没有指定 input split ， 你的M/R程序就会按2个input split 来处理 ， 并分配两个mapper来完成这个job。</p>
<p>但如果你把 split size 指定为 100MB（split.minsize=100MB），那么M/R程序就会把数据处理成一个 split，这时只用分配一个mapper 就可以了。</p>
<p>但如果你把 split size 指定为 25MB（split.maxsize=25MB），M/R就会将数据分成4个split，分配4个mapper来处理这个job。</p>
<p>案例2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[test@dw01 ~]$ hadoop fs -dus -h /tmp/wordcount/input/*</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00000 246.32M</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00002 106.95M</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00003 7.09M</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-0004 0</span><br></pre></td></tr></table></figure>

<p>在本例子中，mapreduce.input.fileinputformat.split.maxsize=104857440 （100MB），mapred.split.zero.file.skip=true，所有文件的blockSize 大小都是256MB，故splitSize=100MB</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00000 	划分成3个split</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00002 	划分成1个split</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-00003 	划分成1个split</span><br><span class="line">hdfs://namenode.test.net:9000/tmp/wordcount/input/part-r-0004 	不划分split</span><br></pre></td></tr></table></figure>

<p><strong>总结</strong></p>
<ol>
<li>block是物理上的数据分割，而split是逻辑上的分割。split是mapreduce中的概念，而block是hdfs中切块的大小。</li>
<li>如果没有特别指定，split size 就等于 HDFS 的 block size 。</li>
<li>用户可以在M/R 程序中自定义split size。</li>
<li>一个split 可以包含多个blocks，也可以把一个block应用多个split操作。</li>
<li>一个split不会包含两个File的Block,不会跨越File边界</li>
<li>有多少个split，就有多少个mapper。</li>
</ol>
<p><strong>补充</strong></p>
<p><strong>性能</strong></p>
<p>一般来说，block size 和 split size 设置成一致，性能较好。</p>
<p>FileInputFormat generates splits in such a way that each split is all or part of a single file. 所以 hadoop处理大文件比处理小文件来得效率高得多。</p>
<p><strong>如何避免切片：</strong></p>
<p>将切片的最小值设置为大于文件的大小</p>
<p>使用FileInputFormat的具体子类，重写isSplitable()方法，将返回值设置为false。</p>
<p>参考：</p>
<p><a href="https://blog.csdn.net/qq_20641565/article/details/53457622">https://blog.csdn.net/qq_20641565/article/details/53457622</a></p>
<p><a href="https://blog.csdn.net/wisgood/article/details/79178663">https://blog.csdn.net/wisgood/article/details/79178663</a></p>
<h2 id="MapReduce-Input-Split（输入分-切片）详解"><a href="#MapReduce-Input-Split（输入分-切片）详解" class="headerlink" title="MapReduce Input Split（输入分/切片）详解"></a>MapReduce Input Split（输入分/切片）详解</h2><p><strong><img src="http://images.cnitblog.com/blog/306623/201306/23175247-1cff38de2f154503bccd89a5d057f696.x-png" alt="img"></strong></p>
<p><strong>输入分片（Input Split）</strong>：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。</p>
<p>分片大小范围可以在mapred-site.xml中设置，mapred.min.split.size mapred.max.split.size，</p>
<p>minSplitSize大小默认为1B，maxSplitSize大小默认为Long.MAX_VALUE = 9223372036854775807</p>
<p><strong>那么分片到底是多大呢？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">long minSize = Math.max(this.getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">long maxSize = getMaxSplitSize(job);</span><br><span class="line">long blockSize = file.getBlockSize();</span><br><span class="line">long splitSize = this.computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123;</span><br><span class="line">        return Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>*<em>所以在我们没有设置分片的范围的时候，分片大小是由block块大小决定的，和它的大小一样。比如把一个258MB的文件上传到HDFS上，假设block块大小是128MB，那么它就会被分成三个block块，与之对应产生三个split</em>***，所以最终会产生三个map task。我又发现了另一个问题，第三个block块里存的文件大小只有2MB，而它的block块大小是128MB，那它实际占用Linux file system的多大空间？</p>
<p>答案是实际的文件大小，而非一个块的大小。有大神已经验证这个答案了：<a href="http://blog.csdn.net/samhacker/article/details/23089157">http://blog.csdn.net/samhacker/article/details/23089157</a></p>
<p>如果hdfs占用Linux file system的磁盘空间按实际文件大小算，那么这个”块大小“有必要存在吗？</p>
<p>其实块大小还是必要的，一个显而易见的作用就是当文件通过append操作不断增长的过程中，可以通过来block size决定何时split文件。以下是Hadoop Community的专家给我的回复： </p>
<p><em>“The block size is a meta attribute. If you append tothe file later, it still needs to know when to split further - so it keeps that value as a mere metadata it can use to advise itself on write boundaries.”</em> </p>
<p><strong>补充：</strong></p>
<p>一个split的大小是由goalSize, minSize, blockSize这三个值决定的。computeSplitSize的逻辑是，先从goalSize和blockSize两个值中选出最小的那个（比如一般不设置map数，这时blockSize为当前文件的块size，而goalSize是“InputFile大小”/“我们在配置文件中定义的mapred.map.tasks”值，如果没设置的话，默认是1）。</p>
<p>goalSize这个计算意味着：</p>
<ul>
<li>拆分不会小于文件中的剩余数据或<code>minSize</code>。</li>
<li>分割不会大于<code>goalSize</code>和<code>blockSize</code>中的较小者。</li>
</ul>
<p>（个人疑问，好像自己翻源码没有<em>goalSize</em>这个参数，都是用maxSize的表示的）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">&#125;</span><br><span class="line">protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>hadooop提供了一个设置map个数的参数mapred.map.tasks，我们可以通过这个参数来控制map的个数。但是通过这种方式设置map的个数，并不是每次都有效的。原因是mapred.map.tasks只是一个hadoop的参考数值，最终map的个数，还取决于其他的因素。</p>
<p>为了方便介绍，先来看几个名词：</p>
<p><strong>block_size</strong> : hdfs的文件块大小，默认为64M，可以通过参数dfs.block.size设置</p>
<p><strong>total_size</strong> : 输入文件整体的大小</p>
<p><strong>input_file_num</strong> : 输入文件的个数</p>
<ol>
<li><p><strong>默认map个数</strong></p>
<p>如果不进行任何设置，默认的map个数是和blcok_size相关的。</p>
<p>default_num = total_size / block_size;</p>
</li>
<li><p><strong>期望大小</strong></p>
<p>可以通过参数 mapred.map.tasks来设置程序员期望的map个数，但是这个个数只有在大于default_num的时候，才会生效。</p>
<p>goal_num = mapred.map.tasks;</p>
</li>
<li><p><strong>设置处理的文件大小</strong></p>
<p>可以通过mapred.min.split.size 设置每个task处理的文件大小，但是这个大小只有在大于 block_size的时候才会生效。</p>
<p>split_size = max( mapred.min.split.size, block_size );</p>
<p>split_num = total_size / split_size;</p>
</li>
<li><p><strong>计算的map个数</strong></p>
<p>compute_map_num = min(split_num,  max(default_num, goal_num))</p>
</li>
</ol>
<p>除了这些配置以外，mapreduce还要遵循一些原则。 mapreduce的每一个map处理的数据是不能跨越文件的，也就是说min_map_num &gt;= input_file_num。 所以，最终的map个数应该为：</p>
<p>   final_map_num = max(compute_map_num, input_file_num)</p>
<p>   经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：</p>
<p>（1）如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。</p>
<p>（2）如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。</p>
<p>（3）如果输入中有很多小文件，依然想减少map个数，则需要将小文件merger为大文件，然后使用准则2。</p>
<p>参考：</p>
<p><a href="https://blog.csdn.net/Dr_Guo/article/details/51150278">https://blog.csdn.net/Dr_Guo/article/details/51150278</a></p>
<p><a href="https://blog.csdn.net/lylcore/article/details/9136555">https://blog.csdn.net/lylcore/article/details/9136555</a></p>
<h2 id="mapreduce中split划分分析（新版api）"><a href="#mapreduce中split划分分析（新版api）" class="headerlink" title="mapreduce中split划分分析（新版api）"></a>mapreduce中split划分分析（新版api）</h2><h3 id="计算splitsize"><a href="#计算splitsize" class="headerlink" title="计算splitsize"></a>计算splitsize</h3><ul>
<li><p>minSize :每个split的最小值，默认为1.getFormatMinSplitSize()为代码中写死，固定返回1，除非修改了hadoop的源代码.getMinSplitSize(job)取决于参数mapreduce.input.fileinputformat.split.minsize，如果没有设置该参数，返回1.故minSize默认为1.</p>
</li>
<li><p>maxSize：每个split的最大值，如果设置了mapreduce.input.fileinputformat.split.maxsize，则为该值，否则为Long的最大值。</p>
</li>
<li><p>blockSize ：默认为HDFS设置的文件存储BLOCK大小。注意：该值并不一定是唯一固定不变的。HDFS上不同的文件该值可能不同。故将文件划分成split的时候，对于每个不同的文件，需要获取该文件的blocksize。</p>
</li>
<li><p>splitSize ：根据公式，默认为blockSize 。</p>
</li>
</ul>
<h3 id="getSplits-方法在-FileInputFormat-addInputPath-job-path-中"><a href="#getSplits-方法在-FileInputFormat-addInputPath-job-path-中" class="headerlink" title="getSplits()方法在 FileInputFormat.addInputPath(job, path)中"></a>getSplits()方法在 FileInputFormat.addInputPath(job, path)中</h3><ol>
<li><p>遍历输入目录中的每个文件，拿到该文件</p>
</li>
<li><p>计算文件长度，A:如果文件长度为0，如果mapred.split.zero.file.skip=true，则不划分split ; 如果mapred.split.zero.file.skip为false，生成一个length=0的split .B:如果长度不为0，跳到步骤3</p>
</li>
<li><p>判断该文件是否支持split :如果支持，跳到步骤4;如果不支持，该文件不切分，生成1个split，split的length等于文件长度。</p>
</li>
<li><p>根据当前文件，计算splitSize，本文中为100M</p>
</li>
<li><p>判断剩余待切分文件大小/splitsize是否大于SPLIT_SLOP(该值为1.1，代码中写死了) 如果true，切分成一个split，待切分文件大小更新为当前值-splitsize ，再次切分。生成的split的length等于splitsize； 如果false 将剩余的切到一个split里，生成的split length等于剩余待切分的文件大小。之所以需要判断剩余待切分文件大小/splitsize,主要是为了避免过多的小的split。比如文件中有100个109M大小的文件，如果splitSize=100M，如果不判断剩余待切分文件大小/splitsize，将会生成200个split，其中100个split的size为100M，而其中100个只有9M，存在100个过小的split。MapReduce首选的是处理大文件，过多的小split会影响性能。</p>
<pre><code> /** 
  * Generate the list of files and make them into FileSplits.
  * @param job the job context
  * @throws IOException
  */
 public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123;
     //用于记录分片开始的时间，最后会得到一个分片总用时，时间单位是纳秒
   StopWatch sw = new StopWatch().start();
   //用来计算分片大小
   //minSize 就是 1
   //maxSize 追到最下面可以发现其实就是long的最大值
   long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
   long maxSize = getMaxSplitSize(job);

   //存放切片对象
   List&lt;InputSplit&gt; splits = new ArrayList&lt;InputSplit&gt;();
   //得到路径下的所有文件
   List&lt;FileStatus&gt; files = listStatus(job);
   //遍历得到的文件
   for (FileStatus file: files) &#123;
     //得到文件路径
     Path path = file.getPath();
     //获取文件大小
     long length = file.getLen();
     //如果文件大小不为0的话
     if (length != 0) &#123;
         //定义块数组，存放块在datanode上的位置
       BlockLocation[] blkLocations;
       if (file instanceof LocatedFileStatus) &#123;
         blkLocations = ((LocatedFileStatus) file).getBlockLocations();
       &#125; else &#123;
         FileSystem fs = path.getFileSystem(job.getConfiguration());
         blkLocations = fs.getFileBlockLocations(file, 0, length);
       //如果这个文件可以分片的话进行分片，zip、视频等不能进行分片
       if (isSplitable(job, path)) &#123;
         //获取块大小，hadoop1默认是64M  hadoop2默认是128M  hadoop3默认是256M
         long blockSize = file.getBlockSize();
         //得到片大小
         //--&gt; 最终决定出切片的大小(128M) --&gt; blockSize值
           //Math.max(minSize, Math.min(max, blockSize));这是实现
         long splitSize = computeSplitSize(blockSize, minSize, maxSize);
         //获取文件大小
         long bytesRemaining = length;
         //文件大小/片大小&gt;1.1 开始分片
         //例如  文件大小为260M  260/128=2.03&gt;1.1 进入循环开始分片
         //132/128 &lt;1.1  不再进行分片，循环结束
         while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;
           int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
           splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                       blkLocations[blkIndex].getHosts(),
                        blkLocations[blkIndex].getCachedHosts()));
           //文件大小 = 原文件大小 - 当前分片大小
           //260 -128 = 132 现在文件大小是132 MB
           bytesRemaining -= splitSize;
         &#125;
          //循环结束之后,只要文件大小不等于0 此时也会在切一个片
         if (bytesRemaining != 0) &#123;
           int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
           splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                      blkLocations[blkIndex].getHosts(),
                      blkLocations[blkIndex].getCachedHosts()));
         &#125;
       &#125; else &#123; // not splitable
         splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                     blkLocations[0].getCachedHosts()));
       &#125;
     &#125; else &#123; 
       //为零长度文件创建空主机数组
       splits.add(makeSplit(path, 0, length, new String[0]));
     &#125;
   &#125;
   // 保存文件数
   job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
   sw.stop();
   //返回携带着切片文件的集合
   return splits;
 &#125;
</code></pre>
</li>
</ol>
<h2 id="Hadoop-map和reduce数量估算"><a href="#Hadoop-map和reduce数量估算" class="headerlink" title="Hadoop map和reduce数量估算"></a>Hadoop map和reduce数量估算</h2><p>Hadoop在运行一个mapreduce job之前，需要估算这个job的maptask数和reducetask数。</p>
<h3 id="map-task数量"><a href="#map-task数量" class="headerlink" title="map task数量"></a>map task数量</h3><p>首先分析一下job的maptask数，当一个job提交时，jobclient首先分析job被拆分的split数量，然后吧job.split文件放置在HDFS中，一个job的MapTask数量就等于split的个数。</p>
<p>job.split中包含split的个数由FileInputFormat.getSplits计算出，方法的逻辑如下：</p>
<ol>
<li><p>读取参数mapred.map.tasks，这个参数默认设置为0，生产系统中很少修改。</p>
</li>
<li><p>计算input文件的总字节数，总字节数/(mapred.map.tasks==0 ? 1: mapred.map.tasks )=goalsize</p>
</li>
<li><p>每个split的最小值minSize由mapred.min.split.size参数设置，这个参数默认设置为0，生产系统中很少修改。</p>
</li>
<li><p>调用computeSplitSize方法，计算出splitsize= Math.max(minSize, Math.min(goalSize, blockSize)),通常这个值=blockSize，输入的文件较小，文件字节数之和小于blocksize时，splitsize=输入文件字节数之和。</p>
</li>
<li><p>对于input的每个文件，计算split的个数。</p>
<p>a) 文件大小/splitsize&gt;1.1，创建一个split，这个split的字节数=splitsize，文件剩余字节数=文件大小-splitsize</p>
<p>b) 文件剩余字节数/splitsize&lt;1.1，剩余的部分作为一个split</p>
</li>
</ol>
<p>举例说明：</p>
<ol>
<li><p>input只有一个文件，大小为100M,splitsize=blocksize,则split数为2，第一个split为64M,第二个为36M</p>
</li>
<li><p>input只有一个文件，大小为65M,splitsize=blocksize，则split数为1，split大小为65M</p>
</li>
<li><p>input只有一个文件，大小为129M,splitsize=blocksize，则split数为2，第一个split为64M,第二个为65M(最后一个split的大小可能超过splitsize)</p>
</li>
<li><p>input只有一个文件，大小为20M ,splitsize=blocksize，则split数为1，split大小为20M</p>
</li>
<li><p>input有两个文件，大小为100M和20M,splitsize=blocksize,则split数为3，第一个文件分为两个split，第一个split为64M,第二个为36M，第二个文件为一个split，大小为20M</p>
</li>
<li><p>input有两个文件，大小为25M和20M,splitsize=blocksize,则split数为2，第一个文件为一个split，大小为25M，第二个文件为一个split，大小为20M</p>
</li>
</ol>
<p>假设一个job的input大小固定为100M,当只包含一个文件时，split个数为2，maptask数为2，但当包含10个10M的文件时，maptask数为10。</p>
<h3 id="reduce-task数量"><a href="#reduce-task数量" class="headerlink" title="reduce task数量"></a>reduce task数量</h3><p>下面来分析reducetask，纯粹的mapreduce task的reduce task数很简单，就是参数mapred.reduce.tasks的值，hadoop-site.xml文件中和mapreduce job运行时不设置的话默认为1。</p>
<p>在HIVE中运行sql的情况又不同，hive会估算reduce task的数量，估算方法如下：</p>
<p>通常是ceil(input文件大小/1024<em>1024</em>1024)，每1GB大小的输入文件对应一个reduce task。</p>
<p>特殊的情况是当sql只查询count(*)时，reduce task数被设置成1。</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>通过map和reduce task数量的分析可以看出，hadoop/hive估算的map和reduce task数可能和实际情况相差甚远。假定某个job的input数据量庞大，reduce task数量也会随之变大，而通过join和group by，实际output的数据可能不多，但reduce会输出大量的小文件，这个job的下游任务将会启动同样多的map来处理前面reduce产生的大量文件。在生产环境中每个user group有一个map task数的限额，一个job启动大量的map task很显然会造成其他job等待释放资源。</p>
<p>Hive对于上面描述的情况有一种补救措施，参数hive.merge.smallfiles.avgsize控制hive对output小文件的合并，当hiveoutput的文件的平均大小小于hive.merge.smallfiles.avgsize-默认为16MB左右，hive启动一个附加的mapreducejob合并小文件，合并后文件大小不超过hive.merge.size.per.task-默认为256MB。</p>
<p>尽管Hive可以启动小文件合并的过程，但会消耗掉额外的计算资源，控制单个reduce task的输出大小&gt;64MB才是最好的解决办法。</p>
<h3 id="map数据计算示例"><a href="#map数据计算示例" class="headerlink" title="map数据计算示例"></a>map数据计算示例</h3><p>hive&gt; set dfs.block.size;<br>dfs.block.size=268435456<br>hive&gt; set mapred.map.tasks;<br>mapred.map.tasks=2</p>
<p>文件块大小为256MB,map.tasks为2</p>
<p>查看文件大小和文件数：（共4539.059804MB,18个文件）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[dwapp@dw-yuntigw-63 hadoop]$ hadoop dfs -ls /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25;</span><br><span class="line">Found 18 items</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290700555 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000000_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290695945 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000001_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 290182606 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000002_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 271979933 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000003_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258448208 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000004_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258440338 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000005_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258419852 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000006_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258347423 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000007_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258349480 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000008_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258301657 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000009_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258270954 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000010_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258266805 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000011_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258253133 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000012_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258236047 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000013_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258239072 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000014_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258170671 2012-11-26 19:00 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000015_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258160711 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000016_0</span><br><span class="line">-rw-r-----  3 alidwicbu cug-alibaba-dw-icbu 258085783 2012-11-26 18:59 /group/alibaba-dw-icbu/hive/bdl_en12_pageview_fatdt0_d/hp_stat_date=2012-11-25/attempt_201211151327_1675393_m_000017_0</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>文件：</th>
<th>大小Bytes</th>
<th>大小MB</th>
<th></th>
<th>splitsize(MB)</th>
<th>每个文件需要的map数量</th>
</tr>
</thead>
<tbody><tr>
<td>文件1</td>
<td>290700555</td>
<td>277.2336531</td>
<td></td>
<td>256</td>
<td>1.082943957</td>
</tr>
<tr>
<td>文件2</td>
<td>290695945</td>
<td>277.2292566</td>
<td></td>
<td>256</td>
<td>1.082926784</td>
</tr>
<tr>
<td>文件3</td>
<td>290182606</td>
<td>276.7396984</td>
<td></td>
<td>256</td>
<td>1.081014447</td>
</tr>
<tr>
<td>文件4</td>
<td>271979933</td>
<td>259.3802767</td>
<td></td>
<td>256</td>
<td>1.013204206</td>
</tr>
<tr>
<td>文件5</td>
<td>258448208</td>
<td>246.4754181</td>
<td></td>
<td>256</td>
<td>0.962794602</td>
</tr>
<tr>
<td>文件6</td>
<td>258440338</td>
<td>246.4679127</td>
<td></td>
<td>256</td>
<td>0.962765284</td>
</tr>
<tr>
<td>文件7</td>
<td>258419852</td>
<td>246.4483757</td>
<td></td>
<td>256</td>
<td>0.962688968</td>
</tr>
<tr>
<td>文件8</td>
<td>258347423</td>
<td>246.379302</td>
<td></td>
<td>256</td>
<td>0.962419149</td>
</tr>
<tr>
<td>文件9</td>
<td>258349480</td>
<td>246.3812637</td>
<td></td>
<td>256</td>
<td>0.962426811</td>
</tr>
<tr>
<td>文件10</td>
<td>258301657</td>
<td>246.3356562</td>
<td></td>
<td>256</td>
<td>0.962248657</td>
</tr>
<tr>
<td>文件11</td>
<td>258270954</td>
<td>246.3063755</td>
<td></td>
<td>256</td>
<td>0.962134279</td>
</tr>
<tr>
<td>文件12</td>
<td>258266805</td>
<td>246.3024187</td>
<td></td>
<td>256</td>
<td>0.962118823</td>
</tr>
<tr>
<td>文件13</td>
<td>258253133</td>
<td>246.2893801</td>
<td></td>
<td>256</td>
<td>0.962067891</td>
</tr>
<tr>
<td>文件14</td>
<td>258236047</td>
<td>246.2730856</td>
<td></td>
<td>256</td>
<td>0.962004241</td>
</tr>
<tr>
<td>文件15</td>
<td>258239072</td>
<td>246.2759705</td>
<td></td>
<td>256</td>
<td>0.96201551</td>
</tr>
<tr>
<td>文件16</td>
<td>258170671</td>
<td>246.2107382</td>
<td></td>
<td>256</td>
<td>0.961760696</td>
</tr>
<tr>
<td>文件17</td>
<td>258160711</td>
<td>246.2012396</td>
<td></td>
<td>256</td>
<td>0.961723592</td>
</tr>
<tr>
<td>文件18</td>
<td>258085783</td>
<td>246.1297827</td>
<td></td>
<td>256</td>
<td>0.961444464</td>
</tr>
<tr>
<td>总文件大小：</td>
<td>4759549173</td>
<td>4539.059804</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>goalSize = 4539.059804 （文件总大小）/ mapred.map.tasks(2) = 2269.529902MB。</p>
<p>分片大小：Math.max(minSize, Math.min(goalSize, blockSize))：blockSize</p>
<p>因此splitsize取值为256MB，所以一共分配18个map。</p>
<p>修改map.tasks参数为32<br>set mapred.map.tasks = 32;</p>
<table>
<thead>
<tr>
<th>文件：</th>
<th>大小Bytes</th>
<th>大小MB</th>
<th></th>
<th>splitsize(MB)</th>
<th>每个文件需要的map数量</th>
</tr>
</thead>
<tbody><tr>
<td>文件1</td>
<td>290700555</td>
<td>277.2336531</td>
<td></td>
<td>141.8</td>
<td>1.955103336</td>
</tr>
<tr>
<td>文件2</td>
<td>290695945</td>
<td>277.2292566</td>
<td></td>
<td>141.8</td>
<td>1.955072332</td>
</tr>
<tr>
<td>文件3</td>
<td>290182606</td>
<td>276.7396984</td>
<td></td>
<td>141.8</td>
<td>1.951619876</td>
</tr>
<tr>
<td>文件4</td>
<td>271979933</td>
<td>259.3802767</td>
<td></td>
<td>141.8</td>
<td>1.829198002</td>
</tr>
<tr>
<td>文件5</td>
<td>258448208</td>
<td>246.4754181</td>
<td></td>
<td>141.8</td>
<td>1.738190537</td>
</tr>
<tr>
<td>文件6</td>
<td>258440338</td>
<td>246.4679127</td>
<td></td>
<td>141.8</td>
<td>1.738137607</td>
</tr>
<tr>
<td>文件7</td>
<td>258419852</td>
<td>246.4483757</td>
<td></td>
<td>141.8</td>
<td>1.737999829</td>
</tr>
<tr>
<td>文件8</td>
<td>258347423</td>
<td>246.379302</td>
<td></td>
<td>141.8</td>
<td>1.737512708</td>
</tr>
<tr>
<td>文件9</td>
<td>258349480</td>
<td>246.3812637</td>
<td></td>
<td>141.8</td>
<td>1.737526543</td>
</tr>
<tr>
<td>文件10</td>
<td>258301657</td>
<td>246.3356562</td>
<td></td>
<td>141.8</td>
<td>1.737204909</td>
</tr>
<tr>
<td>文件11</td>
<td>258270954</td>
<td>246.3063755</td>
<td></td>
<td>141.8</td>
<td>1.736998417</td>
</tr>
<tr>
<td>文件12</td>
<td>258266805</td>
<td>246.3024187</td>
<td></td>
<td>141.8</td>
<td>1.736970513</td>
</tr>
<tr>
<td>文件13</td>
<td>258253133</td>
<td>246.2893801</td>
<td></td>
<td>141.8</td>
<td>1.736878562</td>
</tr>
<tr>
<td>文件14</td>
<td>258236047</td>
<td>246.2730856</td>
<td></td>
<td>141.8</td>
<td>1.73676365</td>
</tr>
<tr>
<td>文件15</td>
<td>258239072</td>
<td>246.2759705</td>
<td></td>
<td>141.8</td>
<td>1.736783995</td>
</tr>
<tr>
<td>文件16</td>
<td>258170671</td>
<td>246.2107382</td>
<td></td>
<td>141.8</td>
<td>1.736323965</td>
</tr>
<tr>
<td>文件17</td>
<td>258160711</td>
<td>246.2012396</td>
<td></td>
<td>141.8</td>
<td>1.736256979</td>
</tr>
<tr>
<td>文件18</td>
<td>258085783</td>
<td>246.1297827</td>
<td></td>
<td>141.8</td>
<td>1.735753051</td>
</tr>
<tr>
<td>总文件大小：</td>
<td>4759549173</td>
<td>4539.059804</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>goalSize = 4539.059804 / mapred.map.tasks(32) = 141.8456189</p>
<p>分片大小：Math.max(minSize, Math.min(goalSize, blockSize))：goalSize </p>
<p>因此splitsize取值为141.8MB，所以一共分配36个map。</p>
<p>原文地址：</p>
<p><a href="https://www.cnblogs.com/ibook360/p/4137592.html">https://www.cnblogs.com/ibook360/p/4137592.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>在Linux系统上部署Mysql</title>
    <url>/2021/11/24/Mysql%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1 id="在Linux系统上部署Mysql"><a href="#在Linux系统上部署Mysql" class="headerlink" title="在Linux系统上部署Mysql"></a>在Linux系统上部署Mysql</h1><p>在Linux上部署Mysql的两种方式：</p>
<ul>
<li>rpm包部署：操作简单，适合学习的场景</li>
<li>tar包部署：定制化配置，生产上一般用tar包部署</li>
</ul>
<span id="more"></span>

<h2 id="一、rpm包部署"><a href="#一、rpm包部署" class="headerlink" title="一、rpm包部署"></a>一、rpm包部署</h2><h3 id="0-查看机器上是否已经部署"><a href="#0-查看机器上是否已经部署" class="headerlink" title="0.查看机器上是否已经部署"></a>0.查看机器上是否已经部署</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# rpm -qa|grep mysql</span><br><span class="line">mysql-5.1.73-3.el6_5.x86_64</span><br><span class="line">mysql-libs-5.1.73-3.el6_5.x86_64</span><br><span class="line">mysql-server-5.1.73-3.el6_5.x86_64</span><br></pre></td></tr></table></figure>

<p>此处已经部署，先卸载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# rpm -e --nodeps mysql*</span><br></pre></td></tr></table></figure>

<h3 id="1-用yum安装"><a href="#1-用yum安装" class="headerlink" title="1.用yum安装"></a>1.用yum安装</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# yum search mysql</span><br><span class="line">[root@hadoop001 mysql]# yum install -y mysql-server.x86_64 mysql.x86_64</span><br></pre></td></tr></table></figure>

<p>安装完毕根据需要修改配置文件:<code>/etc/my.cnf</code></p>
<p>#不用yum安装的话，可到官网下载rpm包，然后通过命令<code>rpm -ivh rpm包</code>进行安装，大同小异</p>
<h3 id="2-启动与停止"><a href="#2-启动与停止" class="headerlink" title="2.启动与停止"></a>2.启动与停止</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# service mysqld start</span><br><span class="line">[root@hadoop001 mysql]# service mysqld stop  </span><br></pre></td></tr></table></figure>



<h2 id="二、tar包部署"><a href="#二、tar包部署" class="headerlink" title="二、tar包部署"></a>二、tar包部署</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# mysql --version</span><br><span class="line">mysql  Ver 14.14 Distrib 5.6.23, for linux-glibc2.5 (x86_64) using  EditLine wrapper</span><br></pre></td></tr></table></figure>

<p>我的linux系统上已经部署mysql 5.6.23版本，现在以部署mysql 5.7.11为例再部署一次。</p>
<h3 id="0-前期准备：tar包"><a href="#0-前期准备：tar包" class="headerlink" title="0.前期准备：tar包"></a>0.前期准备：tar包</h3><p>官方网站选择需要的版本下载：<a href="https://downloads.mysql.com/archives/community/">https://downloads.mysql.com/archives/community/</a></p>
<p>我们这里用到的是mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz，点击下载：<a href="https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz">mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</a></p>
<p>然后通过rz命令上传至linux系统</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# cd /usr/local/</span><br><span class="line">[root@hadoop001 local]# rz</span><br><span class="line">[root@hadoop001 local]# ll|grep mysql</span><br><span class="line">-rw-r--r--.  1 root       root 548193637 Nov 21 03:58 mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="1-解压及创建目录"><a href="#1-解压及创建目录" class="headerlink" title="1.解压及创建目录"></a>1.解压及创建目录</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 local]# tar -xzvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz </span><br><span class="line">[root@hadoop001 local]# ln -s mysql-5.7.11-linux-glibc2.5-x86_64 mysql</span><br><span class="line">[root@hadoop001 local]# cd mysql</span><br><span class="line">[root@hadoop001 mysql]# ll</span><br><span class="line">total 52</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 bin</span><br><span class="line">-rw-r--r--.  1 7161 wheel 17987 Feb  2  2016 COPYING</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 docs</span><br><span class="line">drwxr-xr-x.  3 7161 wheel  4096 Feb  2  2016 include</span><br><span class="line">drwxr-xr-x.  5 7161 wheel  4096 Feb  2  2016 lib</span><br><span class="line">drwxr-xr-x.  4 7161 wheel  4096 Feb  2  2016 man</span><br><span class="line">-rw-r--r--.  1 7161 wheel  2478 Feb  2  2016 README</span><br><span class="line">drwxr-xr-x. 28 7161 wheel  4096 Feb  2  2016 share</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 support-files</span><br><span class="line">[root@hadoop001 mysql]# mkdir arch data tmp</span><br><span class="line">[root@hadoop001 mysql]# ll</span><br><span class="line">total 64</span><br><span class="line">drwxr-xr-x.  2 root root   4096 Nov 24 09:38 arch</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 bin</span><br><span class="line">-rw-r--r--.  1 7161 wheel 17987 Feb  2  2016 COPYING</span><br><span class="line">drwxr-xr-x.  2 root root   4096 Nov 24 09:38 data</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 docs</span><br><span class="line">drwxr-xr-x.  3 7161 wheel  4096 Feb  2  2016 include</span><br><span class="line">drwxr-xr-x.  5 7161 wheel  4096 Feb  2  2016 lib</span><br><span class="line">drwxr-xr-x.  4 7161 wheel  4096 Feb  2  2016 man</span><br><span class="line">-rw-r--r--.  1 7161 wheel  2478 Feb  2  2016 README</span><br><span class="line">drwxr-xr-x. 28 7161 wheel  4096 Feb  2  2016 share</span><br><span class="line">drwxr-xr-x.  2 7161 wheel  4096 Feb  2  2016 support-files</span><br><span class="line">drwxr-xr-x.  2 root root   4096 Nov 24 09:38 tmp</span><br></pre></td></tr></table></figure>

<p>arch:binlog日志存储的文件夹</p>
<h3 id="2-创建配置文件-etc-my-cnf"><a href="#2-创建配置文件-etc-my-cnf" class="headerlink" title="2.创建配置文件/etc/my.cnf"></a>2.创建配置文件/etc/my.cnf</h3><p>#defualt start: /etc/my.cnf-&gt;/etc/mysql/my.cnf-&gt;SYSCONFDIR/my.cnf-&gt;$MYSQL_HOME/my.cnf-&gt; –defaults-extra-file-&gt;~/my.cnf </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]#vi /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line">default-character-set=utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line">#user 			= mysqladmin</span><br><span class="line"></span><br><span class="line">skip-slave-start</span><br><span class="line"></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 2M</span><br><span class="line">read_buffer_size = 2M</span><br><span class="line">read_rnd_buffer_size = 4M</span><br><span class="line">query_cache_size= 32M</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line">myisam_sort_buffer_size=128M</span><br><span class="line">tmp_table_size=32M</span><br><span class="line"></span><br><span class="line">table_open_cache = 512</span><br><span class="line">thread_cache_size = 8</span><br><span class="line">wait_timeout = 86400</span><br><span class="line">interactive_timeout = 86400</span><br><span class="line">max_connections = 600</span><br><span class="line"></span><br><span class="line"># Try number of CPU&#x27;s*2 for thread_concurrency</span><br><span class="line">#thread_concurrency = 32 </span><br><span class="line"></span><br><span class="line">#isolation level and default engine </span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">transaction-isolation = READ-COMMITTED</span><br><span class="line"></span><br><span class="line">server-id  = 1739</span><br><span class="line">basedir     = /usr/local/mysql</span><br><span class="line">datadir     = /usr/local/mysql/data</span><br><span class="line">pid-file     = /usr/local/mysql/data/hostname.pid</span><br><span class="line"></span><br><span class="line">#open performance schema</span><br><span class="line">log-warnings</span><br><span class="line">sysdate-is-now</span><br><span class="line"></span><br><span class="line">binlog_format = ROW</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">log-error  = /usr/local/mysql/data/hostname.err</span><br><span class="line">log-bin = /usr/local/mysql/arch/mysql-bin</span><br><span class="line">expire_logs_days = 7</span><br><span class="line"></span><br><span class="line">innodb_write_io_threads=16</span><br><span class="line"></span><br><span class="line">relay-log  = /usr/local/mysql/relay_log/relay-log</span><br><span class="line">relay-log-index = /usr/local/mysql/relay_log/relay-log.index</span><br><span class="line">relay_log_info_file= /usr/local/mysql/relay_log/relay-log.info</span><br><span class="line"></span><br><span class="line">log_slave_updates=1</span><br><span class="line">gtid_mode=OFF</span><br><span class="line">enforce_gtid_consistency=OFF</span><br><span class="line"></span><br><span class="line"># slave</span><br><span class="line">slave-parallel-type=LOGICAL_CLOCK</span><br><span class="line">slave-parallel-workers=4</span><br><span class="line">master_info_repository=TABLE</span><br><span class="line">relay_log_info_repository=TABLE</span><br><span class="line">relay_log_recovery=ON</span><br><span class="line"></span><br><span class="line">#other logs</span><br><span class="line">#general_log =1</span><br><span class="line">#general_log_file  = /usr/local/mysql/data/general_log.err</span><br><span class="line">#slow_query_log=1</span><br><span class="line">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span><br><span class="line"></span><br><span class="line">#for replication slave</span><br><span class="line">sync_binlog = 500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#for innodb options </span><br><span class="line">innodb_data_home_dir = /usr/local/mysql/data/</span><br><span class="line">innodb_data_file_path = ibdata1:1G;ibdata2:1G:autoextend</span><br><span class="line"></span><br><span class="line">innodb_log_group_home_dir = /usr/local/mysql/arch</span><br><span class="line">innodb_log_files_in_group = 4</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_log_buffer_size = 200M</span><br><span class="line"></span><br><span class="line">#根据生产需要，调整pool size </span><br><span class="line">innodb_buffer_pool_size = 2G</span><br><span class="line">#innodb_additional_mem_pool_size = 50M #deprecated in 5.6</span><br><span class="line">tmpdir = /usr/local/mysql/tmp</span><br><span class="line"></span><br><span class="line">innodb_lock_wait_timeout = 1000</span><br><span class="line">#innodb_thread_concurrency = 0</span><br><span class="line">innodb_flush_log_at_trx_commit = 2</span><br><span class="line"></span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"></span><br><span class="line">#innodb io features: add for mysql5.5.8</span><br><span class="line">performance_schema</span><br><span class="line">innodb_read_io_threads=4</span><br><span class="line">innodb-write-io-threads=4</span><br><span class="line">innodb-io-capacity=200</span><br><span class="line">#purge threads change default(0) to 1 for purge</span><br><span class="line">innodb_purge_threads=1</span><br><span class="line">innodb_use_native_aio=on</span><br><span class="line"></span><br><span class="line">#case-sensitive file names and separate tablespace</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">lower_case_table_names=1</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">max_allowed_packet = 128M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">no-auto-rehash</span><br><span class="line">default-character-set=utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqlhotcopy]</span><br><span class="line">interactive-timeout</span><br><span class="line"></span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 256M</span><br><span class="line">read_buffer = 2M</span><br><span class="line">write_buffer = 2M</span><br></pre></td></tr></table></figure>

<p>根据生产需要，调整pool size，生产上：innodb_buffer_pool_size = 2G</p>
<p>补充：</p>
<p>配置MySQL的环境变量：/etc/profile:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQL_HOME=/usr/local/mysql</span><br><span class="line">export PATH=$&#123;MYSQL_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>

<h3 id="3-创建用户组及用户"><a href="#3-创建用户组及用户" class="headerlink" title="3.创建用户组及用户"></a>3.创建用户组及用户</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# groupadd -g 101 dba</span><br><span class="line">[root@hadoop001 mysql]# useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</span><br><span class="line">[root@hadoop001 mysql]# id mysqladmin</span><br><span class="line">uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)</span><br></pre></td></tr></table></figure>

<p>一般不需要设置mysqladmin的密码，直接从root或者LDAP用户sudo切换</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# passwd mysqladmin</span><br><span class="line">Changing password for user mysqladmin.</span><br><span class="line">New password: </span><br><span class="line">BAD PASSWORD: it is too simplistic/systematic</span><br><span class="line">BAD PASSWORD: is too simple</span><br><span class="line">Retype new password: </span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure>

<p>如果mysqladmin用户已经存在，更改用户组及home目录地址：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# usermod -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</span><br></pre></td></tr></table></figure>

<p>copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# cp /etc/skel/.* /usr/local/mysql</span><br><span class="line">cp: omitting directory `/etc/skel/.&#x27;</span><br><span class="line">cp: omitting directory `/etc/skel/..&#x27;</span><br><span class="line">cp: omitting directory `/etc/skel/.gnome2&#x27;</span><br><span class="line">cp: omitting directory `/etc/skel/.mozilla&#x27;</span><br><span class="line">[root@hadoop001 mysql]# su - mysqladmin</span><br><span class="line">[mysqladmin@hadoop001 ~]$ exit</span><br><span class="line">logout</span><br><span class="line">[root@hadoop001 mysql]# </span><br></pre></td></tr></table></figure>

<h3 id="4-配置环境变量"><a href="#4-配置环境变量" class="headerlink" title="4.配置环境变量"></a>4.配置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# vi .bash_profile</span><br><span class="line"># .bash_profile</span><br><span class="line"># Get the aliases and functions</span><br><span class="line"></span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># User specific environment and startup programs</span><br><span class="line">export MYSQL_BASE=/usr/local/mysql</span><br><span class="line">export PATH=$&#123;MYSQL_BASE&#125;/bin:$PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">unset USERNAME</span><br><span class="line"></span><br><span class="line">#stty erase ^H</span><br><span class="line">set umask to 022</span><br><span class="line">umask 022</span><br><span class="line">PS1=`uname -n`&quot;:&quot;&#x27;$USER&#x27;&quot;:&quot;&#x27;$PWD&#x27;&quot;:&gt;&quot;; export PS1</span><br></pre></td></tr></table></figure>

<h3 id="5-赋权限和用户组"><a href="#5-赋权限和用户组" class="headerlink" title="5.赋权限和用户组"></a>5.赋权限和用户组</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# chown mysqladmin:dba /etc/my.cnf</span><br><span class="line">[root@hadoop001 mysql]# chmod  640 /etc/my.cnf  </span><br><span class="line">[root@hadoop001 mysql]# ll /etc/my.cnf</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 2218 Nov 15 01:07 /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[root@hadoop001 mysql]# chown -R mysqladmin:dba /usr/local/mysql</span><br><span class="line">[root@hadoop001 mysql]# chown -R mysqladmin:dba /usr/local/mysql/*</span><br><span class="line">[root@hadoop001 mysql]# chown -R mysqladmin:dba /usr/local/mysql-5.7.11-linux-glibc2.5-x86_64</span><br><span class="line"></span><br><span class="line">[root@hadoop001 mysql]# chmod -R 755 /usr/local/mysql </span><br><span class="line">[root@hadoop001 mysql]# chmod -R 755 /usr/local/mysql/*</span><br><span class="line">[root@hadoop001 mysql]# chmod -R 755 /usr/local/mysql-5.7.11-linux-glibc2.5-x86_64 </span><br></pre></td></tr></table></figure>

<h3 id="6-配置服务及开机自启动"><a href="#6-配置服务及开机自启动" class="headerlink" title="6.配置服务及开机自启动"></a>6.配置服务及开机自启动</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#将服务文件拷贝到init.d下，并重命名为mysql</span><br><span class="line">[root@hadoop001 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql </span><br><span class="line">#赋予可执行权限</span><br><span class="line">[root@hadoop001 mysql]# chmod +x /etc/rc.d/init.d/mysql</span><br><span class="line">#删除服务</span><br><span class="line">[root@hadoop001 mysql]# chkconfig --del mysql</span><br><span class="line">#添加服务</span><br><span class="line">[root@hadoop001 mysql]# chkconfig --add mysql</span><br><span class="line">[root@hadoop001 mysql]# chkconfig --level 345 mysql on</span><br><span class="line">#开机自启动</span><br><span class="line">[root@hadoop001 mysql]# vi /etc/rc.local </span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line"># THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span><br><span class="line">#</span><br><span class="line"># It is highly advisable to create own systemd services or udev rules</span><br><span class="line"># to run scripts during boot instead of using this file.</span><br><span class="line">#</span><br><span class="line"># In contrast to previous versions due to parallel execution during boot</span><br><span class="line"># this script will NOT be run after all other services.</span><br><span class="line">#</span><br><span class="line"># Please note that you must run &#x27;chmod +x /etc/rc.d/rc.local&#x27; to ensure</span><br><span class="line"># that this script will be executed during boot.</span><br><span class="line"></span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line">su - mysqladmin -c &quot;/etc/init.d/mysql start --federated&quot;</span><br></pre></td></tr></table></figure>

<h3 id="7-安装"><a href="#7-安装" class="headerlink" title="7.安装"></a>7.安装</h3><p>安装libaio及安装mysql的初始db</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# yum -y install libaio</span><br><span class="line">[root@hadoop001 mysql]# su - mysqladmin</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;bin/mysqld \</span><br><span class="line">&gt; --defaults-file=/etc/my.cnf \</span><br><span class="line">&gt; --user=mysqladmin \</span><br><span class="line">&gt; --basedir=/usr/local/mysql/ \</span><br><span class="line">&gt; --datadir=/usr/local/mysql/data/ \</span><br><span class="line">&gt; --initialize</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;</span><br></pre></td></tr></table></figure>

<p>在初始化时如果加上 –initial-insecure，则会创建空密码的 root@localhost 账号，否则会创建带密码的 root@localhost 账号，密码直接写在 log-error 日志文件中（在5.6版本中是放在 ~/.mysql_secret 文件里，更加隐蔽，不熟悉的话可能会无所适从）</p>
<p>bin/mysqld –defaults-file=/etc/my.cnf –user=mysqladmin –basedir=/usr/local/mysql/ –datadir=/usr/local/mysql/data/ –initialize</p>
<p>查看临时密码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001:mysqladmin:/usr/local/mysql/data:&gt;cat hostname.err | grep pass</span><br><span class="line">2021-11-24T06:05:28.300539Z 1 [Note] A temporary password is generated for root@localhost: uhsa*57&gt;hacF</span><br></pre></td></tr></table></figure>

<ul>
<li><p>启动Mysql时报错：mysqld_safe mysqld from pid file /usr/local/mysql/data/Linux.pid ended。参考文章<a href="https://blog.csdn.net/alwaysbefine/article/details/107216380">Linux The server quit without updating PID file的几种解决方法</a>发现，是之前在运行5.6版本的Mysql仍在后台运行，占用pid文件，解决方法：ps -ef|grep mysql找出进程，然后kill掉进程后再重新安装。</p>
</li>
<li><p>若遇到报错，需要重新安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm -rf /usr/local/mysql/arch/*</span><br><span class="line">rm -rf /usr/local/mysql/data/*</span><br></pre></td></tr></table></figure>

<p>然后跳到第7步</p>
</li>
</ul>
<h3 id="8-启动"><a href="#8-启动" class="headerlink" title="8.启动"></a>8.启动</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 mysql]# mysql --version</span><br><span class="line">mysql  Ver 14.14 Distrib 5.7.11, for linux-glibc2.5 (x86_64) using  EditLine wrapper</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql start</span><br><span class="line">Starting MySQL..                                           [  OK  ]</span><br></pre></td></tr></table></figure>

<h3 id="9-登录及修改用户密码"><a href="#9-登录及修改用户密码" class="headerlink" title="9.登录及修改用户密码"></a>9.登录及修改用户密码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;mysql -uroot -p&#x27;&gt;Wo&gt;kh(GL7;D&#x27;</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.11-log</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; alter user root@localhost identified by &#x27;syncdb123!&#x27;;//rd1</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;syncdb123!&#x27;;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; exit;</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure>

<p>重要的三句话：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt;create database ruozedata;</span><br><span class="line">mysql&gt;grant all privileges on ruozedata.* to ruoze@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;</span><br><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure>

<h3 id="10-重启"><a href="#10-重启" class="headerlink" title="10.重启"></a>10.重启</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql restart</span><br><span class="line">Shutting down MySQL..                                      [  OK  ]</span><br><span class="line">rm: cannot remove `/var/lock/subsys/mysql&#x27;: Permission denied</span><br><span class="line">Starting MySQL.                                            [  OK  ]</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;mysql -uroot -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.11-log MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br></pre></td></tr></table></figure>



<h2 id="三-（个人）重启机器后遇到问题"><a href="#三-（个人）重启机器后遇到问题" class="headerlink" title="三.（个人）重启机器后遇到问题"></a>三.（个人）重启机器后遇到问题</h2><p>当我重启机器后，mysqladmin并不能直接service mysql start启动Mysql服务，查看data文件夹内发现有 </p>
<p><code>-rw-r-----. 1 mysql dba  53440 Nov 25 00:40 hostname.err</code></p>
<p>即用户权限出现问题。然后切回root用户修改data/*的用户：用户组为mysqladmin:dba再启动，问题解决，过程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]# service mysql status</span><br><span class="line">MySQL is not running, but lock file (/var/lock/subsys/mysql[FAILED]</span><br><span class="line">[root@hadoop001 ~]# service mysql start</span><br><span class="line">Starting MySQL.The server quit without updating PID file (/usr/local/mysql/data/hadoop001.pid).                                                          [FAILED]</span><br><span class="line">[root@hadoop001 ~]# su - mysqladmin</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql start</span><br><span class="line">Starting MySQL.The server quit without updating PID file (/usr/local/mysql/data/hadoop001.pid).                                                          [FAILED]</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;ll</span><br><span class="line">total 68</span><br><span class="line">drwxr-xr-x.  2 mysqladmin dba  4096 Nov 25 00:35 arch</span><br><span class="line">drwxr-xr-x.  2 mysqladmin dba  4096 Feb  2  2016 bin</span><br><span class="line">-rwxr-xr-x.  1 mysqladmin dba 17987 Feb  2  2016 COPYING</span><br><span class="line">drwxr-xr-x.  5 mysqladmin dba  4096 Nov 25 00:40 data</span><br><span class="line">drwxr-xr-x.  2 mysqladmin dba  4096 Feb  2  2016 docs</span><br><span class="line">drwxr-xr-x.  3 mysqladmin dba  4096 Feb  2  2016 include</span><br><span class="line">drwxr-x---.  2 mysqladmin dba  4096 Nov 24 13:15 keyring</span><br><span class="line">drwxr-xr-x.  5 mysqladmin dba  4096 Feb  2  2016 lib</span><br><span class="line">drwxr-xr-x.  4 mysqladmin dba  4096 Feb  2  2016 man</span><br><span class="line">-rwxr-xr-x.  1 mysqladmin dba  2478 Feb  2  2016 README</span><br><span class="line">drwxr-xr-x. 28 mysqladmin dba  4096 Feb  2  2016 share</span><br><span class="line">drwxr-xr-x.  2 mysqladmin dba  4096 Feb  2  2016 support-files</span><br><span class="line">drwxr-xr-x.  2 mysqladmin dba  4096 Nov 25 00:35 tmp</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;ll data/</span><br><span class="line">total 2097248</span><br><span class="line">-rw-r-----. 1 mysqladmin dba         56 Nov 24 14:55 auto.cnf</span><br><span class="line">-rw-r-----. 1 mysql      dba      53440 Nov 25 00:40 hostname.err</span><br><span class="line">-rw-r-----. 1 mysqladmin dba        294 Nov 25 00:35 ib_buffer_pool</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 25 00:35 ibdata1</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 24 14:55 ibdata2</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 mysql</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 performance_schema</span><br><span class="line">drwxr-x---. 2 mysqladmin dba      12288 Nov 24 14:55 sys</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;exit</span><br><span class="line">logout</span><br><span class="line">[root@hadoop001 ~]# chown mysqladmin:dba /usr/local/mysql/*</span><br><span class="line">[root@hadoop001 ~]# chown mysqladmin:dba /usr/local/mysql/data/*</span><br><span class="line">[root@hadoop001 ~]# su - mysqladmin</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;ll data/</span><br><span class="line">total 2097248</span><br><span class="line">-rw-r-----. 1 mysqladmin dba         56 Nov 24 14:55 auto.cnf</span><br><span class="line">-rw-r-----. 1 mysqladmin dba      53440 Nov 25 00:40 hostname.err</span><br><span class="line">-rw-r-----. 1 mysqladmin dba        294 Nov 25 00:35 ib_buffer_pool</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 25 00:35 ibdata1</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 24 14:55 ibdata2</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 mysql</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 performance_schema</span><br><span class="line">drwxr-x---. 2 mysqladmin dba      12288 Nov 24 14:55 sys</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql start</span><br><span class="line">Starting MySQL.                                            [  OK  ]</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;ll data/</span><br><span class="line">total 2109552</span><br><span class="line">-rw-r-----. 1 mysqladmin dba         56 Nov 24 14:55 auto.cnf</span><br><span class="line">-rw-r-----. 1 mysqladmin dba          5 Nov 25 00:41 hadoop001.pid</span><br><span class="line">-rw-r-----. 1 mysqladmin dba      57597 Nov 25 00:41 hostname.err</span><br><span class="line">-rw-r-----. 1 mysqladmin dba        294 Nov 25 00:35 ib_buffer_pool</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 25 00:41 ibdata1</span><br><span class="line">-rw-r-----. 1 mysqladmin dba 1073741824 Nov 24 14:55 ibdata2</span><br><span class="line">-rw-r-----. 1 mysqladmin dba   12582912 Nov 25 00:41 ibtmp1</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 mysql</span><br><span class="line">-rw-rw----. 1 mysqladmin dba          5 Nov 25 00:41 mysqld_safe.pid</span><br><span class="line">srwxrwxrwx. 1 mysqladmin dba          0 Nov 25 00:41 mysql.sock</span><br><span class="line">-rw-------. 1 mysqladmin dba          5 Nov 25 00:41 mysql.sock.lock</span><br><span class="line">drwxr-x---. 2 mysqladmin dba       4096 Nov 24 14:55 performance_schema</span><br><span class="line">drwxr-x---. 2 mysqladmin dba      12288 Nov 24 14:55 sys</span><br><span class="line">hadoop001:mysqladmin:/usr/local/mysql:&gt;exit</span><br><span class="line">logout</span><br><span class="line">[root@hadoop001 ~]# </span><br></pre></td></tr></table></figure>

<p>再次重启，问题再次出现，hostname.err的用户用户组为：mysql:mysqladmin</p>
<p>经查阅，发现my.cnf中[mysqld]下有默认启动用户<code>user = mysql</code>，指定为<code>user = mysqladmin</code>后重启，问题不再出现，且mysql服务开机自启动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line">#以下注释为默认参数，不指定的话，user=mysql</span><br><span class="line">#user 			= mysql</span><br><span class="line">user 			= mysqladmin</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>SQL练习题</title>
    <url>/2021/11/29/SQL%E7%BB%83%E4%B9%A0%E9%A2%98/</url>
    <content><![CDATA[<h1 id="SQL练习题"><a href="#SQL练习题" class="headerlink" title="SQL练习题"></a>SQL练习题</h1><p>–部门表<br>dept部门表(deptno部门编号/dname部门名称/loc地点)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table dept (deptno numeric(2),dname varchar(14),loc varchar(13));</span><br><span class="line">insert into dept values (10, &#x27;ACCOUNTING&#x27;, &#x27;NEW YORK&#x27;);</span><br><span class="line">insert into dept values (20, &#x27;RESEARCH&#x27;, &#x27;DALLAS&#x27;);</span><br><span class="line">insert into dept values (30, &#x27;SALES&#x27;, &#x27;CHICAGO&#x27;);</span><br><span class="line">insert into dept values (40, &#x27;OPERATIONS&#x27;, &#x27;BOSTON&#x27;);</span><br></pre></td></tr></table></figure>

<p>–工资等级表<br>salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table salgrade (grade numeric,losal numeric,hisal numeric);</span><br><span class="line">insert into salgrade values (1, 700, 1200);</span><br><span class="line">insert into salgrade values (2, 1201, 1400);</span><br><span class="line">insert into salgrade values (3, 1401, 2000);</span><br><span class="line">insert into salgrade values (4, 2001, 3000);</span><br><span class="line">insert into salgrade values (5, 3001, 9999);</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p>–员工表<br>emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)<br>工资 ＝ 薪金 ＋ 佣金</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table emp (</span><br><span class="line">    empno numeric(4) not null,</span><br><span class="line">    ename varchar(10),</span><br><span class="line">    job varchar(9),</span><br><span class="line">    mgr numeric(4),</span><br><span class="line">    hiredate datetime,</span><br><span class="line">    sal numeric(7, 2),</span><br><span class="line">    comm numeric(7, 2),</span><br><span class="line">    deptno numeric(2)</span><br><span class="line">);</span><br><span class="line">insert into emp values (7369, &#x27;SMITH&#x27;, &#x27;CLERK&#x27;, 7902, &#x27;1980-12-17&#x27;, 800, null, 20);</span><br><span class="line">insert into emp values (7499, &#x27;ALLEN&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-02-20&#x27;, 1600, 300, 30);</span><br><span class="line">insert into emp values (7521, &#x27;WARD&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-02-22&#x27;, 1250, 500, 30);</span><br><span class="line">insert into emp values (7566, &#x27;JONES&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-04-02&#x27;, 2975, null, 20);</span><br><span class="line">insert into emp values (7654, &#x27;MARTIN&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-09-28&#x27;, 1250, 1400, 30);</span><br><span class="line">insert into emp values (7698, &#x27;BLAKE&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-05-01&#x27;, 2850, null, 30);</span><br><span class="line">insert into emp values (7782, &#x27;CLARK&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-06-09&#x27;, 2450, null, 10);</span><br><span class="line">insert into emp values (7788, &#x27;SCOTT&#x27;, &#x27;ANALYST&#x27;, 7566, &#x27;1982-12-09&#x27;, 3000, null, 20);</span><br><span class="line">insert into emp values (7839, &#x27;KING&#x27;, &#x27;PRESIDENT&#x27;, null, &#x27;1981-11-17&#x27;, 5000, null, 10);</span><br><span class="line">insert into emp values (7844, &#x27;TURNER&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-09-08&#x27;, 1500, 0, 30);</span><br><span class="line">insert into emp values (7876, &#x27;ADAMS&#x27;, &#x27;CLERK&#x27;, 7788, &#x27;1983-01-12&#x27;, 1100, null, 20);</span><br><span class="line">insert into emp values (7900, &#x27;JAMES&#x27;, &#x27;CLERK&#x27;, 7698, &#x27;1981-12-03&#x27;, 950, null, 30);</span><br><span class="line">insert into emp values (7902, &#x27;FORD&#x27;, &#x27;ANALYST&#x27;, 7566, &#x27;1981-12-03&#x27;, 3000, null, 20);</span><br><span class="line">insert into emp values (7934, &#x27;MILLER&#x27;, &#x27;CLERK&#x27;, 7782, &#x27;1982-01-23&#x27;, 1300, null, 10);</span><br></pre></td></tr></table></figure>

<p>1.查询出部门编号为30的所有员工的编号和姓名</p>
<p>2.找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</p>
<p>3.查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</p>
<p>4.列出薪金大于1500的各种工作及从事此工作的员工人数。</p>
<p>5.列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</p>
<p>6.查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L  __</p>
<p>7.查询每种工作的最高工资、最低工资、人数</p>
<p>8.列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</p>
<p>9.列出薪金  高于  在部门30工作的  所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; use ruozedata</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; create table dept (</span><br><span class="line">    -&gt;     deptno numeric(2),</span><br><span class="line">    -&gt;     dname varchar(14),</span><br><span class="line">    -&gt;     loc varchar(13)</span><br><span class="line">    -&gt; );</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into dept values (10, &#x27;ACCOUNTING&#x27;, &#x27;NEW YORK&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into dept values (20, &#x27;RESEARCH&#x27;, &#x27;DALLAS&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into dept values (30, &#x27;SALES&#x27;, &#x27;CHICAGO&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into dept values (40, &#x27;OPERATIONS&#x27;, &#x27;BOSTON&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; create table salgrade (</span><br><span class="line">    -&gt;     grade numeric,</span><br><span class="line">    -&gt;     losal numeric,</span><br><span class="line">    -&gt;     hisal numeric</span><br><span class="line">    -&gt; );</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into salgrade values (1, 700, 1200);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into salgrade values (2, 1201, 1400);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into salgrade values (3, 1401, 2000);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into salgrade values (4, 2001, 3000);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into salgrade values (5, 3001, 9999);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; create table emp (</span><br><span class="line">    -&gt;     empno numeric(4) not null,</span><br><span class="line">    -&gt;     ename varchar(10),</span><br><span class="line">    -&gt;     job varchar(9),</span><br><span class="line">    -&gt;     mgr numeric(4),</span><br><span class="line">    -&gt;     hiredate datetime,</span><br><span class="line">    -&gt;     sal numeric(7, 2),</span><br><span class="line">    -&gt;     comm numeric(7, 2),</span><br><span class="line">    -&gt;     deptno numeric(2)</span><br><span class="line">    -&gt; );</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7369, &#x27;SMITH&#x27;, &#x27;CLERK&#x27;, 7902, &#x27;1980-12-17&#x27;, 800, null, 20);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7499, &#x27;ALLEN&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-02-20&#x27;, 1600, 300, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7521, &#x27;WARD&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-02-22&#x27;, 1250, 500, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7566, &#x27;JONES&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-04-02&#x27;, 2975, null, 20);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7654, &#x27;MARTIN&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-09-28&#x27;, 1250, 1400, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7698, &#x27;BLAKE&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-05-01&#x27;, 2850, null, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7782, &#x27;CLARK&#x27;, &#x27;MANAGER&#x27;, 7839, &#x27;1981-06-09&#x27;, 2450, null, 10);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7788, &#x27;SCOTT&#x27;, &#x27;ANALYST&#x27;, 7566, &#x27;1982-12-09&#x27;, 3000, null, 20);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7839, &#x27;KING&#x27;, &#x27;PRESIDENT&#x27;, null, &#x27;1981-11-17&#x27;, 5000, null, 10); </span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7844, &#x27;TURNER&#x27;, &#x27;SALESMAN&#x27;, 7698, &#x27;1981-09-08&#x27;, 1500, 0, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7876, &#x27;ADAMS&#x27;, &#x27;CLERK&#x27;, 7788, &#x27;1983-01-12&#x27;, 1100, null, 20);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7900, &#x27;JAMES&#x27;, &#x27;CLERK&#x27;, 7698, &#x27;1981-12-03&#x27;, 950, null, 30);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7902, &#x27;FORD&#x27;, &#x27;ANALYST&#x27;, 7566, &#x27;1981-12-03&#x27;, 3000, null, 20);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into emp values (7934, &#x27;MILLER&#x27;, &#x27;CLERK&#x27;, 7782, &#x27;1982-01-23&#x27;, 1300, null, 10);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+---------------------+</span><br><span class="line">| Tables_in_ruozedata |</span><br><span class="line">+---------------------+</span><br><span class="line">| dept                |</span><br><span class="line">| emp                 |</span><br><span class="line">| salgrade            |</span><br><span class="line">+---------------------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from dept;</span><br><span class="line">+--------+------------+----------+</span><br><span class="line">| deptno | dname      | loc      |</span><br><span class="line">+--------+------------+----------+</span><br><span class="line">|     10 | ACCOUNTING | NEW YORK |</span><br><span class="line">|     20 | RESEARCH   | DALLAS   |</span><br><span class="line">|     30 | SALES      | CHICAGO  |</span><br><span class="line">|     40 | OPERATIONS | BOSTON   |</span><br><span class="line">+--------+------------+----------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from emp;</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">| empno | ename  | job       | mgr  | hiredate            | sal     | comm    | deptno |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 00:00:00 |  800.00 |    NULL |     20 |</span><br><span class="line">|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 00:00:00 | 1600.00 |  300.00 |     30 |</span><br><span class="line">|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 00:00:00 | 1250.00 |  500.00 |     30 |</span><br><span class="line">|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 00:00:00 | 2975.00 |    NULL |     20 |</span><br><span class="line">|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 00:00:00 | 1250.00 | 1400.00 |     30 |</span><br><span class="line">|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 00:00:00 | 2850.00 |    NULL |     30 |</span><br><span class="line">|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 00:00:00 | 2450.00 |    NULL |     10 |</span><br><span class="line">|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 00:00:00 | 5000.00 |    NULL |     10 |</span><br><span class="line">|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 00:00:00 | 1500.00 |    0.00 |     30 |</span><br><span class="line">|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 00:00:00 | 1100.00 |    NULL |     20 |</span><br><span class="line">|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 00:00:00 |  950.00 |    NULL |     30 |</span><br><span class="line">|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 00:00:00 | 1300.00 |    NULL |     10 |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">14 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from salgrade;</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">| grade | losal | hisal |</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">|     1 |   700 |  1200 |</span><br><span class="line">|     2 |  1201 |  1400 |</span><br><span class="line">|     3 |  1401 |  2000 |</span><br><span class="line">|     4 |  2001 |  3000 |</span><br><span class="line">|     5 |  3001 |  9999 |</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br></pre></td></tr></table></figure>

<h2 id="一、查询出部门编号为30的所有员工的编号和姓名"><a href="#一、查询出部门编号为30的所有员工的编号和姓名" class="headerlink" title="一、查询出部门编号为30的所有员工的编号和姓名"></a>一、查询出部门编号为30的所有员工的编号和姓名</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select empno,ename from emp where deptno = 30;</span><br><span class="line">+-------+--------+</span><br><span class="line">| empno | ename  |</span><br><span class="line">+-------+--------+</span><br><span class="line">|  7499 | ALLEN  |</span><br><span class="line">|  7521 | WARD   |</span><br><span class="line">|  7654 | MARTIN |</span><br><span class="line">|  7698 | BLAKE  |</span><br><span class="line">|  7844 | TURNER |</span><br><span class="line">|  7900 | JAMES  |</span><br><span class="line">+-------+--------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="二、找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。"><a href="#二、找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。" class="headerlink" title="二、找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。"></a>二、找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from emp where job = &quot;MANAGER&quot; and deptno = 10 union all select * from emp where job = &quot;SALESMAN&quot; and deptno = 20;</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">| empno | ename | job     | mgr  | hiredate            | sal     | comm | deptno |</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">|  7782 | CLARK | MANAGER | 7839 | 1981-06-09 00:00:00 | 2450.00 | NULL |     10 |</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from emp where job = &quot;MANAGER&quot; and deptno = 10 or job = &quot;SALESMAN&quot; and deptno = 20;</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">| empno | ename | job     | mgr  | hiredate            | sal     | comm | deptno |</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">|  7782 | CLARK | MANAGER | 7839 | 1981-06-09 00:00:00 | 2450.00 | NULL |     10 |</span><br><span class="line">+-------+-------+---------+------+---------------------+---------+------+--------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="三、查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序"><a href="#三、查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序" class="headerlink" title="三、查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序"></a>三、查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from emp order by sal desc, hiredate;</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">| empno | ename  | job       | mgr  | hiredate            | sal     | comm    | deptno |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 00:00:00 | 5000.00 |    NULL |     10 |</span><br><span class="line">|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 00:00:00 | 2975.00 |    NULL |     20 |</span><br><span class="line">|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 00:00:00 | 2850.00 |    NULL |     30 |</span><br><span class="line">|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 00:00:00 | 2450.00 |    NULL |     10 |</span><br><span class="line">|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 00:00:00 | 1600.00 |  300.00 |     30 |</span><br><span class="line">|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 00:00:00 | 1500.00 |    0.00 |     30 |</span><br><span class="line">|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 00:00:00 | 1300.00 |    NULL |     10 |</span><br><span class="line">|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 00:00:00 | 1250.00 |  500.00 |     30 |</span><br><span class="line">|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 00:00:00 | 1250.00 | 1400.00 |     30 |</span><br><span class="line">|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 00:00:00 | 1100.00 |    NULL |     20 |</span><br><span class="line">|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 00:00:00 |  950.00 |    NULL |     30 |</span><br><span class="line">|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 00:00:00 |  800.00 |    NULL |     20 |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">14 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="四、列出薪金大于1500的各种工作及从事此工作的员工人数。"><a href="#四、列出薪金大于1500的各种工作及从事此工作的员工人数。" class="headerlink" title="四、列出薪金大于1500的各种工作及从事此工作的员工人数。"></a>四、列出薪金大于1500的各种工作及从事此工作的员工人数。</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select job,count(job) from emp where sal &gt; 1500 group by job ;</span><br><span class="line">+-----------+------------+</span><br><span class="line">| job       | count(job) |</span><br><span class="line">+-----------+------------+</span><br><span class="line">| ANALYST   |          2 |</span><br><span class="line">| MANAGER   |          3 |</span><br><span class="line">| PRESIDENT |          1 |</span><br><span class="line">| SALESMAN  |          1 |</span><br><span class="line">+-----------+------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="五、列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。"><a href="#五、列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。" class="headerlink" title="五、列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。"></a>五、列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select ename from emp left join dept on emp.deptno = dept.deptno where dept.dname = &quot;SALES&quot;;</span><br><span class="line">+--------+</span><br><span class="line">| ename  |</span><br><span class="line">+--------+</span><br><span class="line">| ALLEN  |</span><br><span class="line">| WARD   |</span><br><span class="line">| MARTIN |</span><br><span class="line">| BLAKE  |</span><br><span class="line">| TURNER |</span><br><span class="line">| JAMES  |</span><br><span class="line">+--------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select ename from emp</span><br><span class="line">    -&gt; where deptno=</span><br><span class="line">    -&gt; (select deptno from dept where dname=&#x27;SALES&#x27;);</span><br><span class="line">+--------+</span><br><span class="line">| ename  |</span><br><span class="line">+--------+</span><br><span class="line">| ALLEN  |</span><br><span class="line">| WARD   |</span><br><span class="line">| MARTIN |</span><br><span class="line">| BLAKE  |</span><br><span class="line">| TURNER |</span><br><span class="line">| JAMES  |</span><br><span class="line">+--------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="六、查询姓名以S开头的-以S结尾-包含S字符-第二个字母为L"><a href="#六、查询姓名以S开头的-以S结尾-包含S字符-第二个字母为L" class="headerlink" title="六、查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L"></a>六、查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select ename from emp where ename like &quot;S%&quot;;</span><br><span class="line">+-------+</span><br><span class="line">| ename |</span><br><span class="line">+-------+</span><br><span class="line">| SMITH |</span><br><span class="line">| SCOTT |</span><br><span class="line">+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select ename from emp where ename like &quot;%S&quot;;</span><br><span class="line">+-------+</span><br><span class="line">| ename |</span><br><span class="line">+-------+</span><br><span class="line">| JONES |</span><br><span class="line">| ADAMS |</span><br><span class="line">| JAMES |</span><br><span class="line">+-------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select ename from emp where ename like &quot;%S%&quot;;</span><br><span class="line">+-------+</span><br><span class="line">| ename |</span><br><span class="line">+-------+</span><br><span class="line">| SMITH |</span><br><span class="line">| JONES |</span><br><span class="line">| SCOTT |</span><br><span class="line">| ADAMS |</span><br><span class="line">| JAMES |</span><br><span class="line">+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select ename from emp where ename like &quot;_L%&quot;;</span><br><span class="line">+-------+</span><br><span class="line">| ename |</span><br><span class="line">+-------+</span><br><span class="line">| ALLEN |</span><br><span class="line">| BLAKE |</span><br><span class="line">| CLARK |</span><br><span class="line">+-------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="七、查询每种工作的最高工资、最低工资、人数"><a href="#七、查询每种工作的最高工资、最低工资、人数" class="headerlink" title="七、查询每种工作的最高工资、最低工资、人数"></a>七、查询每种工作的最高工资、最低工资、人数</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select job,max(sal+ifnull(comm,0)) as sal_max,min(sal+ifnull(comm,0)) as sal_min,count(empno) as count from emp group by job;</span><br><span class="line">+-----------+---------+---------+-------+</span><br><span class="line">| job       | sal_max | sal_min | count |</span><br><span class="line">+-----------+---------+---------+-------+</span><br><span class="line">| ANALYST   | 3000.00 | 3000.00 |     2 |</span><br><span class="line">| CLERK     | 1300.00 |  800.00 |     4 |</span><br><span class="line">| MANAGER   | 2975.00 | 2450.00 |     3 |</span><br><span class="line">| PRESIDENT | 5000.00 | 5000.00 |     1 |</span><br><span class="line">| SALESMAN  | 2650.00 | 1500.00 |     4 |</span><br><span class="line">+-----------+---------+---------+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="八、列出薪金-高于-公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级"><a href="#八、列出薪金-高于-公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级" class="headerlink" title="八、列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级"></a>八、列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</h2><ol>
<li><p>找出平均薪金：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select avg(sal+ifnull(comm,0)) from emp;</span><br><span class="line">+-------------------------+</span><br><span class="line">| avg(sal+ifnull(comm,0)) |</span><br><span class="line">+-------------------------+</span><br><span class="line">|             2230.357143 |</span><br><span class="line">+-------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>薪金 高于 公司平均薪金的所有员工号,员工姓名,工资：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select empno,ename,sal+ifnull(comm,0) as empsal from emp where sal+ifnull(comm,0) &gt; (select avg(sal+ifnull(comm,0)) from emp);</span><br><span class="line">+-------+--------+---------+</span><br><span class="line">| empno | ename  | empsal  |</span><br><span class="line">+-------+--------+---------+</span><br><span class="line">|  7566 | JONES  | 2975.00 |</span><br><span class="line">|  7654 | MARTIN | 2650.00 |</span><br><span class="line">|  7698 | BLAKE  | 2850.00 |</span><br><span class="line">|  7782 | CLARK  | 2450.00 |</span><br><span class="line">|  7788 | SCOTT  | 3000.00 |</span><br><span class="line">|  7839 | KING   | 5000.00 |</span><br><span class="line">|  7902 | FORD   | 3000.00 |</span><br><span class="line">+-------+--------+---------+</span><br><span class="line">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>
<li><p>加上部门名称,工资等级</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select </span><br><span class="line">    -&gt; empno,ename,sal+ifnull(comm,0) as empsal,dname,grade</span><br><span class="line">    -&gt; from emp left join dept </span><br><span class="line">    -&gt; on emp.deptno = dept.deptno </span><br><span class="line">    -&gt; left join salgrade</span><br><span class="line">    -&gt; on sal+ifnull(comm,0) between losal and hisal</span><br><span class="line">    -&gt; where </span><br><span class="line">    -&gt; sal+ifnull(comm,0) &gt; (select avg(sal+ifnull(comm,0)) from emp);</span><br><span class="line">+-------+--------+---------+------------+-------+</span><br><span class="line">| empno | ename  | empsal  | dname      | grade |</span><br><span class="line">+-------+--------+---------+------------+-------+</span><br><span class="line">|  7782 | CLARK  | 2450.00 | ACCOUNTING |     4 |</span><br><span class="line">|  7566 | JONES  | 2975.00 | RESEARCH   |     4 |</span><br><span class="line">|  7788 | SCOTT  | 3000.00 | RESEARCH   |     4 |</span><br><span class="line">|  7902 | FORD   | 3000.00 | RESEARCH   |     4 |</span><br><span class="line">|  7654 | MARTIN | 2650.00 | SALES      |     4 |</span><br><span class="line">|  7698 | BLAKE  | 2850.00 | SALES      |     4 |</span><br><span class="line">|  7839 | KING   | 5000.00 | ACCOUNTING |     5 |</span><br><span class="line">+-------+--------+---------+------------+-------+</span><br><span class="line">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>
<li><p>加上领导名称</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select </span><br><span class="line">    -&gt; e1.empno,e1.ename,e1.sal+ifnull(e1.comm,0) as empsal,dname,grade,e2.ename as mgrname</span><br><span class="line">    -&gt; from emp e1 </span><br><span class="line">    -&gt; left join emp e2 on e1.mgr = e2.empno </span><br><span class="line">    -&gt; left join dept on e1.deptno = dept.deptno </span><br><span class="line">    -&gt; left join salgrade on e1.sal+ifnull(e1.comm,0) between losal and hisal</span><br><span class="line">    -&gt; where e1.sal+ifnull(e1.comm,0) &gt; (select avg(sal+ifnull(comm,0)) from emp);</span><br><span class="line">+-------+--------+---------+------------+-------+---------+</span><br><span class="line">| empno | ename  | empsal  | dname      | grade | mgrname |</span><br><span class="line">+-------+--------+---------+------------+-------+---------+</span><br><span class="line">|  7782 | CLARK  | 2450.00 | ACCOUNTING |     4 | KING    |</span><br><span class="line">|  7788 | SCOTT  | 3000.00 | RESEARCH   |     4 | JONES   |</span><br><span class="line">|  7902 | FORD   | 3000.00 | RESEARCH   |     4 | JONES   |</span><br><span class="line">|  7566 | JONES  | 2975.00 | RESEARCH   |     4 | KING    |</span><br><span class="line">|  7654 | MARTIN | 2650.00 | SALES      |     4 | BLAKE   |</span><br><span class="line">|  7698 | BLAKE  | 2850.00 | SALES      |     4 | KING    |</span><br><span class="line">|  7839 | KING   | 5000.00 | ACCOUNTING |     5 | NULL    |</span><br><span class="line">+-------+--------+---------+------------+-------+---------+</span><br><span class="line">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="九、列出薪金-高于-在部门30工作的-所有-任意一个员工的薪金的员工姓名和薪金、部门名称。"><a href="#九、列出薪金-高于-在部门30工作的-所有-任意一个员工的薪金的员工姓名和薪金、部门名称。" class="headerlink" title="九、列出薪金  高于  在部门30工作的  所有/任意一个员工的薪金的员工姓名和薪金、部门名称。"></a>九、列出薪金  高于  在部门30工作的  所有/任意一个员工的薪金的员工姓名和薪金、部门名称。</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select ename,sal+ifnull(comm,0) as empsal,dname  from emp  left join dept on emp.deptno = dept.deptno where sal+ifnull(comm,0) &gt; ALL( select sal+ifnull(comm,0) from emp where deptno=30 );</span><br><span class="line">+-------+---------+------------+</span><br><span class="line">| ename | empsal  | dname      |</span><br><span class="line">+-------+---------+------------+</span><br><span class="line">| KING  | 5000.00 | ACCOUNTING |</span><br><span class="line">| JONES | 2975.00 | RESEARCH   |</span><br><span class="line">| SCOTT | 3000.00 | RESEARCH   |</span><br><span class="line">| FORD  | 3000.00 | RESEARCH   |</span><br><span class="line">+-------+---------+------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select ename,sal+ifnull(comm,0) as empsal,dname  from emp  left join dept on emp.deptno = dept.deptno where sal+ifnull(comm,0) &gt; ANY( select sal+ifnull(comm,0) from emp where deptno==30 );</span><br><span class="line">+--------+---------+------------+</span><br><span class="line">| ename  | empsal  | dname      |</span><br><span class="line">+--------+---------+------------+</span><br><span class="line">| CLARK  | 2450.00 | ACCOUNTING |</span><br><span class="line">| KING   | 5000.00 | ACCOUNTING |</span><br><span class="line">| MILLER | 1300.00 | ACCOUNTING |</span><br><span class="line">| JONES  | 2975.00 | RESEARCH   |</span><br><span class="line">| SCOTT  | 3000.00 | RESEARCH   |</span><br><span class="line">| ADAMS  | 1100.00 | RESEARCH   |</span><br><span class="line">| FORD   | 3000.00 | RESEARCH   |</span><br><span class="line">| ALLEN  | 1900.00 | SALES      |</span><br><span class="line">| WARD   | 1750.00 | SALES      |</span><br><span class="line">| MARTIN | 2650.00 | SALES      |</span><br><span class="line">| BLAKE  | 2850.00 | SALES      |</span><br><span class="line">| TURNER | 1500.00 | SALES      |</span><br><span class="line">+--------+---------+------------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>YARN and MapReduce的【内存】优化配置详解</title>
    <url>/2021/12/15/YARN-and-MapReduce%E7%9A%84%E3%80%90%E5%86%85%E5%AD%98%E3%80%91%E4%BC%98%E5%8C%96%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="YARN-and-MapReduce的【内存】优化配置详解"><a href="#YARN-and-MapReduce的【内存】优化配置详解" class="headerlink" title="YARN and MapReduce的【内存】优化配置详解"></a>YARN and MapReduce的【内存】优化配置详解</h1><p>在Hadoop2.x中, YARN负责管理MapReduce中的资源(内存, CPU等)并且将其打包成Container。<br>使之专注于其擅长的数据处理任务, 将无需考虑资源调度. 如下图所示<br><img src="http://img.blog.itpub.net/blog/attachment/201611/5/30089851_14783139737Lm1.png?x-oss-process=style/bb" alt="img"><br>YARN会管理集群中所有机器的可用计算资源. 基于这些资源YARN会调度应用(比如MapReduce)发来的资源请求, 然后YARN会通过分配Container来给每个应用提供处理能力, Container是YARN中处理能力的基本单元, 是对内存, CPU等的封装。</p>
<span id="more"></span>

<p>目前我这里的服务器情况：6台slave，每台：32G内存，2*6核CPU。</p>
<p>由于hadoop 1.x存在JobTracker和TaskTracker，资源管理有它们实现，在执行mapreduce作业时，资源分为map task和reduce task。<br>所有存在下面两个参数分别设置每个TaskTracker可以运行的任务数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;6&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;&lt;![CDATA[CPU数量=服务器CPU总核数 / 每个CPU的核数；服务器CPU总核数 = more /proc/cpuinfo | grep &#x27;processor&#x27; | wc -l；每个CPU的核数 = more /proc/cpui nfo | grep &#x27;cpu cores&#x27;]]&gt;&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;一个task tracker最多可以同时运行的reduce任务数量&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>但是在hadoop 2.x中，引入了Yarn架构做资源管理，在每个节点上面运行NodeManager负责节点资源的分配，而slot也不再像1.x那样区分Map slot和Reduce slot。在Yarn上面Container是资源的分配的最小单元。</p>
<p>Yarn集群的内存分配配置在yarn-site.xml文件中配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;22528&lt;/value&gt;</span><br><span class="line">       &lt;discription&gt;每个节点可用内存,单位MB&lt;/discription&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1500&lt;/value&gt;</span><br><span class="line">       &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;16384&lt;/value&gt;</span><br><span class="line">       &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>由于我Yarn集群还需要跑Spark的任务，而Spark的Worker内存相对需要大些，所以需要调大单个任务的最大内存（默认为8G）。</p>
<p>而Mapreduce的任务的内存配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1500&lt;/value&gt;</span><br><span class="line">       &lt;description&gt;每个Map任务的物理内存限制&lt;/description&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;3000&lt;/value&gt;</span><br><span class="line">       &lt;description&gt;每个Reduce任务的物理内存限制&lt;/description&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;-Xmx1200m&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;-Xmx2600m&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>mapreduce.map.memory.mb：每个map任务的内存，应该是大于或者等于Container的最小内存。<br>按照上面的配置：每个slave可以运行map的数据&lt;= 22528/1500,reduce任务的数量&lt;=22528/3000 。</p>
<p>mapreduce.map.memory.mb &gt;mapreduce.map.java.opts<br>mapreduce.reduce.memory.mb &gt;mapreduce.reduce.java.opts</p>
<p>mapreduce.map.java.opts / mapreduce.map.memory.mb=0.70<del>0.80<br>mapreduce.reduce.java.opts / mapreduce.reduce.memory.mb=0.70</del>0.80</p>
<p>在yarn container这种模式下，JVM进程跑在container中，mapreduce.{map|reduce}.java.opts 能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的memory.mb，则预留些空间会存储java,scala code等。</p>
<p>原文链接：<a href="http://blog.itpub.net/30089851/viewspace-2127850">YARN and MapReduce的【内存】优化配置详解</a></p>
<h1 id="YARN的Memory和CPU调优配置详解"><a href="#YARN的Memory和CPU调优配置详解" class="headerlink" title="YARN的Memory和CPU调优配置详解"></a>YARN的Memory和CPU调优配置详解</h1><p>Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。</p>
<p>YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。</p>
<p>在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。</p>
<h1 id="内存配置"><a href="#内存配置" class="headerlink" title="内存配置"></a>内存配置</h1><p>关于<em>内存</em>相关的配置可以参考hortonwork公司的文档<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html">Determine HDP Memory Configuration Settings</a>来配置你的集群。</p>
<blockquote>
<p>YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。</p>
</blockquote>
<p>可以参考下面的表格确定应该保留的内存：</p>
<table>
<thead>
<tr>
<th align="left">每台机子内存</th>
<th align="left">系统需要的内存</th>
<th align="left">HBase需要的内存</th>
</tr>
</thead>
<tbody><tr>
<td align="left">4GB</td>
<td align="left">1GB</td>
<td align="left">1GB</td>
</tr>
<tr>
<td align="left">8GB</td>
<td align="left">2GB</td>
<td align="left">1GB</td>
</tr>
<tr>
<td align="left">16GB</td>
<td align="left">2GB</td>
<td align="left">2GB</td>
</tr>
<tr>
<td align="left">24GB</td>
<td align="left">4GB</td>
<td align="left">4GB</td>
</tr>
<tr>
<td align="left">48GB</td>
<td align="left">6GB</td>
<td align="left">8GB</td>
</tr>
<tr>
<td align="left">64GB</td>
<td align="left">8GB</td>
<td align="left">8GB</td>
</tr>
<tr>
<td align="left">72GB</td>
<td align="left">8GB</td>
<td align="left">8GB</td>
</tr>
<tr>
<td align="left">96GB</td>
<td align="left">12GB</td>
<td align="left">16GB</td>
</tr>
<tr>
<td align="left">128GB</td>
<td align="left">24GB</td>
<td align="left">24GB</td>
</tr>
<tr>
<td align="left">255GB</td>
<td align="left">32GB</td>
<td align="left">32GB</td>
</tr>
<tr>
<td align="left">512GB</td>
<td align="left">64GB</td>
<td align="left">64GB</td>
</tr>
</tbody></table>
<p>计算每台机子最多可以拥有多少个container，可以使用下面的公式:</p>
<p>containers = min (2<em>CORES, 1.8</em>DISKS, (Total available RAM) / MIN_CONTAINER_SIZE)</p>
<p>说明：</p>
<ul>
<li>CORES为机器CPU核数</li>
<li>DISKS为机器上挂载的磁盘个数</li>
<li>Total available RAM为机器总内存</li>
<li>MIN_CONTAINER_SIZE是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格：</li>
</ul>
<table>
<thead>
<tr>
<th align="left">每台机子可用的RAM</th>
<th align="left">container最小值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">小于4GB</td>
<td align="left">256MB</td>
</tr>
<tr>
<td align="left">4GB到8GB之间</td>
<td align="left">512MB</td>
</tr>
<tr>
<td align="left">8GB到24GB之间</td>
<td align="left">1024MB</td>
</tr>
<tr>
<td align="left">大于24GB</td>
<td align="left">2048MB</td>
</tr>
</tbody></table>
<p>每个container的平均使用内存大小计算方式为：</p>
<p>RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))</p>
<p>通过上面的计算，YARN以及MAPREDUCE可以这样配置：</p>
<table>
<thead>
<tr>
<th align="left">配置文件</th>
<th align="left">配置设置</th>
<th align="left">默认值</th>
<th align="left">计算值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.nodemanager.resource.memory-mb</td>
<td align="left">8192 MB</td>
<td align="left">= containers * RAM-per-container</td>
</tr>
<tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.scheduler.minimum-allocation-mb</td>
<td align="left">1024MB</td>
<td align="left">= RAM-per-container</td>
</tr>
<tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.scheduler.maximum-allocation-mb</td>
<td align="left">8192 MB</td>
<td align="left">= containers * RAM-per-container</td>
</tr>
<tr>
<td align="left">yarn-site.xml (check)</td>
<td align="left">yarn.app.mapreduce.am.resource.mb</td>
<td align="left">1536 MB</td>
<td align="left">= 2 * RAM-per-container</td>
</tr>
<tr>
<td align="left">yarn-site.xml (check)</td>
<td align="left">yarn.app.mapreduce.am.command-opts</td>
<td align="left">-Xmx1024m</td>
<td align="left">= 0.8 * 2 * RAM-per-container</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.map.memory.mb</td>
<td align="left">1024 MB</td>
<td align="left">= RAM-per-container</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.reduce.memory.mb</td>
<td align="left">1024 MB</td>
<td align="left">= 2 * RAM-per-container</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.map.java.opts</td>
<td align="left"></td>
<td align="left">= 0.8 * RAM-per-container</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.reduce.java.opts</td>
<td align="left"></td>
<td align="left">= 0.8 * 2 * RAM-per-container</td>
</tr>
</tbody></table>
<p>举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下：</p>
<p>containers = min (2<em>32, 1.8</em> 7 , (128-24)/2) = min (64, 12.6 , 51) = 13</p>
<p>计算RAM-per-container值如下：</p>
<p>RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8</p>
<p>你也可以使用脚本<a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-9.html">yarn-utils.py</a>来计算上面的值：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">import optparse</span><br><span class="line">from pprint import pprint</span><br><span class="line">import logging</span><br><span class="line">import sys</span><br><span class="line">import math</span><br><span class="line">import ast</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27; Reserved for OS + DN + NM, Map: Memory =&gt; Reservation &#x27;&#x27;&#x27;</span><br><span class="line">reservedStack = &#123; 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12,</span><br><span class="line">                   128:24, 256:32, 512:64&#125;</span><br><span class="line">&#x27;&#x27;&#x27; Reserved for HBase. Map: Memory =&gt; Reservation &#x27;&#x27;&#x27;</span><br><span class="line">  </span><br><span class="line">reservedHBase = &#123;4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16,</span><br><span class="line">                   128:24, 256:32, 512:64&#125;</span><br><span class="line">GB = 1024</span><br><span class="line"></span><br><span class="line">def getMinContainerSize(memory):</span><br><span class="line">  if (memory &lt;= 4):</span><br><span class="line">    return 256</span><br><span class="line">  elif (memory &lt;= 8):</span><br><span class="line">    return 512</span><br><span class="line">  elif (memory &lt;= 24):</span><br><span class="line">    return 1024</span><br><span class="line">  else:</span><br><span class="line">    return 2048</span><br><span class="line">  pass</span><br><span class="line"></span><br><span class="line">def getReservedStackMemory(memory):</span><br><span class="line">  if (reservedStack.has_key(memory)):</span><br><span class="line">    return reservedStack[memory]</span><br><span class="line">  if (memory &lt;= 4):</span><br><span class="line">    ret = 1</span><br><span class="line">  elif (memory &gt;= 512):</span><br><span class="line">    ret = 64</span><br><span class="line">  else:</span><br><span class="line">    ret = 1</span><br><span class="line">  return ret</span><br><span class="line"></span><br><span class="line">def getReservedHBaseMem(memory):</span><br><span class="line">  if (reservedHBase.has_key(memory)):</span><br><span class="line">    return reservedHBase[memory]</span><br><span class="line">  if (memory &lt;= 4):</span><br><span class="line">    ret = 1</span><br><span class="line">  elif (memory &gt;= 512):</span><br><span class="line">    ret = 64</span><br><span class="line">  else:</span><br><span class="line">    ret = 2</span><br><span class="line">  return ret</span><br><span class="line">                    </span><br><span class="line">def main():</span><br><span class="line">  log = logging.getLogger(__name__)</span><br><span class="line">  out_hdlr = logging.StreamHandler(sys.stdout)</span><br><span class="line">  out_hdlr.setFormatter(logging.Formatter(&#x27; %(message)s&#x27;))</span><br><span class="line">  out_hdlr.setLevel(logging.INFO)</span><br><span class="line">  log.addHandler(out_hdlr)</span><br><span class="line">  log.setLevel(logging.INFO)</span><br><span class="line">  parser = optparse.OptionParser()</span><br><span class="line">  memory = 0</span><br><span class="line">  cores = 0</span><br><span class="line">  disks = 0</span><br><span class="line">  hbaseEnabled = True</span><br><span class="line">  parser.add_option(&#x27;-c&#x27;, &#x27;--cores&#x27;, default = 16,</span><br><span class="line">                     help = &#x27;Number of cores on each host&#x27;)</span><br><span class="line">  parser.add_option(&#x27;-m&#x27;, &#x27;--memory&#x27;, default = 64,</span><br><span class="line">                    help = &#x27;Amount of Memory on each host in GB&#x27;)</span><br><span class="line">  parser.add_option(&#x27;-d&#x27;, &#x27;--disks&#x27;, default = 4,</span><br><span class="line">                    help = &#x27;Number of disks on each host&#x27;)</span><br><span class="line">  parser.add_option(&#x27;-k&#x27;, &#x27;--hbase&#x27;, default = &quot;True&quot;,</span><br><span class="line">                    help = &#x27;True if HBase is installed, False is not&#x27;)</span><br><span class="line">  (options, args) = parser.parse_args()</span><br><span class="line">  </span><br><span class="line">  cores = int (options.cores)</span><br><span class="line">  memory = int (options.memory)</span><br><span class="line">  disks = int (options.disks)</span><br><span class="line">  hbaseEnabled = ast.literal_eval(options.hbase)</span><br><span class="line">  </span><br><span class="line">  log.info(&quot;Using cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;GB&quot; +</span><br><span class="line">            &quot; disks=&quot; + str(disks) + &quot; hbase=&quot; + str(hbaseEnabled))</span><br><span class="line">  minContainerSize = getMinContainerSize(memory)</span><br><span class="line">  reservedStackMemory = getReservedStackMemory(memory)</span><br><span class="line">  reservedHBaseMemory = 0</span><br><span class="line">  if (hbaseEnabled):</span><br><span class="line">    reservedHBaseMemory = getReservedHBaseMem(memory)</span><br><span class="line">  reservedMem = reservedStackMemory + reservedHBaseMemory</span><br><span class="line">  usableMem = memory - reservedMem</span><br><span class="line">  memory -= (reservedMem)</span><br><span class="line">  if (memory &lt; 2):</span><br><span class="line">    memory = 2</span><br><span class="line">    reservedMem = max(0, memory - reservedMem)</span><br><span class="line">    </span><br><span class="line">  memory *= GB</span><br><span class="line">  </span><br><span class="line">  containers = int (min(2 * cores,</span><br><span class="line">                         min(math.ceil(1.8 * float(disks)),</span><br><span class="line">                              memory/minContainerSize)))</span><br><span class="line">  if (containers &lt;= 2):</span><br><span class="line">    containers = 3</span><br><span class="line"></span><br><span class="line">  log.info(&quot;Profile: cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;MB&quot;</span><br><span class="line">           + &quot; reserved=&quot; + str(reservedMem) + &quot;GB&quot; + &quot; usableMem=&quot;</span><br><span class="line">           + str(usableMem) + &quot;GB&quot; + &quot; disks=&quot; + str(disks))</span><br><span class="line">    </span><br><span class="line">  container_ram = abs(memory/containers)</span><br><span class="line">  if (container_ram &gt; GB):</span><br><span class="line">    container_ram = int(math.floor(container_ram / 512)) * 512</span><br><span class="line">  log.info(&quot;Num Container=&quot; + str(containers))</span><br><span class="line">  log.info(&quot;Container Ram=&quot; + str(container_ram) + &quot;MB&quot;)</span><br><span class="line">  log.info(&quot;Used Ram=&quot; + str(int (containers*container_ram/float(GB))) + &quot;GB&quot;)</span><br><span class="line">  log.info(&quot;Unused Ram=&quot; + str(reservedMem) + &quot;GB&quot;)</span><br><span class="line">  log.info(&quot;yarn.scheduler.minimum-allocation-mb=&quot; + str(container_ram))</span><br><span class="line">  log.info(&quot;yarn.scheduler.maximum-allocation-mb=&quot; + str(containers*container_ram))</span><br><span class="line">  log.info(&quot;yarn.nodemanager.resource.memory-mb=&quot; + str(containers*container_ram))</span><br><span class="line">  map_memory = container_ram</span><br><span class="line">  reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram</span><br><span class="line">  am_memory = max(map_memory, reduce_memory)</span><br><span class="line">  log.info(&quot;mapreduce.map.memory.mb=&quot; + str(map_memory))</span><br><span class="line">  log.info(&quot;mapreduce.map.java.opts=-Xmx&quot; + str(int(0.8 * map_memory)) +&quot;m&quot;)</span><br><span class="line">  log.info(&quot;mapreduce.reduce.memory.mb=&quot; + str(reduce_memory))</span><br><span class="line">  log.info(&quot;mapreduce.reduce.java.opts=-Xmx&quot; + str(int(0.8 * reduce_memory)) + &quot;m&quot;)</span><br><span class="line">  log.info(&quot;yarn.app.mapreduce.am.resource.mb=&quot; + str(am_memory))</span><br><span class="line">  log.info(&quot;yarn.app.mapreduce.am.command-opts=-Xmx&quot; + str(int(0.8*am_memory)) + &quot;m&quot;)</span><br><span class="line">  log.info(&quot;mapreduce.task.io.sort.mb=&quot; + str(int(0.4 * map_memory)))</span><br><span class="line">  pass</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">  try:</span><br><span class="line">    main()</span><br><span class="line">  except(KeyboardInterrupt, EOFError):</span><br><span class="line">    print(&quot;\nAborting ... Keyboard Interrupt.&quot;)</span><br><span class="line">    sys.exit(1)</span><br></pre></td></tr></table></figure>

<p>执行下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python yarn-utils.py -c 32 -m 128 -d 7 -k False </span><br></pre></td></tr></table></figure>

<p>返回结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Using cores=32 memory=128GB disks=7 hbase=False</span><br><span class="line">Profile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7</span><br><span class="line">Num Container=13</span><br><span class="line">Container Ram=8192MB</span><br><span class="line">Used Ram=104GB</span><br><span class="line">Unused Ram=24GB</span><br><span class="line">yarn.scheduler.minimum-allocation-mb=8192</span><br><span class="line">yarn.scheduler.maximum-allocation-mb=106496</span><br><span class="line">yarn.nodemanager.resource.memory-mb=106496</span><br><span class="line">mapreduce.map.memory.mb=8192</span><br><span class="line">mapreduce.map.java.opts=-Xmx6553m</span><br><span class="line">mapreduce.reduce.memory.mb=8192</span><br><span class="line">mapreduce.reduce.java.opts=-Xmx6553m</span><br><span class="line">yarn.app.mapreduce.am.resource.mb=8192</span><br><span class="line">yarn.app.mapreduce.am.command-opts=-Xmx6553m</span><br><span class="line">mapreduce.task.io.sort.mb=3276</span><br></pre></td></tr></table></figure>



<p>这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下：</p>
<table>
<thead>
<tr>
<th align="left">配置文件</th>
<th align="left">配置设置</th>
<th align="left">计算值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.nodemanager.resource.memory-mb</td>
<td align="left">= 52 * 2 =104 G</td>
</tr>
<tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.scheduler.minimum-allocation-mb</td>
<td align="left">= 2G</td>
</tr>
<tr>
<td align="left">yarn-site.xml</td>
<td align="left">yarn.scheduler.maximum-allocation-mb</td>
<td align="left">= 52 * 2 = 104G</td>
</tr>
<tr>
<td align="left">yarn-site.xml (check)</td>
<td align="left">yarn.app.mapreduce.am.resource.mb</td>
<td align="left">= 2 * 2=4G</td>
</tr>
<tr>
<td align="left">yarn-site.xml (check)</td>
<td align="left">yarn.app.mapreduce.am.command-opts</td>
<td align="left">= 0.8 * 2 * 2=3.2G</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.map.memory.mb</td>
<td align="left">= 2G</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.reduce.memory.mb</td>
<td align="left">= 2 * 2=4G</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.map.java.opts</td>
<td align="left">= 0.8 * 2=1.6G</td>
</tr>
<tr>
<td align="left">mapred-site.xml</td>
<td align="left">mapreduce.reduce.java.opts</td>
<td align="left">= 0.8 * 2 * 2=3.2G</td>
</tr>
</tbody></table>
<p>对应的xml配置为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;106496&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;106496&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;-Xmx3276m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>另外，还有一下几个参数：</p>
<ul>
<li>yarn.nodemanager.vmem-pmem-ratio：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。</li>
<li>yarn.nodemanager.pmem-check-enabled：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。</li>
<li>yarn.nodemanager.vmem-pmem-ratio：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。</li>
</ul>
<p>第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。</p>
<h1 id="CPU配置"><a href="#CPU配置" class="headerlink" title="CPU配置"></a>CPU配置</h1><p>YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。</p>
<p>在YARN中，CPU相关配置参数如下：</p>
<ul>
<li>yarn.nodemanager.resource.cpu-vcores：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。</li>
<li>yarn.scheduler.minimum-allocation-vcores：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。</li>
<li>yarn.scheduler.maximum-allocation-vcores：单个任务可申请的最多虚拟CPU个数，默认是32。</li>
</ul>
<p>对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;31&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;124&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>原文：<a href="http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html?utm_source=tuicool&amp;utm_medium=referral">http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html?utm_source=tuicool&amp;utm_medium=referral</a></p>
]]></content>
  </entry>
  <entry>
    <title>hdfs伪分布式部署</title>
    <url>/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1 id="hdfs伪分布式部署"><a href="#hdfs伪分布式部署" class="headerlink" title="hdfs伪分布式部署"></a>hdfs伪分布式部署</h1><p><a href="https://hadoop.apache.org/release.html">Releases Archive</a>中选择要部署的版本，我们以<a href="https://hadoop.apache.org/release/3.2.2.html">Release 3.2.2 available</a>版本为例</p>
<p>参考文档：<a href="https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop: Setting up a Single Node Cluster.</a></p>
<h2 id="一、部署"><a href="#一、部署" class="headerlink" title="一、部署"></a>一、部署</h2><h3 id="1-软件要求"><a href="#1-软件要求" class="headerlink" title="1.软件要求"></a>1.软件要求</h3><ul>
<li>Java：Hadoop对Java版本有要求，具体参考<a href="https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions">Hadoop Java Versions</a>，基本上Java8通用</li>
<li>ssh</li>
</ul>
<p>补充：组件名称<code>大写-数字</code>，如：SPARK-2908，表明该组件是有问题的</p>
<span id="more"></span>

<h3 id="2-tar包解压"><a href="#2-tar包解压" class="headerlink" title="2.tar包解压"></a>2.tar包解压</h3><p>点击下载:<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz">hadoop-3.2.2.tar.gz</a></p>
<p>下载后通过rz命令上传至Linux系统</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ ll software/</span><br><span class="line">total 805204</span><br><span class="line">-rw-r--r--.  1 hadoop hadoop 395448622 Nov 21 10:03 hadoop-3.2.2.tar.gz</span><br></pre></td></tr></table></figure>

<p>解压hadoop到app目录下，创建软连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ tar -xzvf software/hadoop-3.2.2.tar.gz -C app/</span><br><span class="line">[hadoop@hadoop001 ~]$ cd app</span><br><span class="line">[hadoop@hadoop001 app]$ </span><br><span class="line">[hadoop@hadoop001 app]$ ln -s /home/hadoop/app/hadoop-3.2.2 hadoop</span><br><span class="line">[hadoop@hadoop001 app]$ ll</span><br><span class="line">total 2</span><br><span class="line">lrwxrwxrwx. 1 hadoop hadoop   29 Nov 25 16:28 hadoop -&gt; /home/hadoop/app/hadoop-3.2.2</span><br><span class="line">drwxr-xr-x. 9 hadoop hadoop 4096 Jan  3  2021 hadoop-3.2.2</span><br></pre></td></tr></table></figure>

<h3 id="3-查看文件目录"><a href="#3-查看文件目录" class="headerlink" title="3.查看文件目录"></a>3.查看文件目录</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 app]$ cd hadoop/</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll</span><br><span class="line">total 216</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 bin				#命令执行脚本</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 etc				#配置文件</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 Jan  3  2021 include</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:14 input</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 lib</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 libexec</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 150569 Dec  5  2020 LICENSE.txt</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop   4096 Nov 21 10:48 logs</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  21943 Dec  5  2020 NOTICE.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Nov 21 11:01 output</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop   1361 Dec  5  2020 README.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 Jan  3  2021 sbin			#启动停止脚本</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 Jan  3  2021 share</span><br></pre></td></tr></table></figure>

<p>大部分的大数据项目解压后目录：bin</p>
<h3 id="4-手动配置Java环境变量（必须）"><a href="#4-手动配置Java环境变量（必须）" class="headerlink" title="4.手动配置Java环境变量（必须）"></a>4.手动配置Java环境变量（必须）</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># The java implementation to use. By default, this environment</span><br><span class="line"># variable is REQUIRED on ALL platforms except OS X!</span><br><span class="line"># export JAVA_HOME=/usr/java/latest</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br></pre></td></tr></table></figure>

<h3 id="5-执行bin-hadoop查看hdfs使用说明"><a href="#5-执行bin-hadoop查看hdfs使用说明" class="headerlink" title="5.执行bin/hadoop查看hdfs使用说明"></a>5.执行bin/hadoop查看hdfs使用说明</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ bin/hadoop</span><br><span class="line">Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]</span><br><span class="line"> or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]</span><br><span class="line">  where CLASSNAME is a user-provided Java class</span><br><span class="line"></span><br><span class="line">  OPTIONS is none or any of:</span><br><span class="line"></span><br><span class="line">buildpaths                       attempt to add class files from build tree</span><br><span class="line">--config dir                     Hadoop config directory</span><br><span class="line">--debug                          turn on shell script debug mode</span><br><span class="line">--help                           usage information</span><br><span class="line">hostnames list[,of,host,names]   hosts to use in slave mode</span><br><span class="line">hosts filename                   list of hosts to use in slave mode</span><br><span class="line">loglevel level                   set the log4j level for this command</span><br><span class="line">workers                          turn on worker mode</span><br><span class="line"></span><br><span class="line">  SUBCOMMAND is one of:</span><br><span class="line">  </span><br><span class="line">    Admin Commands:</span><br><span class="line"></span><br><span class="line">daemonlog     get/set the log level for each daemon</span><br><span class="line"></span><br><span class="line">    Client Commands:</span><br><span class="line"></span><br><span class="line">archive       create a Hadoop archive</span><br><span class="line">checknative   check native Hadoop and compression libraries availability</span><br><span class="line">classpath     prints the class path needed to get the Hadoop jar and the required libraries</span><br><span class="line">conftest      validate configuration XML files</span><br><span class="line">credential    interact with credential providers</span><br><span class="line">distch        distributed metadata changer</span><br><span class="line">distcp        copy file or directories recursively</span><br><span class="line">dtutil        operations related to delegation tokens</span><br><span class="line">envvars       display computed Hadoop environment variables</span><br><span class="line">fs            run a generic filesystem user client</span><br><span class="line">gridmix       submit a mix of synthetic job, modeling a profiled from production load</span><br><span class="line">jar &lt;jar&gt;     run a jar file. NOTE: please use &quot;yarn jar&quot; to launch YARN applications, not</span><br><span class="line">              this command.</span><br><span class="line">jnipath       prints the java.library.path</span><br><span class="line">kdiag         Diagnose Kerberos Problems</span><br><span class="line">kerbname      show auth_to_local principal conversion</span><br><span class="line">key           manage keys via the KeyProvider</span><br><span class="line">rumenfolder   scale a rumen input trace</span><br><span class="line">rumentrace    convert logs into a rumen trace</span><br><span class="line">s3guard       manage metadata on S3</span><br><span class="line">trace         view and modify Hadoop tracing settings</span><br><span class="line">version       print the version</span><br><span class="line"></span><br><span class="line">    Daemon Commands:</span><br><span class="line"></span><br><span class="line">kms           run KMS, the Key Management Server</span><br><span class="line"></span><br><span class="line">SUBCOMMAND may print help when invoked w/o parameters or with -h.</span><br></pre></td></tr></table></figure>

<h3 id="6-修改配置文件：伪分布式部署"><a href="#6-修改配置文件：伪分布式部署" class="headerlink" title="6.修改配置文件：伪分布式部署"></a>6.修改配置文件：伪分布式部署</h3><ul>
<li>前置修改： <strong>/etc/host</strong></li>
</ul>
<p>通过命令<code>ifconfig</code>找到本机ip地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 00:0C:29:E2:5A:5E  </span><br><span class="line">          inet addr:XXX.XXX.XXX.XXX//这个ip地址</span><br><span class="line">          inet6 addr: fea2::24c:29fs:fee2:5a7e/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:2742 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:3033 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:267707 (261.4 KiB)  TX bytes:1724144 (1.6 MiB)</span><br></pre></td></tr></table></figure>

<p><code>vi  /etc/host</code>修改域名与ip的对应关系（注意，在后面追加即可，前面的信息不要修改）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat /etc/host</span><br><span class="line">cat: /etc/host: No such file or directory</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">XXX.XXX.XXX.XXX hadoop001</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/core-site.xml</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/core-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>我的域名：hadoop001,所以</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>个人补充：可顺便添加以下代码更改tmp目录地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/hdfs-site.xml</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hdfs-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>个人补充：可顺便添加以下代码（我的域名：hadoop001）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop001:9868&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop001:9869&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>个人补充：</strong></p>
<ul>
<li><strong>etc/hadoop/workers</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/workers</span><br><span class="line">hadoop001</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>etc/hadoop/hadoop-env.sh</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ vi etc/hadoop/hadoop-env.sh</span><br><span class="line"># Where pid files are stored.  /tmp by default.</span><br><span class="line"># export HADOOP_PID_DIR=/tmp</span><br><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>

<h3 id="7-设置SSH私钥取消密码"><a href="#7-设置SSH私钥取消密码" class="headerlink" title="7.设置SSH私钥取消密码"></a>7.设置<em>SSH</em>私钥取消密码</h3><p>通过命令<code>ssh localhost</code>检查是否可以用ssh免密连接到localhost</p>
<ul>
<li>成功：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Sat Oct  9 17:05:54 2021 from localhost</span><br></pre></td></tr></table></figure>

<ul>
<li>失败：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">hadoop@localhost&#x27;s password: </span><br><span class="line">Permission denied, please try again.</span><br></pre></td></tr></table></figure>

<p>执行以下命令：<code>ssh-keygen</code>，然后回车两次，若有Overwrite (y/n)?，则输入 y回车</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh-keygen</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): </span><br><span class="line">/home/hadoop/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/hadoop/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">9c:1c:53:05:cd:dc:23:1b:51:62:06:b0:92:57:66:da hadoop@hadoop001</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+--[ RSA 2048]----+</span><br><span class="line">|        ..BB*+ . |</span><br><span class="line">|       . O o*.o  |</span><br><span class="line">|      o * E  +.  |</span><br><span class="line">|       = + .     |</span><br><span class="line">|       S         |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">|                 |</span><br><span class="line">+-----------------+</span><br><span class="line">[hadoop@hadoop001 hadoop]$ ll ~/.ssh/</span><br><span class="line">total 16</span><br><span class="line">-rw-------. 1 hadoop hadoop  796 Nov 21 10:34 authorized_keys</span><br><span class="line">-rw-------. 1 hadoop hadoop 1675 Nov 25 17:04 id_rsa</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  398 Nov 25 17:04 id_rsa.pub</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  798 Nov 21 10:29 known_hosts</span><br></pre></td></tr></table></figure>

<p>添加ssh密钥到authorized_keys中,更改权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">[hadoop@hadoop001 hadoop]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ssh localhost</span><br><span class="line">Last login: Thu Nov 25 16:57:28 2021 from localhost</span><br></pre></td></tr></table></figure>

<h2 id="二、执行"><a href="#二、执行" class="headerlink" title="二、执行"></a>二、执行</h2><p>我的系统中环境变量配置了HADOOP_HOME：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br></pre></td></tr></table></figure>

<h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><p>以下介绍为在本地执行一个MapReduce任务：</p>
<h4 id="1-格式化文件系统"><a href="#1-格式化文件系统" class="headerlink" title="1.格式化文件系统"></a>1.格式化文件系统</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h4 id="2-启动NameNode节点和DataNode节点"><a href="#2-启动NameNode节点和DataNode节点" class="headerlink" title="2.启动NameNode节点和DataNode节点"></a>2.启动NameNode节点和DataNode节点</h4><p>The hadoop daemon log output is written to the <code>$HADOOP_LOG_DIR</code> directory (defaults to <code>$HADOOP_HOME/logs</code>).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br></pre></td></tr></table></figure>

<p>启动后，可以用jps命令查看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">6530 Jps</span><br><span class="line">6355 SecondaryNameNode</span><br><span class="line">6197 DataNode</span><br><span class="line">6089 NameNode</span><br></pre></td></tr></table></figure>

<hr>
<p>个人补充：jps后发现DataNode节点丢失，没在运行。原因大概是我格式化太多次namenode导致csid不同步，网上解决办法是data和name文件夹的dfs/data/cruurt/VERSION的id进行同步。最终个人解决方法如下：</p>
<ol>
<li><p>找到自己临时文档/tmp/即core.site.xml文件中的/home/hadoop/data/tmp路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ cat etc/hadoop/core-site.xml </span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>删除<code>home/hadoop/tmp</code>目录下文件，重新格式化Namenode</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ rm -rf tmp/*</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li>
<li><p>重启hdfs，问题解决</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [hadoop001]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop001]</span><br><span class="line">2021-11-25 18:17:14,032 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">14148 SecondaryNameNode</span><br><span class="line">13991 DataNode</span><br><span class="line">13883 NameNode</span><br><span class="line">14270 Jps</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-通过浏览器访问NameNode"><a href="#3-通过浏览器访问NameNode" class="headerlink" title="3.通过浏览器访问NameNode"></a>3.通过浏览器访问NameNode</h4><ul>
<li>NameNode - <code>http://localhost:9870/</code></li>
<li>hadoop 2.x版本是50070端口，现在版本是9870端口</li>
</ul>
<h4 id="4-在hdfs中创建目录执行MapReduce-jobs"><a href="#4-在hdfs中创建目录执行MapReduce-jobs" class="headerlink" title="4.在hdfs中创建目录执行MapReduce jobs"></a>4.在hdfs中创建目录执行MapReduce jobs</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:58 /user</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir /user/hadoop</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 19:59 /user/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="5-复制input文件夹下的文件到hdfs系统中"><a href="#5-复制input文件夹下的文件到hdfs系统中" class="headerlink" title="5.复制input文件夹下的文件到hdfs系统中"></a>5.复制input文件夹下的文件到hdfs系统中</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -mkdir input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -put etc/hadoop/*.xml input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-11-25 20:05 /user/hadoop/input</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/input</span><br><span class="line">Found 9 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       9213 2021-11-25 20:05 /user/hadoop/input/capacity-scheduler.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        975 2021-11-25 20:05 /user/hadoop/input/core-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup      11392 2021-11-25 20:05 /user/hadoop/input/hadoop-policy.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       1068 2021-11-25 20:05 /user/hadoop/input/hdfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        620 2021-11-25 20:05 /user/hadoop/input/httpfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       3518 2021-11-25 20:05 /user/hadoop/input/kms-acls.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        682 2021-11-25 20:05 /user/hadoop/input/kms-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        758 2021-11-25 20:05 /user/hadoop/input/mapred-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        690 2021-11-25 20:05 /user/hadoop/input/yarn-site.xml</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure>

<p>可以看到，在第一句命令中，input的路径并没有写成<code>/user/hadoop/input</code>，因为执行后会在当前用户的路径下执行</p>
<h4 id="6-执行MapReduce任务"><a href="#6-执行MapReduce任务" class="headerlink" title="6.执行MapReduce任务"></a>6.执行MapReduce任务</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">2021-11-25 20:10:12,608 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-11-25 20:10:13,702 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2021-11-25 20:10:13,807 INFO impl.MetricsSystemImpl: JobTracker metrics system started</span><br><span class="line">2021-11-25 20:10:14,497 INFO input.FileInputFormat: Total input files to process : 9</span><br><span class="line">......</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=232</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=90</span><br></pre></td></tr></table></figure>

<h4 id="7-查看执行结果"><a href="#7-查看执行结果" class="headerlink" title="7.查看执行结果"></a>7.查看执行结果</h4><ul>
<li><p>直接在hdfs系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs  -ls /user/hadoop/output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-11-25 20:10 /user/hadoop/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         90 2021-11-25 20:10 /user/hadoop/output/part-r-00000</span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -cat output/*</span><br><span class="line">cat: `output/output&#x27;: No such file or directory</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
<li><p>拿到linux系统上看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -get output output</span><br><span class="line">[hadoop@hadoop001 hadoop]$ cat output/*</span><br><span class="line">1	dfsadmin</span><br><span class="line">1	dfs.replication</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="8-停止服务"><a href="#8-停止服务" class="headerlink" title="8.停止服务"></a>8.停止服务</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ stop-dfs.sh </span><br><span class="line">Stopping namenodes on [hadoop001]</span><br><span class="line">Stopping datanodes</span><br><span class="line">Stopping secondary namenodes [hadoop001]</span><br><span class="line">[hadoop@hadoop001 hadoop]$ </span><br></pre></td></tr></table></figure>


<h3 id="YARN上执行"><a href="#YARN上执行" class="headerlink" title="YARN上执行"></a>YARN上执行</h3><p>想要在YARN上执行MapReduce任务，需要设置参数运行ResourceManager守护进程和NodeManager守护进程。下面执行的基础是已经执行上述<a href="#%E6%9C%AC%E5%9C%B0%E6%89%A7%E8%A1%8C">本地执行1~4</a>的步骤。</p>
<h4 id="1-修改配置文件"><a href="#1-修改配置文件" class="headerlink" title="1.修改配置文件"></a>1.修改配置文件</h4><ul>
<li><p><strong>etc/hadoop/mapred-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li>
<li><p><strong>etc/hadoop/yarn-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>个人：HADOOP_CONF_DIR还没配置</p>
</li>
</ul>
<h4 id="2-启动ResourceManager守护进程和NodeManager守护进程"><a href="#2-启动ResourceManager守护进程和NodeManager守护进程" class="headerlink" title="2.启动ResourceManager守护进程和NodeManager守护进程"></a>2.启动ResourceManager守护进程和NodeManager守护进程</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ start-yarn.sh </span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br><span class="line">20592 Jps</span><br><span class="line">18579 NameNode</span><br><span class="line">20392 ResourceManager</span><br><span class="line">18843 SecondaryNameNode</span><br><span class="line">18686 DataNode</span><br><span class="line">20495 NodeManager</span><br></pre></td></tr></table></figure>

<h4 id="3-通过浏览器访问访问ResourceManager"><a href="#3-通过浏览器访问访问ResourceManager" class="headerlink" title="3.通过浏览器访问访问ResourceManager"></a>3.通过浏览器访问访问ResourceManager</h4><ul>
<li>ResourceManager - <code>http://localhost:8088/</code></li>
</ul>
<h4 id="4-执行MapReduce任务"><a href="#4-执行MapReduce任务" class="headerlink" title="4.执行MapReduce任务"></a>4.执行MapReduce任务</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">2021-11-26 08:15:32,639 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2021-11-26 08:15:34,024 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">2021-11-26 08:15:35,285 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:36,280 INFO input.FileInputFormat: Total input files to process : 9</span><br><span class="line">2021-11-26 08:15:36,379 INFO mapreduce.JobSubmitter: number of splits:9</span><br><span class="line">2021-11-26 08:15:36,974 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:36,976 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">2021-11-26 08:15:37,319 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">2021-11-26 08:15:37,320 INFO resource.ResourceUtils: Unable to find &#x27;resource-types.xml&#x27;.</span><br><span class="line">2021-11-26 08:15:37,878 INFO impl.YarnClientImpl: Submitted application application_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:37,945 INFO mapreduce.Job: The url to track the job: http://hadoop001:8088/proxy/application_1637885511879_0001/</span><br><span class="line">2021-11-26 08:15:37,945 INFO mapreduce.Job: Running job: job_1637885511879_0001</span><br><span class="line">2021-11-26 08:15:55,539 INFO mapreduce.Job: Job job_1637885511879_0001 running in uber mode : false</span><br><span class="line">2021-11-26 08:15:55,550 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">2021-11-26 08:16:43,204 INFO mapreduce.Job:  map 44% reduce 0%</span><br><span class="line">2021-11-26 08:16:44,369 INFO mapreduce.Job:  map 67% reduce 0%</span><br><span class="line">2021-11-26 08:17:05,488 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">2021-11-26 08:17:06,495 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2021-11-26 08:17:07,508 INFO mapreduce.Job: Job job_1637885511879_0001 completed successfully</span><br><span class="line">2021-11-26 08:17:07,638 INFO mapreduce.Job: Counters: 55</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=128</span><br><span class="line">		FILE: Number of bytes written=2350649</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=29993</span><br><span class="line">		HDFS: Number of bytes written=232</span><br><span class="line">		HDFS: Number of read operations=32</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">		HDFS: Number of bytes read erasure-coded=0</span><br><span class="line">	Job Counters </span><br><span class="line">		Killed map tasks=1</span><br><span class="line">		Launched map tasks=10</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=10</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=333545</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=18937</span><br><span class="line">		Total time spent by all map tasks (ms)=333545</span><br><span class="line">		Total time spent by all reduce tasks (ms)=18937</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=333545</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=18937</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=341550080</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=19391488</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=781</span><br><span class="line">		Map output records=4</span><br><span class="line">		Map output bytes=114</span><br><span class="line">		Map output materialized bytes=176</span><br><span class="line">		Input split bytes=1077</span><br><span class="line">		Combine input records=4</span><br><span class="line">		Combine output records=4</span><br><span class="line">		Reduce input groups=4</span><br><span class="line">		Reduce shuffle bytes=176</span><br><span class="line">		Reduce input records=4</span><br><span class="line">		Reduce output records=4</span><br><span class="line">		Spilled Records=8</span><br><span class="line">		Shuffled Maps =9</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=9</span><br><span class="line">		GC time elapsed (ms)=6070</span><br><span class="line">		CPU time spent (ms)=7920</span><br><span class="line">		Physical memory (bytes) snapshot=1882726400</span><br><span class="line">		Virtual memory (bytes) snapshot=27148435456</span><br><span class="line">		Total committed heap usage (bytes)=1269469184</span><br><span class="line">		Peak Map Physical memory (bytes)=211144704</span><br><span class="line">		Peak Map Virtual memory (bytes)=2715578368</span><br><span class="line">		Peak Reduce Physical memory (bytes)=106315776</span><br><span class="line">		Peak Reduce Virtual memory (bytes)=2720391168</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=28916</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=232</span><br><span class="line">2021-11-26 08:17:07,682 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://hadoop001:9000/user/hadoop/output already exists</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)</span><br><span class="line">	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)</span><br><span class="line">	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1565)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1562)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1562)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1583)</span><br><span class="line">	at org.apache.hadoop.examples.Grep.run(Grep.java:94)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.hadoop.examples.Grep.main(Grep.java:103)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)</span><br><span class="line">	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)</span><br><span class="line">	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/11/25/hdfs%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/image-20211126093406585.png" alt="通过浏览器查看任务"></p>
<h4 id="5-停止服务"><a href="#5-停止服务" class="headerlink" title="5.停止服务"></a>5.停止服务</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ stop-yarn.sh </span><br><span class="line">Stopping nodemanagers</span><br><span class="line">localhost: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9</span><br><span class="line">Stopping resourcemanager</span><br><span class="line">[hadoop@hadoop001 hadoop]$ jps</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>关于YARN</title>
    <url>/2021/12/07/%E5%85%B3%E4%BA%8EYARN/</url>
    <content><![CDATA[<h2 id="一、YARN应用运行机制"><a href="#一、YARN应用运行机制" class="headerlink" title="一、YARN应用运行机制"></a>一、YARN应用运行机制</h2><h3 id="模块工作职能（主要架构）"><a href="#模块工作职能（主要架构）" class="headerlink" title="模块工作职能（主要架构）"></a>模块工作职能（主要架构）</h3><ul>
<li><h4 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h4><p>RM是一个全局的资源管理器，负责对各NM上的资源进行统一管理和调度，为AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager）。</p>
</li>
<li><h4 id="调度器（Scheduler）"><a href="#调度器（Scheduler）" class="headerlink" title="调度器（Scheduler）"></a>调度器（Scheduler）</h4><p>调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Scheduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为用程序分配封装在Container中的资源。调度器是可插拔的，例如CapacityScheduler、FairScheduler。（PS：在实际应用中，只需要简单配置即可）</p>
</li>
<li><h4 id="应用程序管理器（Applications-Manager）"><a href="#应用程序管理器（Applications-Manager）" class="headerlink" title="应用程序管理器（Applications Manager）"></a>应用程序管理器（Applications Manager）</h4><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动AM、监控AM运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</p>
</li>
<li><h4 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h4><p>NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。</p>
</li>
<li><h4 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h4><p>用户提交的应用程序均包含一个AM，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。如果需要，我们也可以自己写一个符合规范的YARN application。</p>
<blockquote>
<p><strong>RM只负责监控AM，在AM运行失败时候启动它，RM并不负责AM内部任务的容错，这由AM自己来完成。</strong></p>
</blockquote>
</li>
<li><h4 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h4><p>是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</p>
<blockquote>
<p><strong>Container不同于MRv1中的slot，它是一个动态资源划分单位，是根据应用程序的需求动态生成的。不会出现集群资源闲置的尴尬情况.</strong></p>
</blockquote>
</li>
</ul>
<span id="more"></span>

<h3 id="YARN应用的运行"><a href="#YARN应用的运行" class="headerlink" title="YARN应用的运行"></a>YARN应用的运行</h3><img src="/2021/12/07/%E5%85%B3%E4%BA%8EYARN/work.png" alt="work" style="zoom: 33%;">

<ol>
<li>首先客户端请求RM，运行一个application master</li>
</ol>
<ol start="2">
<li><p>RM找到可以在容器中启动application master的NM，在NM启动容器，运行application master</p>
</li>
<li><p>容器通过心跳机制向RM请求运行资源(内存和CPU)</p>
</li>
</ol>
<ol start="4">
<li><p>application master运行起来之后需要做什么依赖于客户端传递的应用</p>
<p> a. 简单地运算后直接返回结果给客户端</p>
<p> b. 请求更多容器进行分布式计算（<a href="https://k12coding.github.io/2021/11/25/Hadoop%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/#%E5%9B%9B%E3%80%81MapReduce-on-Yarn-Yarn%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B">MapReduce on YARN</a>)</p>
</li>
</ol>
<h2 id="二、Yarn与MapReduce-1相比"><a href="#二、Yarn与MapReduce-1相比" class="headerlink" title="二、Yarn与MapReduce 1相比"></a>二、Yarn与MapReduce 1相比</h2><p>MapReduce1</p>
<ul>
<li>JobTracker的职责：<ul>
<li>Job调度（将Tasks与TaskTrackers匹配）</li>
<li>Task进程监控（keeping track of tasks, restarting failed orslow tasks, and doing task bookkeeping, such as maintaining counter totals）</li>
<li>存储已经完成的job的历史信息</li>
</ul>
</li>
<li>TaskTracker的职责：运行tasks，向JobTracker发送进展报告</li>
</ul>
<p>YARN的基本思想是将<code>JobTracker</code>的两个主要功能（资源管理和作业调度/监控）分离，主要方法是创建一个全局的ResourceManager（RM）和若干个针对应用程序的ApplicationMaster（AM）。这里的应用程序是指传统的MapReduce作业或作业的DAG。</p>
<p>YARN 分层结构的本质是 ResourceManager。这个实体控制整个集群并管理应用程序向基础计算资源的分配。</p>
<p>ResourceManager将各个资源部分（计算、内存、带宽等）精心安排给基础 NodeManager。ResourceManager 还与 ApplicationMaster 一起分配资源，与 NodeManager一起启动和监视它们的基础应用程序。</p>
<p>在此上下文中，ApplicationMaster 承担了以前的 <code>TaskTracker</code> 的一些角色，ResourceManager 承担了 <code>JobTracker</code> 的角色。</p>
<p>ApplicationMaster 管理着在 YARN 内运行的应用程序的每个实例。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器的执行和资源使用（CPU、内存等的资源分配）。</p>
<table>
<thead>
<tr>
<th>MapReduce 1</th>
<th>Yarn</th>
</tr>
</thead>
<tbody><tr>
<td>Jobtracker</td>
<td>资源管理器ResourceManager、Application Master、时间轴服务器TimeLine Server</td>
</tr>
<tr>
<td>Trasktracker</td>
<td>节点管理器NodeManager</td>
</tr>
<tr>
<td>Slot</td>
<td>容器Container</td>
</tr>
</tbody></table>
<p>相比于MapReduce 1,YARN的好处包括以下几方面：</p>
<p><strong>可扩展性（Scalability）</strong></p>
<p>利用资源管理器和applicatoin master分离的架构优点分离管理资源和处理job的功能；job tracker则同时负责这两项，还要存储已完成作业历史，更包含了timeline server的功能，不利于扩展</p>
<p><strong>可用性（Availability）</strong></p>
<p>jobtracker多功能导致复杂的内存状态，难以实现高可用；YARN分而治之，先实现RM的高可用(多个RM)，再实现application master的高可用(其他NM上运行相同job)</p>
<p><strong>利用率（Utilization）</strong></p>
<p>每个tasktracker有若干固定长度的slot，可能过大或者过小；每个容器维护一个资源池，按需请求资源</p>
<p><strong>多租户（Multitenancy）</strong></p>
<p>YARN的通用性向除了MapReduce之外其他应用开放了Hadoop，如Spark，Storm等</p>
<h3 id="YARN主要优点"><a href="#YARN主要优点" class="headerlink" title="YARN主要优点"></a>YARN主要优点</h3><p>大大减小了 Yarn的<code>ResourceManager</code>资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。</p>
<p>在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 <code>AppMaster</code>，让更多类型的编程模型能够跑在 Hadoop 集群中。</p>
<p>老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsManager，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。</p>
<p><code>Container</code> 是 Yarn 为了将来作资源隔离而提出的一个框架。目前是一个框架，仅仅提供 java 虚拟机内存的隔离,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制。</p>
<h2 id="三、YARN中的调度"><a href="#三、YARN中的调度" class="headerlink" title="三、YARN中的调度"></a>三、YARN中的调度</h2><p>理想情况下，YARN应用发出的资源请求应该立刻给予满足。但现实中资源是有限的，在一个繁忙的集群中，一个应用经常需要等待才能得到所需的资源。YARN调度器的工作就是根据既定策略分配资源。YARN中提供了多种调度器和可配置策略供我们选择。hadoop3.2.2默认：</p>
<table>
<thead>
<tr>
<th>yarn.resourcemanager.scheduler.class</th>
<th>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</th>
</tr>
</thead>
</table>
<h3 id="FIFO-Scheduler（先入先出调度器）"><a href="#FIFO-Scheduler（先入先出调度器）" class="headerlink" title="FIFO Scheduler（先入先出调度器）"></a>FIFO Scheduler（先入先出调度器）</h3><p>​    FIFO调度器将应用放置在一个队列中，按照提交的顺序（先进先出）运行应用。首先为队列中第一个应用的请求分配资源，第一个应用的请求被满足后再依次处理队列中的下一个应用服务。FIFO调度器不需要任何配置，但不适合共享集群。因为大的应用会占据集群中的所有资源，每个应用都必须等待直到轮到自己运行。hadoop1.x使用的默认调度器就是FIFO。</p>
<p><img src="/2021/12/07/%E5%85%B3%E4%BA%8EYARN/FIFO.jpg" alt="FIFO"></p>
<p>​    在集群中，更适合使用Capacity Scheduler（容量调度器）或Fair Scheduler（公平调度器），这样允许长时间运行的作业能及时完成，同时也允许正在进行较小临时查询的用户能够在合理时间内得到返回结果。</p>
<h3 id="Capacity-Scheduler（容量调度器）"><a href="#Capacity-Scheduler（容量调度器）" class="headerlink" title="Capacity Scheduler（容量调度器）"></a>Capacity Scheduler（容量调度器）</h3><p>在使用容量调度器时，一个独立的专门队列保证小作业一提交就可以启动，由于队列容量是为那个队列中的作业所保留的，因此这种策略是以整个集群的利用率为代价的。这意味着与使用FIFO调度器相比，大作业执行的时间要长。</p>
<p><img src="/2021/12/07/%E5%85%B3%E4%BA%8EYARN/Capacity.jpg" alt="Capacity"></p>
<h3 id="Fair-Scheduler（公平调度器）"><a href="#Fair-Scheduler（公平调度器）" class="headerlink" title="Fair Scheduler（公平调度器）"></a>Fair Scheduler（公平调度器）</h3><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的作业之间动态平衡资源。第一个（大）作业启动时，它也是唯一运行的作业，因而获得集群中所有的资源。当第二个（小）作业启动时，它被分配到集群的一半资源，这样每个作业都能公平共享资源。</p>
<p><img src="/2021/12/07/%E5%85%B3%E4%BA%8EYARN/FAIR.jpg" alt="FAIR"></p>
<p>注意，从第二个作业的启动到获得共享资源之间会有时间滞后，因为它必须等待第一个作业使用的容器用完并释放出资源。当小作业结束且不再申请资源后，大作业将回去再次使用全部的集群资源。最终的效果是：既得到了较高的集群利用率，又能保证小作业能及时完成。</p>
<h2 id="四、容量调度器配置"><a href="#四、容量调度器配置" class="headerlink" title="四、容量调度器配置"></a>四、容量调度器配置</h2><p><code>yarn.resourcemanager.scheduler.class：org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</code></p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>​    容量调度器允许多个组织共享一个Hadoop集群，每个组织可以分配到全部集群资源的一部分。每个组织被配置一个专门的队列，每个队列被配置为可以使用一定的集群资源。队列可以进一步按层次划分，这样每个组织内的不同用户能够共该组织队列所分配的资源。在一个队列内，使用FIFO调度策略对应用进行调度。</p>
<p>​    如上面那幅图所示，单个作业使用的资源不会超过其队列容量。然而，如果队列中运行多个作业，这个队列的资源不够用且仍有可用的空闲资源，那么容量调度器可能会将空闲的资源分配给这个队列中的作业，尽管会超出队列容量。这就是**“弹性队列”(queue elasticity)**的概念。</p>
<p>​    正常的操作时，容量调度器不会通过强行中止来抢占容器强（container）。所以，如果一个队列一开始资源够用，然后随着需求增长，资源才开始不够用时，那么这个队列也就只能等其他队列释放容器资源。缓解这种情况的方法是，为队列设置一个最大容量限制，这样这个队列就不会过多侵占其他队列的容量了。这样做是牺牲队列弹性为代价，需要找到一个合理的折中值。</p>
<p>​    假设一个队列的层次结构如下：</p>
<pre><code>root 
├── prod 
└── dev    
    ├── eng    
    └── science
</code></pre>
<p>​    下面是一个基于上述队列层次的容量调度器配置文件，文件名为<code>capacity-scheduler.xml</code>。在root队列下面定义了两个子队列：<code>prod</code>和<code>dev</code>，分别占40%和60%的容量。需要注意的是，对特定队列进行配置时，是通过以下形式<code>yarn.sheduler.capacity.&lt;queue-path&gt;.&lt;sub-property&gt;</code>的配置属性进行设置的，其中<code>&lt;queue-path&gt;</code> 表示队列的层次路径（用“.”隔开），例如<code>root.prod</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>prod,dev<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.dev.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>eng,science<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.prod.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.dev.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.dev.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>75<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.dev.eng.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>50<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.dev.science.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>50<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​    可以看到，dev队列进一步被划分成eng和science两个容量相等的队列。由于dev队列的最大容量被设置为75%，因此即使prod队列空闲，dev队列也不会占用全部集群资源。换而言之，prod队列能即刻使用的可用资源比例总是能达到25%。由于其他队列没有设置最大容量限制，eng或science中的作业可能会占用dev队列的所有容量（将近75%的集群资源），而prod队列实际则可能会占用全部集群资源。</p>
<p>​    除了可以配置队列层次和容量，还有些设置是用来控制单个用户或应用能被分配到的最大资源数量、同时运行的应用数量及队列的ACL认证等属性。更多的配置内容可参考<a href="https://hadoop.apache.org/docs/r3.2.2/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html%E3%80%82">https://hadoop.apache.org/docs/r3.2.2/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html。</a></p>
<p>​    补充：关于应用放置在哪个队列，取决于应用本身。例如，在MapReduce中，可以通过设置属性<code>mapreduce.job.queuename</code>来指定要用的队列。如果队列不存在，则在提交时会发送错误。如果不指定队列，那么应用将被放在一个名为<code>default</code>的默认队列中。（队列名应该是队列层次名的最后一部分，如prod和eng是合法的队列名，root.dev.eng和dev.eng作为队列名是无效的。）</p>
<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li>层次化的队列设计，这种层次化的队列设计保证了子队列可以使用父队列设置的全部资源。这样通过层次化的管理，更容易合理分配和限制资源的使用。</li>
<li>容量保证，队列上都会设置一个资源的占比，这样可以保证每个队列都不会占用整个集群的资源。</li>
<li>安全，每个队列有严格的访问控制。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。</li>
<li>弹性分配，空闲的资源可以被分配给任何队列。当多个队列出现争用的时候，则会按照比例进行平衡。</li>
<li>多租户租用，通过队列的容量限制，多个用户就可以共享同一个集群，同时保证每个队列分配到自己的容量，提高利用率。</li>
<li>操作性，yarn支持动态修改调整容量、权限等的分配，可以在运行时直接修改。还提供给管理员界面，来显示当前的队列状况。管理员可以在运行时，添加一个队列；但是不能删除一个队列。管理员还可以在运行时暂停某个队列，这样可以保证当前的队列在执行过程中，集群不会接收其他的任务。如果一个队列被设置成了stopped，那么就不能向他或者子队列上提交任务了。</li>
<li>基于资源的调度，协调不同资源需求的应用程序，比如内存、CPU、磁盘等等。</li>
</ul>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li><p>队列属性</p>
<ul>
<li><p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.capacity</code></p>
<p>它是队列的资源容量占比(百分比)。系统繁忙时，每个队列都应该得到设置的量的资源；当系统空闲时，该队列的资源则可以被其他的队列使用。同一层的所有队列加起来必须是100%。</p>
</li>
<li><p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-capacity</code></p>
<p>队列资源的使用上限。由于系统空闲时，队列可以使用其他的空闲资源，因此最多使用的资源量则是该参数控制。默认是-1，即禁用。</p>
</li>
<li><p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.minimum-user-limit-percent</code></p>
<p>每个用户最低资源保障（百分比）。任何时刻，一个队列中每个用户可使用的资源量均有一定的限制。当一个队列中同时运行多个用户的应用程序时中，每个用户的使用资源量在一个最小值和最大值之间浮动，其中，最小值取决于正在运行的应用程序数目，而最大值则由minimum-user-limit-percent决定。比如，假设minimum-user-limit-percent为25。当两个用户向该队列提交应用程序时，每个用户可使用资源量不能超过50%，如果三个用户提交应用程序，则每个用户可使用资源量不能超多33%，如果四个或者更多用户提交应用程序，则每个用户可用资源量不能超过25%。默认是100，即不去做限制。</p>
</li>
<li><p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.user-limit-factor</code></p>
<p>每个用户最多使用的队列资源占比，如果设置为50.那么每个用户使用的资源最多就是50%。</p>
</li>
</ul>
</li>
<li><p>运行和提交应用限制</p>
<ul>
<li><p><code>yarn.scheduler.capacity.maximum-applications</code></p>
<p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-applications</code></p>
<p>设置系统中可以同时运行和等待的应用数量。默认是10000。</p>
</li>
<li><p><code>yarn.scheduler.capacity.maximum-am-resource-percent</code><br><code>yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-am-resource-percent</code></p>
<p>设置有多少资源可以用来运行app master，即控制当前激活状态的应用。默认是10%。</p>
</li>
</ul>
</li>
<li><p>队列管理</p>
<ul>
<li><p><code>yarn.scheduler.capacity.&lt;queue-path&gt;.state</code></p>
<p>队列的状态，可以使RUNNING或者STOPPED.如果队列是STOPPED状态，那么新应用不会提交到该队列或者子队列。同样，如果root被设置成STOPPED，那么整个集群都不能提交任务了。现有的应用可以等待完成，因此队列可以优雅的退出关闭。</p>
</li>
<li><p><code>yarn.scheduler.capacity.root.&lt;queue-path&gt;.acl_submit_applications</code></p>
<p>访问控制列表ACL控制谁可以向该队列提交任务。如果一个用户可以向该队列提交，那么也可以提交任务到它的子队列。</p>
</li>
<li><p><code>yarn.scheduler.capacity.root.&lt;queue-path&gt;.acl_administer_queue</code></p>
<p>设置队列的管理员的ACL控制，管理员可以控制队列的所有应用程序。同样，它也具有继承性。</p>
<p>注意：ACL的设置是<code>user1,user2 group1,group2</code>这种格式。如果是<code>*</code>则代表任何人。<code>空格</code>表示任何人都不允许。默认是<code>*</code>.</p>
</li>
</ul>
</li>
<li><p>其他属性</p>
<ul>
<li><p><code>yarn.scheduler.capacity.resource-calculator</code></p>
<p>资源计算方法，默认是<code>org.apache.hadoop.yarn.util.resource.DefaultResourseCalculator</code>,它只会计算内存。<code>DominantResourceCalculator</code>则会计算内存和CPU。</p>
</li>
<li><p><code>yarn.scheduler.capacity.node-locality-delay</code></p>
<p>调度器尝试进行调度的次数。一般都是跟集群的节点数量有关。默认40（一个机架上的节点数）<br>一旦设置完这些队列属性，就可以在web ui上看到了。可以访问下面的连接：<br>hostname:8088/scheduler</p>
</li>
</ul>
</li>
</ul>
<h3 id="修改队列配置"><a href="#修改队列配置" class="headerlink" title="修改队列配置"></a>修改队列配置</h3><p>如果想要修改队列或者调度器的配置，可以修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi $HADOOP_CONF_DIR/capacity-scheduler.xml</span><br></pre></td></tr></table></figure>

<p>修改完成后，需要执行下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$HADOOP_YARN_HOME/bin/yarn rmadmin -refreshQueues</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>队列不能被删除，只能新增。</li>
<li>更新队列的配置需要是有效的值</li>
<li>同层级的队列容量限制想加需要等于100%。</li>
</ul>
<h2 id="五、公平调度器配置"><a href="#五、公平调度器配置" class="headerlink" title="五、公平调度器配置"></a>五、公平调度器配置</h2><p><code>yarn.resourcemanager.scheduler.class：org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code></p>
<h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>​    公平调度器旨在为所有运行的应用公平分配资源。上面展示了同一个队列中的应用是如何实现资源公平享用的。然而公平共享实际也可以在多个队列间工作。</p>
<p>​    想象两个用户A和B，分别拥有自己的队列（如下图）。A启动一个作业，在B没有需求时A会分配到全部可用资源；当A的作业仍在运行时B启动一个作业，一段时间后，按照我们先前看到的方式，每个作业都用了一半的集群资源。这时候，如果B启动第二个作业且其他作业仍在运行，那么第二个作业将和B的其他作业（这里是第一个）共享资源，因此B的每个作业将占用1/4的集群资源，而A仍继续占用一半的集群资源。最终的结果就是资源在用户之间实现了公平共享。</p>
<p><img src="/2021/12/07/%E5%85%B3%E4%BA%8EYARN/fair3app.png" alt="fair3app"></p>
<h4 id="队列配置"><a href="#队列配置" class="headerlink" title="队列配置"></a>队列配置</h4><p>​    通过一个名为fair-scheduler.xml的分配文件对公平调度器进行配置，该文件位于类路径下。（可以通过设置属性yarn.scheduler.fair.allocation.file来修改文件名）。当没有该分配文件时，公平调度器的工作策略同先前所描述的一样：每个应用放置在一个以用户名命名的队列中，队列是在用户提交第一个应用时动态创建的。</p>
<p>​    假设在生产环境Yarn中，总共有四类用户需要使用集群，production、spark、default、streaming。为了使其提交的任务不受影响，我们在Yarn上规划配置了四个资源池，分别为production,spark,default,streaming。并根据实际业务情况，为每个资源池分配了相应的资源及优先级等,default用于开发测试目的.</p>
<p>ResourceManager上fair-scheduler.xml配置如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;allocations&gt;</span><br><span class="line">    &lt;queue name=&quot;root&quot;&gt;</span><br><span class="line">        &lt;aclSubmitApps&gt;&lt;/aclSubmitApps&gt;</span><br><span class="line">        &lt;aclAdministerApps&gt;&lt;/aclAdministerApps&gt;</span><br><span class="line">        &lt;queue name=&quot;production&quot;&gt;</span><br><span class="line">            &lt;minResources&gt;8192mb,8vcores&lt;/minResources&gt;</span><br><span class="line">            &lt;maxResources&gt;419840mb,125vcores&lt;/maxResources&gt;</span><br><span class="line">            &lt;maxRunningApps&gt;60&lt;/maxRunningApps&gt;</span><br><span class="line">            &lt;schedulingMode&gt;fair&lt;/schedulingMode&gt;</span><br><span class="line">            &lt;weight&gt;7.5&lt;/weight&gt;</span><br><span class="line">            &lt;aclSubmitApps&gt;*&lt;/aclSubmitApps&gt;</span><br><span class="line">            &lt;aclAdministerApps&gt;production&lt;/aclAdministerApps&gt;</span><br><span class="line">        &lt;/queue&gt;</span><br><span class="line">        &lt;queue name=&quot;spark&quot;&gt;</span><br><span class="line">            &lt;minResources&gt;8192mb,8vcores&lt;/minResources&gt;</span><br><span class="line">            &lt;maxResources&gt;376480mb,110vcores&lt;/maxResources&gt;</span><br><span class="line">            &lt;maxRunningApps&gt;50&lt;/maxRunningApps&gt;</span><br><span class="line">            &lt;schedulingMode&gt;fair&lt;/schedulingMode&gt;</span><br><span class="line">            &lt;weight&gt;1&lt;/weight&gt;</span><br><span class="line">            &lt;aclSubmitApps&gt;*&lt;/aclSubmitApps&gt;</span><br><span class="line">            &lt;aclAdministerApps&gt;spark&lt;/aclAdministerApps&gt;</span><br><span class="line">        &lt;/queue&gt;</span><br><span class="line">        &lt;queue name=&quot;default&quot;&gt;</span><br><span class="line">            &lt;minResources&gt;8192mb,8vcores&lt;/minResources&gt;</span><br><span class="line">            &lt;maxResources&gt;202400mb,20vcores&lt;/maxResources&gt;</span><br><span class="line">            &lt;maxRunningApps&gt;20&lt;/maxRunningApps&gt;</span><br><span class="line">            &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt;</span><br><span class="line">            &lt;weight&gt;0.5&lt;/weight&gt;</span><br><span class="line">            &lt;aclSubmitApps&gt;*&lt;/aclSubmitApps&gt;</span><br><span class="line">            &lt;aclAdministerApps&gt;*&lt;/aclAdministerApps&gt;</span><br><span class="line">        &lt;/queue&gt;</span><br><span class="line">        &lt;queue name=&quot;streaming&quot;&gt;</span><br><span class="line">            &lt;minResources&gt;8192mb,8vcores&lt;/minResources&gt;</span><br><span class="line">            &lt;maxResources&gt;69120mb,16vcores&lt;/maxResources&gt;</span><br><span class="line">            &lt;maxRunningApps&gt;20&lt;/maxRunningApps&gt;</span><br><span class="line">            &lt;schedulingMode&gt;fair&lt;/schedulingMode&gt;</span><br><span class="line">            &lt;aclSubmitApps&gt;*&lt;/aclSubmitApps&gt;</span><br><span class="line">            &lt;weight&gt;1&lt;/weight&gt;</span><br><span class="line">            &lt;aclAdministerApps&gt;streaming&lt;/aclAdministerApps&gt;</span><br><span class="line">        &lt;/queue&gt;</span><br><span class="line">    &lt;/queue&gt;</span><br><span class="line">    &lt;user name=&quot;production&quot;&gt;</span><br><span class="line">        &lt;!-- 对于特定用户的配置:production最多可以同时运行的任务 --&gt;</span><br><span class="line">        &lt;maxRunningApps&gt;100&lt;/maxRunningApps&gt;</span><br><span class="line">    &lt;/user&gt;</span><br><span class="line">    &lt;user name=&quot;default&quot;&gt;</span><br><span class="line">        &lt;!-- 对于默认用户配置最多可以同时运行的任务 --&gt;</span><br><span class="line">        &lt;maxRunningApps&gt;10&lt;/maxRunningApps&gt;</span><br><span class="line">    &lt;/user&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- users max running apps --&gt;</span><br><span class="line">    &lt;userMaxAppsDefault&gt;50&lt;/userMaxAppsDefault&gt;</span><br><span class="line">    &lt;!--默认的用户最多可以同时运行的任务 --&gt;</span><br><span class="line">    &lt;queuePlacementPolicy&gt;</span><br><span class="line">        &lt;rule name=&quot;specified&quot;/&gt; </span><br><span class="line">        &lt;rule name=&quot;primaryGroup&quot; create=&quot;false&quot; /&gt;</span><br><span class="line">        &lt;rule name=&quot;secondaryGroupExistingQueue&quot; create=&quot;false&quot; /&gt;</span><br><span class="line">        &lt;rule name=&quot;default&quot; queue=&quot;default&quot;/&gt;</span><br><span class="line">    &lt;/queuePlacementPolicy&gt;</span><br><span class="line">&lt;/allocations&gt;</span><br></pre></td></tr></table></figure>

<p>​    队列的层次是通过嵌套<code>&lt;queue&gt;</code>元素实现的。所有的队列都是root队列的孩子，即使没有配到<code>&lt;root&gt;</code>元素里。</p>
<p>​    队列有权重属性（这个权重就是对公平的定义），并把这个属性作为公平调度的依据。在这个例子中，当集群7.5,1,1,0.5资源给production,spark,streaming,default时便视作公平,这里的权重并不是百分比。注意，对于在没有配置文件时按用户自动创建的队列，它们仍有权重并且权重值为1。每个队列内部仍可以有不同的调度策略。队列的默认调度策略可以通过顶级元素<code>&lt;defaultQueueSchedulingPolicy&gt;</code>进行配置，如果没有配置，默认采用公平调度。尽管名称是“公平”，公平调度器也支持队列级别的FIFO策略，以及<strong>Dominant Resource Fairness(drf)<strong>策略。每个队列的调度策略可以被其内部的<code>&lt;schedulingPolicy&gt;</code>元素覆盖，在上面这个例子中，default队列就被指定采用fifo进行调度，所以，对于提交到default队列的任务就可以按照FIFO规则顺序的执行了。</strong>需要注意，spark,production,streaming,default之间的调度仍然是公平调度</strong>。每个队列可配置最大、最小资源占用数和最大可运行的应用的数量。</p>
<h4 id="队列放置"><a href="#队列放置" class="headerlink" title="队列放置"></a>队列放置</h4><p>​    公平调度器采用了一套基于规则的系统来确定应用应该放到哪个队列。在上面的例子中，<code>&lt;queuePlacementPolicy&gt;</code> 元素定义了一个规则列表，其中的每个规则会被逐个尝试直到匹配成功。例如，上例第一个规则specified，则会把应用放到它指定的队列中，若这个应用没有指定队列名或队列名不存在，则说明不匹配这个规则，然后尝试下一个规则。primaryGroup规则会尝试把应用放在以用户所在的Unix组名命名的队列中，如果没有这个队列，会尝试下一个规则而不是创建队列。当前面所有规则不满足时，则触发default规则，把应用放在default队列中。</p>
<p>​    当然，可以完全省略queuePlacementPolicy规则，此时队列放置默认遵从如下规则：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;queuePlacementPolicy&gt;</span><br><span class="line">      &lt;rule name=&quot;specified&quot; /&gt;</span><br><span class="line">      &lt;rule name=&quot;user&quot; /&gt;</span><br><span class="line">&lt;/queuePlacementPolicy&gt;</span><br></pre></td></tr></table></figure>

<p>​    上面规则意思是，<strong>除非队列被准确的定义，否则会以用户名为队列名创建队列</strong>。</p>
<p>​    还有一个简单的配置策略可以使得所有的应用放入同一个队列（default），这样就可以让所有应用之间平等共享集群而不是在用户之间。这个配置的定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;queuePlacementPolicy&gt;</span><br><span class="line">     &lt;rule name=&quot;default&quot; /&gt;</span><br><span class="line">&lt;/queuePlacementPolicy&gt;</span><br></pre></td></tr></table></figure>

<p>​    实现上面功能我们还可以不使用配置文件，直接设置<strong>yarn.scheduler.fair.user-as-default-queue=false</strong>，这样应用便会被放入default 队列，而不是各个用户名队列。另外，我们还可以设置<strong>yarn.scheduler.fair.allow-undeclared-pools=false</strong>，这样用户就无法创建队列了。</p>
<h4 id="抢占"><a href="#抢占" class="headerlink" title="抢占"></a>抢占</h4><p>​    在一个繁忙的集群中，当作业被提交到一个的空队列时，作业并不会马上执行，而是阻塞直到正在运行的作业释放系统资源。为了作业从提交到执行所需的时间更具预测性（可以设置等待的超时时间），公平调度器支持<strong>抢占（preemption）</strong>功能。抢占就是允许调度器杀掉占用超过其应占份额资源队列的容器，这些容器资源便可被分配到应该享有这些份额资源的队列中。需要注意抢占会降低集群的执行效率，因为被终止的容器需要被重新执行。</p>
<p>​    通过设置一个全局的参数<strong>yarn.scheduler.fair.preemption=true</strong>可以全面启用抢占功能。此外，还有两个参数用来控制抢占的过期时间（这两个参数默认没有配置，需要至少配置一个来允许抢占容器）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">minSharePreemptionTimeout  最小共享</span><br><span class="line">fairSharePreemptionTimeout 公平共享</span><br></pre></td></tr></table></figure>

<p>​    如果队列在minimum share preemption timeout指定的时间内未获得被承诺的最小共享资源，调度器就会抢占其他容器。通过配置文件中的顶级元素<code>&lt;defaultMinSharePreemptionTimeout&gt;</code>为所有队列配置这个超时时间；我们还可以在<code>&lt;queue&gt;</code>元素内配置<code>&lt;minSharePreemptionTimeout&gt;</code>元素来为某个队列指定超时时间。</p>
<p>​    类似，如果队列在fair share preemption timeout指定时间内未获得平等的资源的一半（这个比例可以配置），调度器则会进行抢占其他容器。这个超时时间可以通过顶级元素<code>&lt;defaultFairSharePreemptionTimeout&gt;</code>和元素级元素<code>&lt;fairSharePreemptionTimeout&gt;</code>分别配置所有队列和某个队列的超时时间。上面提到的比例可以通过<code>&lt;defaultFairSharePreemptionThreshold&gt;</code>(针对所有队列)和<code>&lt;fairSharePreemptionThreshold&gt;</code>(针对某个队列)进行配置修改超时阈值，默认是0.5。</p>
<h3 id="Fair-Scheduler与Capacity-Scheduler区别"><a href="#Fair-Scheduler与Capacity-Scheduler区别" class="headerlink" title="Fair Scheduler与Capacity Scheduler区别"></a>Fair Scheduler与Capacity Scheduler区别</h3><ul>
<li>资源公平共享：在每个队列中，Fair Scheduler可选择按照FIFO、Fair或DRF策略为应用程序分配资源。Fair策略即平均分配，默认情况下，每个队列采用该方式分配资源</li>
<li>支持资源抢占：当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占；从那些超额使用资源的队列中杀死一部分任务，进而释放资源</li>
<li>负载均衡：Fair Scheduler提供了一个基于任务数的负载均衡机制，该机制尽可能将系统中的任务均匀分配到各个节点上。此外，用户也可以根据自己的需求设计负载均衡机制</li>
<li>调度策略灵活配置：Fiar Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair或DRF三种）</li>
<li>提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成</li>
</ul>
<h2 id="六、延迟调度delay-scheduling"><a href="#六、延迟调度delay-scheduling" class="headerlink" title="六、延迟调度delay scheduling"></a>六、延迟调度delay scheduling</h2><p>​    如果申请一个正忙的节点，一般方式是放宽容器的本地限制，去到相同机架上的另一个节点来分配容器，但是实践来说，如果多等待一小会(不超过几秒)，能够增加在所请求的忙节点上分配容器的机会，则可以提高集群的效率，这就叫延迟调度。容量调度器和公平调度器都支持延迟调度。</p>
<p><strong>延迟调度的心跳机制(heartbeat)</strong></p>
<p>​    每个节点管理器周期性地(默认每秒1次)向资源管理器发送心跳请求，心跳中携带了节点管理器正在运行的容器/新容器可用的资源等信息，这对于每个申请节点的应用来说，每个心跳就是一个潜在的调度机会（scheduling opportunity）</p>
<p><strong>调度机会</strong>：等待多少次心跳的问题</p>
<p><strong>容量调度器</strong>：设置<code>yarn.scheduler.capacity.node-locality-delay</code>来配置延迟调度，设置为正整数，表示调度器在放松节点限制、改为匹配同一个机架上的其他节点之前，准备错过的调度机会数量</p>
<p><strong>公平调度器</strong>：设置<code>yarn.scheduler.fair.locality.threshold.node</code>为0.5（集群规模的比例），表示调度器在接受同一机架上的不同节点之前，将一直等待集群中的一半节点都已经给过调度机会。还有个相关属性：<code>yarn.scheduler.fair.locality.threshold.rack</code>，表示接受另一个机架替代所申请的机架之前需要等待的时长阈值。</p>
<h2 id="七、主导资源公平性（Dominant-Resource-Fairness-DRF）"><a href="#七、主导资源公平性（Dominant-Resource-Fairness-DRF）" class="headerlink" title="七、主导资源公平性（Dominant Resource Fairness,DRF）"></a>七、主导资源公平性（Dominant Resource Fairness,DRF）</h2><p>​    对于单一类型资源，如内存的调度，容量或公平性的概念很容易确定。但当有多种资源类型需要调度时，就会变得复杂。如一个用户的应用对CPU的需求打，但对内存的需求少；而另一个用户的需求相反，该如何比较？</p>
<p>​    YARN中调度器解决这个问题的思路是，观察每个用户的主导资源，并将其作为对集群资源使用的一个度量。这个方法成为<strong>主导资源公平性（Dominant Resource Fairness,DRF）</strong></p>
<p>假设当前：</p>
<ul>
<li>集群资源：100vCPU,10TB内存</li>
<li>A请求资源：2vCPU,300GB内存，请求的资源在集群资源中占比分别为2%和3%，内存是A的主导资源</li>
<li>B请求资源：6vCPU,100GB内存，请求的资源在集群资源中占比分别为6%和1%，CPU是B的主导资源</li>
</ul>
<p>由于B申请的资源是A的两倍（6% vs 3%），所以在公平调度下，B只分到A一般的容器数。</p>
<p>​    默认情况下不用DRF，因此在资源计算期间，只需考虑内存，不必考虑CPU。对容量调度器进行配置后，可以使用DRF，将<code>capacity-scheduler.xml</code>中文件中的<code>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</code>设为<code>yarn.scheduler.capacity.resource-calculator</code>即可。</p>
<p>​    公平调度器若要使用DRF，通过将分配文件中的顶层元素<code>defaultQueueSchedulingPolicy</code>设为<code>drf</code>即可。</p>
]]></content>
  </entry>
</search>
