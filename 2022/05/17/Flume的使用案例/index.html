<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Flume的使用案例 | k12的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="案例一：netcat source需求：监听localhost机器的44444端口，接收到数据sink到终端  创建agent配置文件 12[hadoop@hadoop001 ~]$ cd app&#x2F;flume&#x2F;conf&#x2F;[hadoop@hadoop001 conf]$ vi example.conf  example.conf 12345678910111213141516171819202122">
<meta property="og:type" content="article">
<meta property="og:title" content="Flume的使用案例">
<meta property="og:url" content="https://k12coding.github.io/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="案例一：netcat source需求：监听localhost机器的44444端口，接收到数据sink到终端  创建agent配置文件 12[hadoop@hadoop001 ~]$ cd app&#x2F;flume&#x2F;conf&#x2F;[hadoop@hadoop001 conf]$ vi example.conf  example.conf 12345678910111213141516171819202122">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-16T23:41:07.000Z">
<meta property="article:modified_time" content="2022-05-17T02:57:16.688Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="k12的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">k12的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">k12的笔记</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <!--
      <nav id="sub-nav">
        
          此处隐藏rss,注释掉 <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a> -->
        
        <!--此处隐藏,注释掉 <a id="nav-search-btn" class="nav-icon" title="搜索"></a> 
      </nav>
      -->
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://k12coding.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Flume的使用案例" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/" class="article-date">
  <time class="dt-published" datetime="2022-05-16T23:41:07.000Z" itemprop="datePublished">2022-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Flume的使用案例
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="案例一：netcat-source"><a href="#案例一：netcat-source" class="headerlink" title="案例一：netcat source"></a>案例一：netcat source</h2><p>需求：监听localhost机器的44444端口，接收到数据sink到终端</p>
<ol>
<li><p>创建agent配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd app/flume/conf/</span><br><span class="line">[hadoop@hadoop001 conf]$ vi example.conf</span><br></pre></td></tr></table></figure>

<p>example.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line">#需求：监听localhost机器的44444端口，接收到数据sink到终端</span><br><span class="line"></span><br><span class="line"># Name the components on this agent   配置各种名字</span><br><span class="line">a1.sources = r1  #配置source的名字</span><br><span class="line">a1.sinks = k1    #配置sink的名字</span><br><span class="line">a1.channels = c1  #配置channel的名字</span><br><span class="line"></span><br><span class="line"># Describe/configure the source       配置source的基本属性</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory  配置channel的基本属性</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Describe the sink                  配置sink的基本属性</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel      连线</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>This configuration defines a single agent named a1. a1 has a source that listens for data on port 44444, a channel that buffers event data in memory, and a sink that logs event data to the console. The configuration file names the various components, then describes their types and configuration parameters. A given configuration file might define several named agents; when a given Flume process is launched a flag is passed telling it which named agent to manifest.</p>
<span id="more"></span></li>
<li><p>启动flume agent a1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/example.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li><p>在另一个终端telnet localhost 44444 发送event</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ telnet localhost 44444</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">啊</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li>
<li><p>第一个终端的日志控制台也会有相应的显示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$ flume-ng agent \</span><br><span class="line">&gt; --name a1 \</span><br><span class="line">&gt; --conf $FLUME_HOME/conf \</span><br><span class="line">&gt; --conf-file $FLUME_HOME/conf/example.conf \</span><br><span class="line">&gt; -Dflume.root.logger=INFO,console</span><br><span class="line">Info: Sourcing environment configuration script /home/hadoop/app/flume/conf/flume-env.sh</span><br><span class="line">Info: Including Hadoop libraries found via (/home/hadoop/app/hadoop/bin/hadoop) for HDFS access</span><br><span class="line">Info: Including Hive libraries found via (/home/hadoop/app/hive) for Hive access</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">2022-05-16 21:57:12,297 (lifecycleSupervisor-1-3) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:166)] Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br><span class="line">2022-05-16 21:58:30,762 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 31 0D                                           1. &#125;</span><br><span class="line">2022-05-16 21:58:36,151 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 0D                                           2. &#125;</span><br><span class="line">2022-05-16 21:58:38,455 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 33 0D                                           3. &#125;</span><br><span class="line">2022-05-16 21:58:39,071 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 0D                                           a. &#125;</span><br><span class="line">2022-05-16 21:58:39,847 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62 0D                                           b. &#125;</span><br><span class="line">2022-05-16 22:02:54,721 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: E5 95 8A 0D                                     .... &#125;</span><br></pre></td></tr></table></figure>

<p>补充一点，flume只能传递英文和字符，不能用中文。</p>
</li>
</ol>
<h2 id="案例二：avro-client"><a href="#案例二：avro-client" class="headerlink" title="案例二：avro client"></a>案例二：avro client</h2><p>flume-avro-client.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">avro-client-agent.sources = r1</span><br><span class="line">avro-client-agent.sinks = k1</span><br><span class="line">avro-client-agent.channels = c1</span><br><span class="line"> </span><br><span class="line"># Describe/configure the source</span><br><span class="line">avro-client-agent.sources.r1.type = avro</span><br><span class="line">avro-client-agent.sources.r1.bind = localhost</span><br><span class="line">avro-client-agent.sources.r1.port = 41414</span><br><span class="line">#注意这个端口名，在后面的教程中会用得到</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line">avro-client-agent.sinks.k1.type = logger</span><br><span class="line"> </span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">avro-client-agent.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">avro-client-agent.sources.r1.channels = c1</span><br><span class="line">avro-client-agent.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>文本准备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ echo &quot;flume&quot; &gt;&gt; data/flume-avro-client.test</span><br><span class="line">[hadoop@hadoop001 ~]$ cat data/flume-avro-client.test</span><br><span class="line">flume</span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name avro-client-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume-avro-client.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>source启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng avro-client \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">-H localhost \</span><br><span class="line">-p 41414 \</span><br><span class="line">-F ~/data/flume-avro-client.test</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p>source端：</p>
<p>命令执行，event传输完后会退出。退出后再往文件添加数据，并不会传输到sink端，所以avro client的方式不适合于增量data。</p>
<p>sink端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 66 6C 75 6D 65                                  flume &#125;</span><br></pre></td></tr></table></figure>

<h2 id="HDFS-Sink跟写文件相关配置"><a href="#HDFS-Sink跟写文件相关配置" class="headerlink" title="HDFS Sink跟写文件相关配置"></a><strong>HDFS Sink跟写文件相关配置</strong></h2><p>hdfs.path -&gt; hdfs目录路径</p>
<p>hdfs.filePrefix -&gt; 文件前缀。默认值FlumeData</p>
<p>hdfs.fileSuffix -&gt; 文件后缀</p>
<p>hdfs.rollInterval -&gt; 多久时间后close hdfs文件。单位是秒，默认30秒。设置为0的话表示不根据时间close hdfs文件</p>
<p>hdfs.rollSize -&gt; 文件大小超过一定值后，close文件。默认值1024，单位是字节。设置为0的话表示不基于文件大小</p>
<p>hdfs.rollCount -&gt; 写入了多少个事件后close文件。默认值是10个。设置为0的话表示不基于事件个数</p>
<p>hdfs.fileType -&gt; 文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStream</p>
<p>hdfs.batchSize -&gt; 批次数，HDFS Sink每次从Channel中拿的事件个数。默认值100（与事务有关）</p>
<p>hdfs.minBlockReplicas -&gt; HDFS每个块最小的replicas数字，不设置的话会取hadoop中的配置</p>
<p>hdfs.maxOpenFiles -&gt; 允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭</p>
<p>serializer -&gt; HDFS Sink写文件的时候会进行序列化操作。会调用对应的Serializer借口，可以自定义符合需求的Serializer</p>
<p>hdfs.retryInterval -&gt; 关闭HDFS文件失败后重新尝试关闭的延迟数，单位是秒</p>
<p>hdfs.callTimeout -&gt; HDFS操作允许的时间，比如hdfs文件的open，write，flush，close操作。单位是毫秒，默认值是10000</p>
<p><strong>hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.batchSize这5个配置影响着hdfs文件的关闭。</strong></p>
<p>注意，这5个配置影响的是一个hdfs文件，是一个hdfs文件。当hdfs文件关闭的时候，这些配置指标会重新开始计算。因为BucketWriter中的open方法里会调用resetCounters方法，这个方法会重置计数器。而基于hdfs.rollInterval的timedRollFuture线程返回值是在close方法中被销毁的。因此，只要close文件，并且open新文件的时候，这5个属性都会重新开始计算。</p>
<p>hdfs.rollInterval与时间有关，当时间达到hdfs.rollInterval配置的秒数，那么会close文件。</p>
<p>hdfs.rollSize与每个event的字节大小有关，当一个一个event的字节相加起来大于等于hdfs.rollSize的时候，那么会close文件。</p>
<p>hdfs.rollCount与事件的个数有关，当事件个数大于等于hdfs.rollCount的时候，那么会close文件。</p>
<p>hdfs.batchSize表示当事件添加到hdfs.batchSize个的时候，也就是说HDFS Sink每次会拿hdfs.batchSize个事件，而且这些所有的事件都写进了同一个hdfs文件，这才会触发本次条件，并且其他4个配置都未达成条件。然后会close文件。</p>
<p>hdfs.minBlockReplicas表示期望hdfs对文件最小的复制块数。所以有时候我们配置了hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数，并且这3个参数都没有符合条件，但是还是生成了多个文件，这就是因为这个参数导致的，而且这个参数的优先级比hdfs.rollSize，hdfs.rollCount要高。（该参数没有设置则与hadoop conf中的一致）</p>
<h2 id="案例三：exec-hdfs-agent"><a href="#案例三：exec-hdfs-agent" class="headerlink" title="案例三：exec-hdfs-agent"></a>案例三：exec-hdfs-agent</h2><p>flume-exec-hdfs.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#define agent</span><br><span class="line">exec-hdfs-agent.sources = exec-source</span><br><span class="line">exec-hdfs-agent.channels = exec-memory-channel</span><br><span class="line">exec-hdfs-agent.sinks = hdfs-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">exec-hdfs-agent.sources.exec-source.type = exec</span><br><span class="line">exec-hdfs-agent.sources.exec-source.command = tail -F ~/data/flume-exec.test</span><br><span class="line">exec-hdfs-agent.sources.exec-source.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line">#define channel </span><br><span class="line">exec-hdfs-agent.channels.exec-memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">#define sink </span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.type = hdfs</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.path = hdfs://hadoop001:9000/user/hadoop/data/flume/tail</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.fileType = DataStream</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.writeFormat = Text</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.batchSize = 5</span><br><span class="line"></span><br><span class="line">#bind source and sink to channel</span><br><span class="line">exec-hdfs-agent.sources.exec-source.channels = exec-memory-channel  </span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.channel = exec-memory-channel  </span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name exec-hdfs-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume-exec-hdfs.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ echo &quot;a&quot; &gt;&gt; ~/data/flume-exec.test</span><br><span class="line">[hadoop@hadoop001 ~]$ for i in &#123;1..100&#125;; do echo &quot;hadoop $i&quot; &gt;&gt; ~/data/flume-exec.test;sleep 0.1;done</span><br></pre></td></tr></table></figure>

<p>结果</p>
<ol>
<li>如果<code>~/data/flume-exec.test</code>之前已经有数据，则在agent启动时就会有第一个文件。</li>
<li>如果第一个文件信息数量超过<code>rollSize</code>，则输入的数据从第二个文件开始。</li>
<li>结果共有1（先前数据）+ 10 （a和1-99，超过<code>hdfs.rollCount</code>而产生） + 1（数字100，30s没输入，超过<code>hdfs.rollInterval</code>而产生），共12个文件。</li>
<li>无数据输入不额外产生空文件。</li>
<li>可以通过设置参数把roll间隔调大，避免小文件问题。</li>
</ol>
<h2 id="案例四：hdfs-round-agent"><a href="#案例四：hdfs-round-agent" class="headerlink" title="案例四：hdfs-round-agent"></a>案例四：hdfs-round-agent</h2><p>hdfs sink每分钟roll一次，并格式化输出路径</p>
<p>hdfs-round-agent.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hdfs-round-agent.sources = r1 </span><br><span class="line">hdfs-round-agent.channels = c1</span><br><span class="line">hdfs-round-agent.sinks = k1</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sources.r1.type = netcat</span><br><span class="line">hdfs-round-agent.sources.r1.bind = localhost</span><br><span class="line">hdfs-round-agent.sources.r1.port = 44444</span><br><span class="line">hdfs-round-agent.sources.r1.interceptors = i1</span><br><span class="line">hdfs-round-agent.sources.r1.interceptors.i1.type = timestamp</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sinks.k1.type = hdfs</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.path = data/flume/events/%y-%m-%d/%H%M/%S</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.round = true</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sources.r1.channels = c1</span><br><span class="line">hdfs-round-agent.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>因为sink中用到了时间，所以需要利用时间拦截器在event的header中加入时间，否则报错。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:459)] process failed</span><br><span class="line">java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null</span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name hdfs-round-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/hdfs-round-agent.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# telnet localhost 44441</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line">e</span><br><span class="line">OK</span><br><span class="line">g</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /user/hadoop/data/flume/events/22-05-17/</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:47 /user/hadoop/data/flume/events/22-05-17/0247</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:48 /user/hadoop/data/flume/events/22-05-17/0248</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:49 /user/hadoop/data/flume/events/22-05-17/0249</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0247/00/*</span><br><span class="line">1652755625761	61 0d</span><br><span class="line">1652755633572	62 0d</span><br><span class="line">1652755634589	63 0d</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0248/00/*</span><br><span class="line">1652755722065	64 0d</span><br><span class="line">1652755726784	65 0d</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0249/00/*</span><br><span class="line">1652755754582	67 0d</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://k12coding.github.io/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/" data-id="cl3b9tua80004akud9s1y07fd" data-title="Flume的使用案例" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/">Flume的使用案例</a>
          </li>
        
          <li>
            <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process</a>
          </li>
        
          <li>
            <a href="/2022/05/16/Flume%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/">Flume介绍与使用</a>
          </li>
        
          <li>
            <a href="/2022/05/13/Kafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%82%A3%E4%B9%88%E5%BF%AB/">Kafka为什么能那么快</a>
          </li>
        
          <li>
            <a href="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/">我的mysql笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 k12<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>