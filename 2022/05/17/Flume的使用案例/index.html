<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Flume的使用案例 | k12的博客</title>
  <meta name="description" content="案例一：单一日志传输avro clientflume-avro-client.conf 12345678910111213141516171819avro-client-agent.sources &#x3D; r1avro-client-agent.sinks &#x3D; k1avro-client-agent.channels &#x3D; c1 # Describe&#x2F;configure the sourceavro-c">
<meta property="og:type" content="article">
<meta property="og:title" content="Flume的使用案例">
<meta property="og:url" content="https://k12coding.github.io/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="案例一：单一日志传输avro clientflume-avro-client.conf 12345678910111213141516171819avro-client-agent.sources &#x3D; r1avro-client-agent.sinks &#x3D; k1avro-client-agent.channels &#x3D; c1 # Describe&#x2F;configure the sourceavro-c">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-16T23:41:07.000Z">
<meta property="article:modified_time" content="2022-05-19T20:40:55.937Z">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="https://k12coding.github.io/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/index.html">
  
    <link rel="alternate" href="/atom.xml" title="k12的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/images/elephant.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">k12</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">大数据技术</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Guangzhou, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      

    
      

    
      
    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a><span class="archive-list-count">26</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/" class="title">Flume的使用案例</a>
              </p>
              <p class="item-date">
                <time datetime="2022-05-16T23:41:07.000Z" itemprop="datePublished">2022-05-17</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/" class="title">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process</a>
              </p>
              <p class="item-date">
                <time datetime="2022-05-16T23:12:13.000Z" itemprop="datePublished">2022-05-17</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2022/05/16/Flume%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/" class="title">Flume介绍与使用</a>
              </p>
              <p class="item-date">
                <time datetime="2022-05-16T09:47:05.000Z" itemprop="datePublished">2022-05-16</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2022/05/13/Kafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%82%A3%E4%B9%88%E5%BF%AB/" class="title">Kafka为什么能那么快</a>
              </p>
              <p class="item-date">
                <time datetime="2022-05-12T18:33:47.000Z" itemprop="datePublished">2022-05-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/" class="title">我的mysql笔记</a>
              </p>
              <p class="item-date">
                <time datetime="2022-03-26T14:01:37.000Z" itemprop="datePublished">2022-03-26</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%8D%95%E4%B8%80%E6%97%A5%E5%BF%97%E4%BC%A0%E8%BE%93avro-client"><span class="toc-number">1.</span> <span class="toc-text">案例一：单一日志传输avro client</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E4%BA%8C%EF%BC%9A%E7%9B%91%E6%8E%A7%E7%AB%AF%E5%8F%A3%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">案例二：监控端口数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-Sink%E8%B7%9F%E5%86%99%E6%96%87%E4%BB%B6%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE"><span class="toc-number">3.</span> <span class="toc-text">HDFS Sink跟写文件相关配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E4%B8%89%EF%BC%9A%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E5%8D%95%E4%B8%AA%E8%BF%BD%E5%8A%A0%E6%96%87%E4%BB%B6%EF%BC%88%E4%B8%8D%E6%94%AF%E6%8C%81%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">案例三：实时监控单个追加文件（不支持断点续传）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%9B%9B%EF%BC%9Ahdfs-round-agent"><span class="toc-number">5.</span> <span class="toc-text">案例四：hdfs-round-agent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E4%BA%94%EF%BC%9A%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%A4%9A%E4%B8%AA%E6%96%B0%E6%96%87%E4%BB%B6%EF%BC%88%E4%B8%8D%E8%83%BD%E5%AF%B9%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">案例五：实时监控目录下多个新文件（不能对文件内容进行实时同步）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%85%AD%EF%BC%9AFlume%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0Taildir-Source"><span class="toc-number">7.</span> <span class="toc-text">案例六：Flume断点续传Taildir Source</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E4%B8%83%EF%BC%9AFlume-Channel-Selectors"><span class="toc-number">8.</span> <span class="toc-text">案例七：Flume Channel Selectors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%88%98"><span class="toc-number">8.1.</span> <span class="toc-text">实战</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">8.1.1.</span> <span class="toc-text">配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">8.1.2.</span> <span class="toc-text">启动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">8.1.3.</span> <span class="toc-text">测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%85%AB%EF%BC%9AFlume-Sink-Processors"><span class="toc-number">9.</span> <span class="toc-text">案例八：Flume Sink Processors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Default-Sink-Processor"><span class="toc-number">9.1.</span> <span class="toc-text">Default Sink Processor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Failover-Sink-Processor"><span class="toc-number">9.2.</span> <span class="toc-text">Failover Sink Processor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Custom-Sink-Processor"><span class="toc-number">9.3.</span> <span class="toc-text">Custom Sink Processor</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C"><span class="toc-number">10.</span> <span class="toc-text">完</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-Flume的使用案例" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Flume的使用案例
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/" class="article-date">
	  <time datetime="2022-05-16T23:41:07.000Z" itemprop="datePublished">2022-05-17</time>
	</a>
</span>
        
        

        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h2 id="案例一：单一日志传输avro-client"><a href="#案例一：单一日志传输avro-client" class="headerlink" title="案例一：单一日志传输avro client"></a>案例一：单一日志传输avro client</h2><p>flume-avro-client.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">avro-client-agent.sources = r1</span><br><span class="line">avro-client-agent.sinks = k1</span><br><span class="line">avro-client-agent.channels = c1</span><br><span class="line"> </span><br><span class="line"># Describe/configure the source</span><br><span class="line">avro-client-agent.sources.r1.type = avro</span><br><span class="line">avro-client-agent.sources.r1.bind = localhost</span><br><span class="line">avro-client-agent.sources.r1.port = 41414</span><br><span class="line">#注意这个端口名，在后面的教程中会用得到</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line">avro-client-agent.sinks.k1.type = logger</span><br><span class="line"> </span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">avro-client-agent.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">avro-client-agent.sources.r1.channels = c1</span><br><span class="line">avro-client-agent.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>文本准备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ echo &quot;flume&quot; &gt;&gt; data/flume-avro-client.test</span><br><span class="line">[hadoop@hadoop001 ~]$ cat data/flume-avro-client.test</span><br><span class="line">flume</span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name avro-client-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume-avro-client.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>source启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng avro-client \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">-H localhost \</span><br><span class="line">-p 41414 \</span><br><span class="line">-F ~/data/flume-avro-client.test</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p>source端：</p>
<p>命令执行，event传输完后会退出。退出后再往文件添加数据，并不会传输到sink端，所以avro client的方式不适合于增量data。</p>
<p>sink端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 66 6C 75 6D 65                                  flume &#125;</span><br></pre></td></tr></table></figure>



<span id="more"></span>

<h2 id="案例二：监控端口数据"><a href="#案例二：监控端口数据" class="headerlink" title="案例二：监控端口数据"></a>案例二：监控端口数据</h2><p>需求：监听localhost机器的44444端口，接收到数据sink到终端</p>
<ol>
<li><p>创建agent配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd app/flume/conf/</span><br><span class="line">[hadoop@hadoop001 conf]$ vi example.conf</span><br></pre></td></tr></table></figure>

<p>example.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line">#需求：监听localhost机器的44444端口，接收到数据sink到终端</span><br><span class="line"></span><br><span class="line"># Name the components on this agent   配置各种名字</span><br><span class="line">a1.sources = r1  #配置source的名字</span><br><span class="line">a1.sinks = k1    #配置sink的名字</span><br><span class="line">a1.channels = c1  #配置channel的名字</span><br><span class="line"></span><br><span class="line"># Describe/configure the source       配置source的基本属性</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory  配置channel的基本属性</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Describe the sink                  配置sink的基本属性</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel      连线</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>This configuration defines a single agent named a1. a1 has a source that listens for data on port 44444, a channel that buffers event data in memory, and a sink that logs event data to the console. The configuration file names the various components, then describes their types and configuration parameters. A given configuration file might define several named agents; when a given Flume process is launched a flag is passed telling it which named agent to manifest.</p>
</li>
<li><p>启动flume agent a1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/example.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li><p>在另一个终端telnet localhost 44444 发送event</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ telnet localhost 44444</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">啊</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li>
<li><p>第一个终端的日志控制台也会有相应的显示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$ flume-ng agent \</span><br><span class="line">&gt; --name a1 \</span><br><span class="line">&gt; --conf $FLUME_HOME/conf \</span><br><span class="line">&gt; --conf-file $FLUME_HOME/conf/example.conf \</span><br><span class="line">&gt; -Dflume.root.logger=INFO,console</span><br><span class="line">Info: Sourcing environment configuration script /home/hadoop/app/flume/conf/flume-env.sh</span><br><span class="line">Info: Including Hadoop libraries found via (/home/hadoop/app/hadoop/bin/hadoop) for HDFS access</span><br><span class="line">Info: Including Hive libraries found via (/home/hadoop/app/hive) for Hive access</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">2022-05-16 21:57:12,297 (lifecycleSupervisor-1-3) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:166)] Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br><span class="line">2022-05-16 21:58:30,762 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 31 0D                                           1. &#125;</span><br><span class="line">2022-05-16 21:58:36,151 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 0D                                           2. &#125;</span><br><span class="line">2022-05-16 21:58:38,455 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 33 0D                                           3. &#125;</span><br><span class="line">2022-05-16 21:58:39,071 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 0D                                           a. &#125;</span><br><span class="line">2022-05-16 21:58:39,847 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62 0D                                           b. &#125;</span><br><span class="line">2022-05-16 22:02:54,721 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: E5 95 8A 0D                                     .... &#125;</span><br></pre></td></tr></table></figure>

<p>补充一点，flume只能传递英文和字符，不能用中文。</p>
</li>
</ol>
<h2 id="HDFS-Sink跟写文件相关配置"><a href="#HDFS-Sink跟写文件相关配置" class="headerlink" title="HDFS Sink跟写文件相关配置"></a><strong>HDFS Sink跟写文件相关配置</strong></h2><p>hdfs.path -&gt; hdfs目录路径</p>
<p>hdfs.filePrefix -&gt; 文件前缀。默认值FlumeData</p>
<p>hdfs.fileSuffix -&gt; 文件后缀</p>
<p>hdfs.rollInterval -&gt; 多久时间后close hdfs文件。单位是秒，默认30秒。设置为0的话表示不根据时间close hdfs文件</p>
<p>hdfs.rollSize -&gt; 文件大小超过一定值后，close文件。默认值1024，单位是字节。设置为0的话表示不基于文件大小</p>
<p>hdfs.rollCount -&gt; 写入了多少个事件后close文件。默认值是10个。设置为0的话表示不基于事件个数</p>
<p>hdfs.fileType -&gt; 文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStream</p>
<p>hdfs.batchSize -&gt; 批次数，HDFS Sink每次从Channel中拿的事件个数。默认值100（与事务有关）</p>
<p>hdfs.minBlockReplicas -&gt; HDFS每个块最小的replicas数字，不设置的话会取hadoop中的配置</p>
<p>hdfs.maxOpenFiles -&gt; 允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭</p>
<p>serializer -&gt; HDFS Sink写文件的时候会进行序列化操作。会调用对应的Serializer借口，可以自定义符合需求的Serializer</p>
<p>hdfs.retryInterval -&gt; 关闭HDFS文件失败后重新尝试关闭的延迟数，单位是秒</p>
<p>hdfs.callTimeout -&gt; HDFS操作允许的时间，比如hdfs文件的open，write，flush，close操作。单位是毫秒，默认值是10000</p>
<p><strong>hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.batchSize这5个配置影响着hdfs文件的关闭。</strong></p>
<p>注意，这5个配置影响的是一个hdfs文件，是一个hdfs文件。当hdfs文件关闭的时候，这些配置指标会重新开始计算。因为BucketWriter中的open方法里会调用resetCounters方法，这个方法会重置计数器。而基于hdfs.rollInterval的timedRollFuture线程返回值是在close方法中被销毁的。因此，只要close文件，并且open新文件的时候，这5个属性都会重新开始计算。</p>
<p>hdfs.rollInterval与时间有关，当时间达到hdfs.rollInterval配置的秒数，那么会close文件。</p>
<p>hdfs.rollSize与每个event的字节大小有关，当一个一个event的字节相加起来大于等于hdfs.rollSize的时候，那么会close文件。</p>
<p>hdfs.rollCount与事件的个数有关，当事件个数大于等于hdfs.rollCount的时候，那么会close文件。</p>
<p>hdfs.batchSize表示当事件添加到hdfs.batchSize个的时候，也就是说HDFS Sink每次会拿hdfs.batchSize个事件，而且这些所有的事件都写进了同一个hdfs文件，这才会触发本次条件，并且其他4个配置都未达成条件。然后会close文件。</p>
<p>hdfs.minBlockReplicas表示期望hdfs对文件最小的复制块数。所以有时候我们配置了hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数，并且这3个参数都没有符合条件，但是还是生成了多个文件，这就是因为这个参数导致的，而且这个参数的优先级比hdfs.rollSize，hdfs.rollCount要高。（该参数没有设置则与hadoop conf中的一致）</p>
<h2 id="案例三：实时监控单个追加文件（不支持断点续传）"><a href="#案例三：实时监控单个追加文件（不支持断点续传）" class="headerlink" title="案例三：实时监控单个追加文件（不支持断点续传）"></a>案例三：实时监控单个追加文件（不支持断点续传）</h2><p>flume-exec-hdfs.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#define agent</span><br><span class="line">exec-hdfs-agent.sources = exec-source</span><br><span class="line">exec-hdfs-agent.channels = exec-memory-channel</span><br><span class="line">exec-hdfs-agent.sinks = hdfs-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">exec-hdfs-agent.sources.exec-source.type = exec</span><br><span class="line">exec-hdfs-agent.sources.exec-source.command = tail -F ~/data/flume-exec.test</span><br><span class="line">exec-hdfs-agent.sources.exec-source.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line">#define channel </span><br><span class="line">exec-hdfs-agent.channels.exec-memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">#define sink </span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.type = hdfs</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.path = hdfs://hadoop001:9000/user/hadoop/data/flume/tail</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.fileType = DataStream</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.writeFormat = Text</span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.hdfs.batchSize = 5</span><br><span class="line"></span><br><span class="line">#bind source and sink to channel</span><br><span class="line">exec-hdfs-agent.sources.exec-source.channels = exec-memory-channel  </span><br><span class="line">exec-hdfs-agent.sinks.hdfs-sink.channel = exec-memory-channel  </span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name exec-hdfs-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume-exec-hdfs.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ echo &quot;a&quot; &gt;&gt; ~/data/flume-exec.test</span><br><span class="line">[hadoop@hadoop001 ~]$ for i in &#123;1..100&#125;; do echo &quot;hadoop $i&quot; &gt;&gt; ~/data/flume-exec.test;sleep 0.1;done</span><br></pre></td></tr></table></figure>

<p>结果</p>
<ol>
<li>如果<code>~/data/flume-exec.test</code>之前已经有数据，则在agent启动时就会有第一个文件。</li>
<li>如果第一个文件信息数量超过<code>rollSize</code>，则输入的数据从第二个文件开始。</li>
<li>结果共有1（先前数据）+ 10 （a和1-99，超过<code>hdfs.rollCount</code>而产生） + 1（数字100，30s没输入，超过<code>hdfs.rollInterval</code>而产生），共12个文件。</li>
<li>无数据输入不额外产生空文件。</li>
<li>可以通过设置参数把roll间隔调大，避免小文件问题。</li>
</ol>
<h2 id="案例四：hdfs-round-agent"><a href="#案例四：hdfs-round-agent" class="headerlink" title="案例四：hdfs-round-agent"></a>案例四：hdfs-round-agent</h2><p>hdfs sink每分钟roll一次，并格式化输出路径</p>
<p>hdfs-round-agent.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hdfs-round-agent.sources = r1</span><br><span class="line">hdfs-round-agent.channels = c1</span><br><span class="line">hdfs-round-agent.sinks = k1</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sources.r1.type = netcat</span><br><span class="line">hdfs-round-agent.sources.r1.bind = localhost</span><br><span class="line">hdfs-round-agent.sources.r1.port = 44444</span><br><span class="line">hdfs-round-agent.sources.r1.interceptors = i1</span><br><span class="line">hdfs-round-agent.sources.r1.interceptors.i1.type = timestamp</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sinks.k1.type = hdfs</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.path = data/flume/events/%y-%m-%d/%H%M</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.round = true</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">hdfs-round-agent.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">#hdfs-round-agent.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line">hdfs-round-agent.sources.r1.channels = c1</span><br><span class="line">hdfs-round-agent.sinks.k1.channel = c1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>因为sink中用到了时间，所以需要利用时间拦截器在event的header中加入时间，否则报错。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:459)] process failed</span><br><span class="line">java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null</span><br></pre></td></tr></table></figure>

<p>官方有说明：要么利用时间拦截器在header加timestamp，要么启动设置hdfs.useLocalTimeStamp=true</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Note For all of the time related escape sequences, a header with the key “timestamp” must exist among the headers of the event (unless hdfs.useLocalTimeStamp is set to true). One way to add this automatically is to use the TimestampInterceptor.</span><br></pre></td></tr></table></figure>

<p>sink启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name hdfs-round-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/hdfs-round-agent.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# telnet localhost 44444</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line">e</span><br><span class="line">OK</span><br><span class="line">g</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /user/hadoop/data/flume/events/22-05-17/</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:47 /user/hadoop/data/flume/events/22-05-17/0247</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:48 /user/hadoop/data/flume/events/22-05-17/0248</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2022-05-17 02:49 /user/hadoop/data/flume/events/22-05-17/0249</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0247/*</span><br><span class="line">1652755625761	61 0d</span><br><span class="line">1652755633572	62 0d</span><br><span class="line">1652755634589	63 0d</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0248/*</span><br><span class="line">1652755722065	64 0d</span><br><span class="line">1652755726784	65 0d</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -text /user/hadoop/data/flume/events/22-05-17/0249/*</span><br><span class="line">1652755754582	67 0d</span><br></pre></td></tr></table></figure>



<h2 id="案例五：实时监控目录下多个新文件（不能对文件内容进行实时同步）"><a href="#案例五：实时监控目录下多个新文件（不能对文件内容进行实时同步）" class="headerlink" title="案例五：实时监控目录下多个新文件（不能对文件内容进行实时同步）"></a>案例五：实时监控目录下多个新文件（不能对文件内容进行实时同步）</h2><p>spooldir-hdfs-agent.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">spooldir-hdfs-agent.sources = r1</span><br><span class="line">spooldir-hdfs-agent.sinks = k1</span><br><span class="line">spooldir-hdfs-agent.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">spooldir-hdfs-agent.sources.r1.type = spooldir</span><br><span class="line"># spoolDir 指定监控的目录</span><br><span class="line">spooldir-hdfs-agent.sources.r1.spoolDir = /home/hadoop/data/flume</span><br><span class="line"># fileSuffix 指定本地文件上传完成后，自动追加的文件后缀</span><br><span class="line">spooldir-hdfs-agent.sources.r1.fileSuffix = .COMPLETED</span><br><span class="line">spooldir-hdfs-agent.sources.r1.fileHeader = true</span><br><span class="line">#忽略所有以.tmp 结尾的文件，不上传</span><br><span class="line">spooldir-hdfs-agent.sources.r1.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.type = hdfs</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.path = hdfs://hadoop001:9000/user/hadoop/data/flume/events/%Y%m%d/%H</span><br><span class="line"></span><br><span class="line">#上传到hdfs后，文件的前缀</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.filePrefix = upload-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.roundUnit = second</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">#设置每个文件的滚动大小大概是 128M</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与 Event 数量为3</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.hdfs.rollCount = 3</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">spooldir-hdfs-agent.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">spooldir-hdfs-agent.sources.r1.channels = c1</span><br><span class="line">spooldir-hdfs-agent.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>文件准备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ pwd</span><br><span class="line">/home/hadoop/data</span><br><span class="line">[hadoop@hadoop001 data]$ cat flume.text1</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br><span class="line">[hadoop@hadoop001 data]$ cat flume.text2</span><br><span class="line">d</span><br><span class="line">e</span><br><span class="line">f</span><br></pre></td></tr></table></figure>

<p>启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name spooldir-hdfs-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/spooldir-hdfs-agent.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;hello world&quot; &gt; flume/flume.text0</span><br><span class="line">[hadoop@hadoop001 data]$ cp flume.text1 flume/</span><br><span class="line">[hadoop@hadoop001 data]$ cp flume.text2 flume/</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;hello world&quot; &gt; flume/flume.text4</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ ll flume</span><br><span class="line">total 16</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 12 May 19 16:09 flume.text0.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  6 May 19 16:09 flume.text1.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop  6 May 19 16:09 flume.text2.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 12 May 19 16:10 flume.text4.COMPLETED</span><br><span class="line">[hadoop@hadoop001 data]$ hadoop fs -cat /user/hadoop/data/flume/events/20220519/16/upload-.1652976566664</span><br><span class="line">hello world</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">[hadoop@hadoop001 data]$ hadoop fs -cat /user/hadoop/data/flume/events/20220519/16/upload-.1652976566665</span><br><span class="line">c</span><br><span class="line">d</span><br><span class="line">e</span><br><span class="line">[hadoop@hadoop001 data]$ hadoop fs -cat /user/hadoop/data/flume/events/20220519/16/upload-.1652976566666</span><br><span class="line">f</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure>

<p>对于已经标识为<code>.COMPLETED</code>的文件，再次处理或者重名则会报错。<code>.COMPLETED</code>文件内容不会改变。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;hello world22&quot; &gt; flume/flume.text4</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ERROR - org.apache.flume.source.SpoolDirectorySource$SpoolDirectoryRunnable.run(SpoolDirectorySource.java:296)] FATAL: Spool Directory source r1: &#123; spoolDir: /home/hadoop/data/flume &#125;: Uncaught exception in SpoolDirectorySource thread. Restart or reconfigure Flume to continue processing.</span><br><span class="line">java.lang.IllegalStateException: File name has been re-used with different files. Spooling assumptions violated for /home/hadoop/data/flume/flume.text4.COMPLETED</span><br></pre></td></tr></table></figure>



<h2 id="案例六：Flume断点续传Taildir-Source"><a href="#案例六：Flume断点续传Taildir-Source" class="headerlink" title="案例六：Flume断点续传Taildir Source"></a>案例六：Flume断点续传Taildir Source</h2><p>一般的flume日志采集方式会出现重复采集的情况，比如：当某个flume应用挂掉后，重启应用，就会将采集过得日志重复采集。<br><strong>解决办法：采用断点续传taildir，记录上一次的采集位置，重启应用后，从记录的位置开始采集。</strong></p>
<blockquote>
<p>注：Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。文件改名，inode不变。<br>TAILDIR采用（inode+文件全路径名）作为监听文件的唯一标识（可考虑修改源码，只取inode）</p>
</blockquote>
<p>taildir-hdfs-agent.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">taildir-hdfs-agent.sources=r1</span><br><span class="line">taildir-hdfs-agent.sinks=k1</span><br><span class="line">taildir-hdfs-agent.channels=c1</span><br><span class="line"></span><br><span class="line"># source的配置</span><br><span class="line"># source类型</span><br><span class="line">taildir-hdfs-agent.sources.r1.type = TAILDIR</span><br><span class="line"># 元数据位置</span><br><span class="line">taildir-hdfs-agent.sources.r1.positionFile = /home/hadoop/data/flume/meta/taildir_position.json</span><br><span class="line"># 监控的目录</span><br><span class="line">taildir-hdfs-agent.sources.r1.filegroups = f1</span><br><span class="line">taildir-hdfs-agent.sources.r1.filegroups.f1=/home/hadoop/data/bd/.*log</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">taildir-hdfs-agent.sinks.k1.type = hdfs</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.path = hdfs://hadoop001:9000/user/hadoop/data/flume/events/%Y%m%d/%H</span><br><span class="line"></span><br><span class="line">#上传到hdfs后，文件的前缀</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.filePrefix = upload-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">#设置每个文件的滚动大小大概是 128M</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与 Event 数量为3</span><br><span class="line">taildir-hdfs-agent.sinks.k1.hdfs.rollCount = 3</span><br><span class="line"></span><br><span class="line">#channel的配置</span><br><span class="line">taildir-hdfs-agent.channels.c1.type = file</span><br><span class="line">taildir-hdfs-agent.channels.c1.checkpointDir = /home/hadoop/data/checkpoint</span><br><span class="line">taildir-hdfs-agent.channels.c1.dataDirs = /home/hadoop/data/flume-data</span><br><span class="line">taildir-hdfs-agent.channels.c1.capacity = 10000000</span><br><span class="line">taildir-hdfs-agent.channels.c1.transactionCapacity = 5000</span><br><span class="line"></span><br><span class="line">#用channel链接source和sink</span><br><span class="line">taildir-hdfs-agent.sources.r1.channels = c1</span><br><span class="line">taildir-hdfs-agent.sinks.k1.channel =c1</span><br></pre></td></tr></table></figure>

<p>启动命令</p>
<p>（启动前要确保监控目录已经创建）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name taildir-hdfs-agent \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/taildir-hdfs-agent.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;1&quot; &gt;&gt; /home/hadoop/data/bd/bd.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;2&quot; &gt;&gt; /home/hadoop/data/bd/bd.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;3&quot; &gt;&gt; /home/hadoop/data/bd/bd.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;4&quot; &gt;&gt; /home/hadoop/data/bd/bd1.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;5&quot; &gt;&gt; /home/hadoop/data/bd/bd1.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;6&quot; &gt;&gt; /home/hadoop/data/bd/bd1.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;7&quot; &gt;&gt; /home/hadoop/data/bd/bd2.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;8&quot; &gt;&gt; /home/hadoop/data/bd/bd2.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;9&quot; &gt;&gt; /home/hadoop/data/bd/bd2.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;10&quot; &gt;&gt; /home/hadoop/data/bd/bd3.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;11&quot; &gt;&gt; /home/hadoop/data/bd/bd3.log</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ ll /home/hadoop/data/flume/meta</span><br><span class="line">total 0</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 0 May 19 16:45 taildir_position.json</span><br><span class="line">[hadoop@hadoop001 data]$ ll /home/hadoop/data/checkpoint</span><br><span class="line">total 78152</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 80008232 May 19 16:46 checkpoint</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop       37 May 19 16:46 checkpoint.meta</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop       32 May 19 16:46 inflightputs</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop       32 May 19 16:46 inflighttakes</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop        0 May 19 16:45 in_use.lock</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop     4096 May 19 16:45 queueset</span><br><span class="line">1652974981413	6f 6b 0d</span><br><span class="line">[hadoop@hadoop001 conf]$ hadoop fs -text /user/hadoop/data/flume/events/20220519/17/*</span><br><span class="line">2022-05-19 17:56:32,881 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries</span><br><span class="line">2022-05-19 17:56:32,893 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev 26dc7b4620ff16bb6f1fdd48f915ce5fb8222d6f]</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">7</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;12&quot; &gt;&gt; /home/hadoop/data/bd/bd.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;13&quot; &gt;&gt; /home/hadoop/data/bd/bd.log</span><br><span class="line">[hadoop@hadoop001 data]$ cat /home/hadoop/data/flume/meta/taildir_position.json</span><br><span class="line">[&#123;&quot;inode&quot;:1267448,&quot;pos&quot;:12,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd.log&quot;&#125;,&#123;&quot;inode&quot;:1267450,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd1.log&quot;&#125;,&#123;&quot;inode&quot;:1267451,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd2.log&quot;&#125;,&#123;&quot;inode&quot;:1267449,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd3.log&quot;&#125;][hadoop@hadoop001 data]$ </span><br></pre></td></tr></table></figure>

<p>终止agent，输入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;14&quot; &gt;&gt; /home/hadoop/data/bd/bd3.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;15&quot; &gt;&gt; /home/hadoop/data/bd/bd3.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;16&quot; &gt;&gt; /home/hadoop/data/bd/bd4.log</span><br><span class="line">[hadoop@hadoop001 data]$ cat /home/hadoop/data/flume/meta/taildir_position.json</span><br><span class="line">[&#123;&quot;inode&quot;:1267448,&quot;pos&quot;:12,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd.log&quot;&#125;,&#123;&quot;inode&quot;:1267450,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd1.log&quot;&#125;,&#123;&quot;inode&quot;:1267451,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd2.log&quot;&#125;,&#123;&quot;inode&quot;:1267449,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd3.log&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p>重启agent测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ echo &quot;17&quot; &gt;&gt; /home/hadoop/data/bd/bd4.log</span><br><span class="line">[hadoop@hadoop001 data]$ echo &quot;18&quot; &gt;&gt; /home/hadoop/data/bd/bd4.log</span><br><span class="line">[hadoop@hadoop001 data]$ cat /home/hadoop/data/flume/meta/taildir_position.json</span><br><span class="line">[&#123;&quot;inode&quot;:1267448,&quot;pos&quot;:12,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd.log&quot;&#125;,</span><br><span class="line">&#123;&quot;inode&quot;:1267450,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd1.log&quot;&#125;,&#123;&quot;inode&quot;:1267451,&quot;pos&quot;:6,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd2.log&quot;&#125;,&#123;&quot;inode&quot;:1267449,&quot;pos&quot;:12,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd3.log&quot;&#125;,&#123;&quot;inode&quot;:1267439,&quot;pos&quot;:9,&quot;file&quot;:&quot;/home/hadoop/data/bd/bd4.log&quot;&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 conf]$ hadoop fs -text /user/hadoop/data/flume/events/20220519/17/*</span><br><span class="line">2022-05-19 17:56:32,881 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries</span><br><span class="line">2022-05-19 17:56:32,893 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev 26dc7b4620ff16bb6f1fdd48f915ce5fb8222d6f]</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">7</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td></tr></table></figure>

<p>一切正常，数据无丢失无重复，支持断点续传。</p>
<h2 id="案例七：Flume-Channel-Selectors"><a href="#案例七：Flume-Channel-Selectors" class="headerlink" title="案例七：Flume Channel Selectors"></a>案例七：Flume Channel Selectors</h2><p>Flume 支持扇出从一个 source 到多个 channel 的流量. 扇出有两种模式 : 复制和多路复用.。</p>
<p><strong>replicating selector</strong></p>
<p>在复制流程中，event 将发送到所有已配置的 channel。在多路复用的情况下， event 仅被发送到合格 channels 的子集。为了散开流量，需要指定 source 的 channel 列表以及扇出它的策略。 这是通过添加可以复制或多路复用的 channel”选择器” 来完成的。如果它是多路复用器， 则进一步指定选择规则。如果您没有指定选择器，那么默认情况下它会复制：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2 c3</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line">a1.sources.r1.channels = c1 c2 c3</span><br><span class="line">#这意味着c3是可选的，向c3写入失败会被忽略。但是向c1，c2写入失败会出错</span><br><span class="line">a1.sources.r1.selector.optional = c3</span><br></pre></td></tr></table></figure>

<p>上面这个例子中没有声明sink，c3配置成了可选的。向c3发送数据如果失败了会被忽略。c1和c2没有配置成可选的，向c1和c2写数据失败会导致事务失败回滚。</p>
<p><strong>multiplexing selector</strong></p>
<p>多路复用选择具有另一组属性以分流。 这需要指定 event 属性到 channel 集的映射。 选择器检查 event 头中的每个已配置属性。 如果它与指定的值匹配，则该 event 将发送到映射到该值的所有 channel。如果没有匹配项，则将 event 发送到配置为默认值的 channel 集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Mapping for multiplexing selector</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = multiplexing</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.header = &lt;someHeader&gt;</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value1&gt; = &lt;Channel1&gt;</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value2&gt; = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value3&gt; = &lt;Channel2&gt;</span><br><span class="line">#...</span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.default = &lt;Channel2&gt; </span><br></pre></td></tr></table></figure>

<p>映射允许为每个值重叠 channel。</p>
<p>以下示例具有多路复用到两个路径的单个流。 名为 agent_foo 的 agent 具有单个 avrosource 和两个链接到两个 sink 的 channel:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># list the sources, sinks and channels in the agent</span><br><span class="line">agent_foo.sources = avro-AppSrv-source1</span><br><span class="line">agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2</span><br><span class="line">agent_foo.channels = mem-channel-1 file-channel-2</span><br><span class="line"># set channels for source</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2</span><br><span class="line"># set channel for sinks</span><br><span class="line">agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1</span><br><span class="line">agent_foo.sinks.avro-forward-sink2.channel = file-channel-2</span><br><span class="line"># channel selector configuration</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.header = State</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1</span><br></pre></td></tr></table></figure>

<p>选择器检查名为 “State” 的标头。 如果该值为 “CA”，则将其发送到 mem-channel-1，如果其为 “AZ”，则将其发送到文件 channel-2，或者如果其为 “NY” 则为两者。 如果 “状态” 标题未设置或与三者中的任何一个都不匹配，则它将转到 mem-channel-1，其被指定为 “default”。</p>
<p>选择器还支持可选 channel。 要为标头指定可选 channel，可通过以下方式使用 config 参数 “optional”：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># channel selector configuration</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.header = State</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2</span><br><span class="line">agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1</span><br></pre></td></tr></table></figure>

<p>选择器将首先尝试写入所需的 channel，如果其中一个 channel 无法使用 event ，则会使事务失败。 在所有渠道上重新尝试交易。 一旦所有必需的 channel 消耗了 event ，则选择器将尝试写入可选 channel。 任何可选 channel 使用该 event 的失败都会被忽略而不会重试。</p>
<p>如果可选信道与特定报头的所需信道之间存在重叠，则认为该信道是必需的，并且信道中的故障将导致重试所有必需信道集。 例如，在上面的示例中，对于标题 “CA”,mem-channel-1 被认为是必需的 channel，即使它被标记为必需和可选，并且写入此 channel 的失败将导致该 event 在为选择器配置的所有 channel 上重试。</p>
<p>请注意，如果标头没有任何所需的 channel，则该 event 将被写入默认 channel，并将尝试写入该标头的可选 channel。 如果未指定所需的 channel，则指定可选 channel 仍会将 event 写入默认 channel。 如果没有将 channel 指定为默认 channel 且没有必需 channel，则选择器将尝试将 event 写入可选 channel。 在这种情况下，任何失败都会被忽略。</p>
<p><strong>自定义选择器</strong><br>自定义选择器就是你可以自己写一个org.apache.flume.ChannelSelector接口的实现类。老规矩，你自己写的实现类以及依赖的jar包在启动时候都必须放入Flume的classpath。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = com.lxk.flume.custom.BalanceChannelSelector</span><br></pre></td></tr></table></figure>

<p>要自定义自己的channel 选择器，比如上面的负载均衡的channel选择器，因为上面系统提供的2个原生的选择器要么全复制，要么选择性的改变数据流向，现在想增加channel数量，缓解压力，数据就需要均衡的发布到声明的n个channel里面去。要自定义，就得了解这个选择器的实现。channel 是在 agent 上暂存 event 的缓冲池。 event由source添加，由sink消费后删除。</p>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Flume多路复用：</span><br><span class="line">  同一时刻，source只能传输给一个channel，source是通过 event header 来决定传输到哪一个 channel。</span><br><span class="line">  即：Flume 多路复用，需配合Static Interceptor来使用。比如：从A端口过来的数据，key=from,value=A，logger输出；从B端口过来的数据，key=from,value=B，hdfs输出，此时就可以使用Flume多路复用，通过event header来决定传输到哪个Channel)</span><br><span class="line"> </span><br><span class="line">案例：</span><br><span class="line">  Flume-1 监听 44444端口；</span><br><span class="line">  Flume-2 监听 44441端口，avro sink 输出到 44444端口；</span><br><span class="line">  Flume-3 监听 44442端口，avro sink 输出到 44444端口；</span><br><span class="line"> </span><br><span class="line">选型：</span><br><span class="line"> Flume-1：avro source + memory channel + Multiplexing Channel Selector(多路复用渠道选择器)  + logger sink + hdfs sink</span><br><span class="line"> Flume-2：netcat source + memory channel + avro sink</span><br><span class="line"> Flume-3：netcat source + memory channel + avro sink</span><br></pre></td></tr></table></figure>

<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><p>multiplexing-flume-1.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">multiplexing-flume-1.sources = r1</span><br><span class="line">multiplexing-flume-1.sinks = k1 k2</span><br><span class="line">multiplexing-flume-1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">multiplexing-flume-1.sources.r1.type = avro</span><br><span class="line">multiplexing-flume-1.sources.r1.bind = localhost</span><br><span class="line">multiplexing-flume-1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Multiplexing Channel Selector configure(多路复用通道选择器配置)</span><br><span class="line">multiplexing-flume-1.sources.r1.selector.type = multiplexing</span><br><span class="line">multiplexing-flume-1.sources.r1.selector.header = from</span><br><span class="line">multiplexing-flume-1.sources.r1.selector.mapping.A = c1</span><br><span class="line">multiplexing-flume-1.sources.r1.selector.mapping.B = c2</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">multiplexing-flume-1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">multiplexing-flume-1.sinks.k2.type = hdfs</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.path = data/flume/events/%y-%m-%d/%H-%M</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.filePrefix = multiplexing-</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.round = true</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.roundValue = 1</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.roundUnit = minute</span><br><span class="line">multiplexing-flume-1.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">multiplexing-flume-1.channels.c1.type = memory</span><br><span class="line">multiplexing-flume-1.channels.c1.capacity = 1000</span><br><span class="line">multiplexing-flume-1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">multiplexing-flume-1.channels.c2.type = memory</span><br><span class="line">multiplexing-flume-1.channels.c2.capacity = 1000</span><br><span class="line">multiplexing-flume-1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">multiplexing-flume-1.sources.sources.r1.channels = c1 c2</span><br><span class="line">multiplexing-flume-1.sources.sinks.k1.channel = c1</span><br><span class="line">multiplexing-flume-1.sources.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>multiplexing-flume-2.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">multiplexing-flume-2.sources = r1  </span><br><span class="line">multiplexing-flume-2.sinks = k1   </span><br><span class="line">multiplexing-flume-2.channels = c1  </span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">multiplexing-flume-2.sources.r1.type = netcat</span><br><span class="line">multiplexing-flume-2.sources.r1.bind = localhost</span><br><span class="line">multiplexing-flume-2.sources.r1.port = 44441</span><br><span class="line">multiplexing-flume-2.sources.r1.interceptors = i1</span><br><span class="line">multiplexing-flume-2.sources.r1.interceptors.i1.type = static</span><br><span class="line">multiplexing-flume-2.sources.r1.interceptors.i1.key = from</span><br><span class="line">multiplexing-flume-2.sources.r1.interceptors.i1.value = A</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">multiplexing-flume-2.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">multiplexing-flume-2.sinks.k1.type = avro</span><br><span class="line">multiplexing-flume-2.sinks.k1.hostname = localhost</span><br><span class="line">multiplexing-flume-2.sinks.k1.port = 44444</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">multiplexing-flume-2.sources.r1.channels = c1</span><br><span class="line">multiplexing-flume-2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>multiplexing-flume-3.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">multiplexing-flume-3.sources = r1</span><br><span class="line">multiplexing-flume-3.sinks = k1</span><br><span class="line">multiplexing-flume-3.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">multiplexing-flume-3.sources.r1.type = netcat</span><br><span class="line">multiplexing-flume-3.sources.r1.bind = localhost</span><br><span class="line">multiplexing-flume-3.sources.r1.port = 44442</span><br><span class="line">multiplexing-flume-3.sources.r1.interceptors = i1</span><br><span class="line">multiplexing-flume-3.sources.r1.interceptors.i1.type = static</span><br><span class="line">multiplexing-flume-3.sources.r1.interceptors.i1.key = from</span><br><span class="line">multiplexing-flume-3.sources.r1.interceptors.i1.value = B</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">multiplexing-flume-3.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">multiplexing-flume-3.sinks.k1.type = avro</span><br><span class="line">multiplexing-flume-3.sinks.k1.hostname = localhost</span><br><span class="line">multiplexing-flume-3.sinks.k1.port = 44444</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">multiplexing-flume-3.sources.r1.channels = c1</span><br><span class="line">multiplexing-flume-3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a><strong>启动</strong></h4><p>启动顺序：multiplexing-flume-1,multiplexing-flume-2、multiplexing-flume-3</p>
<p>窗口1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name multiplexing-flume-1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/multiplexing-flume-1.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>窗口2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name multiplexing-flume-2 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/multiplexing-flume-2.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>窗口3</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name multiplexing-flume-3 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/conf/multiplexing-flume-3.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>窗口4</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# telnet localhost 44441</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>窗口5</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# telnet localhost 44442</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">4</span><br><span class="line">OK</span><br><span class="line">5</span><br><span class="line">OK</span><br><span class="line">6</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>窗口1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2022-05-19 20:00:47,539 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 31 0D                                           1. &#125;</span><br><span class="line">2022-05-19 20:00:47,539 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 32 0D                                           2. &#125;</span><br><span class="line">2022-05-19 20:00:47,539 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 33 0D                                           3. &#125;</span><br><span class="line">2022-05-19 20:00:54,825 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSSequenceFile.configure(HDFSSequenceFile.java:63)] writeFormat = Writable, UseRawLocalFileSystem = false</span><br><span class="line">2022-05-19 20:00:55,007 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:246)] Creating data/flume/events/22-05-19/20-00/multiplexing-.1652990454821.tmp</span><br><span class="line">2022-05-19 20:00:55,231 (hdfs-k2-call-runner-0) [WARN - org.apache.hadoop.util.NativeCodeLoader.&lt;clinit&gt;(NativeCodeLoader.java:60)] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2022-05-19 20:01:26,692 (hdfs-k2-roll-timer-0) [INFO - org.apache.flume.sink.hdfs.HDFSEventSink$1.run(HDFSEventSink.java:393)] Writer callback called.</span><br><span class="line">2022-05-19 20:01:26,693 (hdfs-k2-roll-timer-0) [INFO - org.apache.flume.sink.hdfs.BucketWriter.doClose(BucketWriter.java:438)] Closing data/flume/events/22-05-19/20-00/multiplexing-.1652990454821.tmp</span><br><span class="line">2022-05-19 20:01:26,716 (hdfs-k2-call-runner-6) [INFO - org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:681)] Renaming data/flume/events/22-05-19/20-00/multiplexing-.1652990454821.tmp to data/flume/events/22-05-19/20-00/multiplexing-.1652990454821</span><br></pre></td></tr></table></figure>

<p>logger输出c1结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 31 0D                                           1. &#125;</span><br><span class="line">2022-05-19 20:00:47,539 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 32 0D                                           2. &#125;</span><br><span class="line">2022-05-19 20:00:47,539 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;from=A&#125; body: 33 0D                                           3. &#125;</span><br></pre></td></tr></table></figure>

<p>hdfs查看c2结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hadoop fs -text data/flume/events/22-05-19/20-00/multiplexing-.1652990454821</span><br><span class="line">2022-05-19 20:03:35,791 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">1652990456695	34 0d</span><br><span class="line">1652990456698	35 0d</span><br><span class="line">1652990456699	36 0d</span><br><span class="line">[hadoop@hadoop001 ~]$ </span><br></pre></td></tr></table></figure>

<p>测试成功。</p>
<h2 id="案例八：Flume-Sink-Processors"><a href="#案例八：Flume-Sink-Processors" class="headerlink" title="案例八：Flume Sink Processors"></a>案例八：Flume Sink Processors</h2><p>具体案例参考其他博主：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lzb348110175/article/details/118220586">Flume案例七：故障转移(Failover Sink Processor)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lzb348110175/article/details/118223517">Flume案例八：负载均衡(Load balancing Sink Processor)</a></p>
<p>接收组允许用户将多个 sink 分组到一个实体中. sink 处理器可用于在组内的所有 sink 上提供负载平衡功能, 或在时间故障的情况下实现从一个 sink 到另一个 sink 的故障转移.</p>
<p>必需属性以粗体显示</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>sinks</strong></td>
<td>-</td>
<td>以空格分隔的参与组的 sink 列表</td>
</tr>
<tr>
<td><strong>processor.type</strong></td>
<td>default</td>
<td>组件 type 名称需要是 default,failover 或 load_balance</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Example for agent named a1:</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br></pre></td></tr></table></figure>

<h3 id="Default-Sink-Processor"><a href="#Default-Sink-Processor" class="headerlink" title="Default Sink Processor"></a>Default Sink Processor</h3><p>默认 sink 只接受一个 sink. 用户不必为单个 sink 创建处理器(sink 组). 相反, 用户可以遵循本用户指南中上面解释的 source - channel - sink 模式</p>
<h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><p>故障转移 sink 维护一个优先级的 sink 列表, 保证只要有一个可用的 event 将被处理(传递)</p>
<p>故障转移机制的工作原理是将故障 sink 降级到池中, 在池中为它们分配一个冷却期, 在重试之前随顺序故障而增加. sink 成功发送 event 后, 它将恢复到实时池. sink 具有与之相关的优先级, 数量越大, 优先级越高. 如果在发送 event 时 sink 发生故障, 则接下来将尝试下一个具有最高优先级的 sink 以发送 event . 例如, 在优先级为 80 的 sink 之前激活优先级为 100 的 sink. 如果未指定优先级, 则根据配置中指定 sink 的顺序确定 Sinks 优先级.</p>
<p>要进行配置, 请将 sink 组处理器设置为故障转移并为所有单个 sink 设置优先级. 所有指定的优先级必须是唯一的 此外, 可以使用 maxpenalty 属性设置故障转移时间的上限(以毫秒为单位)</p>
<p>必需属性以粗体显示</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>sinks</strong></td>
<td>-</td>
<td>以空格分隔的参与组的 sink 列表</td>
</tr>
<tr>
<td><strong>processor.type</strong></td>
<td>default</td>
<td>组件 type 名称需要进行故障转移 &amp; nbsp;  failover</td>
</tr>
<tr>
<td><strong>processor.priority.&lt;sinkName&gt;</strong></td>
<td>-</td>
<td>优先价值。 必须是与当前 sink 组关联的 sink 实例之一。较高优先级值 Sink 较早被激活。绝对值越大表示优先级越高</td>
</tr>
<tr>
<td>processor.maxpenalty</td>
<td>30000</td>
<td>失败的 sink 的最大退避时间（以毫秒为单位）</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Example for agent named a1:</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line">Load balancing Sink Processor(负载均衡接收处理器)</span><br><span class="line">负载平衡 sink 处理器提供了在多个 sink 上进行负载均衡流量的功能. 它维护一个索引的活动 sink 列表, 必须在其上分配负载. 实现支持使用 round_robin 或 random 机制分配负载. 选择机制的选择默认为 round_robin 类型, 但可以通过配置覆盖. 通过从 AbstractSinkSelector 继承的自定义类支持自定义选择机制.</span><br></pre></td></tr></table></figure>

<p>调用时, 此选择器使用其配置的选择机制选择下一个 sink 并调用它. 对于 round_robin 和 random 如果所选的 sink 无法传递 event , 则处理器通过其配置的选择机制选择下一个可用的 sink. 此实现不会将失败的 sink 列入黑名单, 而是继续乐观地尝试每个可用的 sink. 如果所有 sink 调用都导致失败, 则选择器将故障传播到 sink 运行器.</p>
<p>如果启用了 backoff , 则 sink 处理器会将失败的 sink 列入黑名单, 将其删除以供给定超时的选择. 当超时结束时, 如果 sink 仍然没有响应, 则超时会以指数方式增加, 以避免在无响应的 sink 上长时间等待时卡住. 在禁用此功能的情况下, 在循环中, 所有失败的 sink 负载将被传递到下一个 sink, 因此不均衡</p>
<p>必需属性以粗体显示</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>processor.sink</strong></td>
<td>-</td>
<td>以空格分隔的参与组的 sink 列表</td>
</tr>
<tr>
<td><strong>processor.type</strong></td>
<td>default</td>
<td>组件 type 名称需要是 load_balance</td>
</tr>
<tr>
<td>processor.backoff</td>
<td>false</td>
<td>失败的 sink 是否会以指数方式退回。</td>
</tr>
<tr>
<td>processor.selector</td>
<td>round_robin</td>
<td>选择机制。必须是 round_robin,random 或自定义类的 FQCN，它继承自 AbstractSinkelector</td>
</tr>
<tr>
<td>processor.selector.maxTimeOut</td>
<td>30000</td>
<td>由退避选择器用于限制指数 backoff （以毫秒为单位）</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example for agent named a1:</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br></pre></td></tr></table></figure>

<h3 id="Custom-Sink-Processor"><a href="#Custom-Sink-Processor" class="headerlink" title="Custom Sink Processor"></a>Custom Sink Processor</h3><p>目前不支持自定义 sink 处理器</p>
<h2 id="完"><a href="#完" class="headerlink" title="完"></a>完</h2>
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://k12coding.github.io/2022/05/17/Flume%E7%9A%84%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/" title="Flume的使用案例" target="_blank" rel="external">https://k12coding.github.io/2022/05/17/Flume的使用案例/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/elephant.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">k12</span><small class="ml-1x">大数据技术</small></a></h3>
        <div>个人简介。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    
    <li class="next">
      <a href="/2022/05/17/Flume-v1-9-0%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99ERROR-org-apache-flume-sink-hdfs-HDFSEventSink-process/" title="Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>