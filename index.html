<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"k12coding.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="2">
<meta property="og:type" content="website">
<meta property="og:title" content="k12的博客">
<meta property="og:url" content="https://k12coding.github.io/index.html">
<meta property="og:site_name" content="k12的博客">
<meta property="og:description" content="2">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="k12">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://k12coding.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>k12的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">k12的博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">k12的笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">k12</p>
  <div class="site-description" itemprop="description">2</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">我的mysql笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-26 22:01:37" itemprop="dateCreated datePublished" datetime="2022-03-26T22:01:37+08:00">2022-03-26</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-03-27 02:53:31" itemprop="dateModified" datetime="2022-03-27T02:53:31+08:00">2022-03-27</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="语句"><a href="#语句" class="headerlink" title="语句"></a>语句</h3><h4 id="insert-ignore-into"><a href="#insert-ignore-into" class="headerlink" title="insert ignore into"></a>insert ignore into</h4><p>INSERT IGNORE 与INSERT INTO的区别就是INSERT IGNORE会忽略数据库中已经存在 的数据，如果数据库没有数据，就插入新的数据，如果有数据的话就跳过这条数据。这样就可以保留数据库中已经存在数据，达到在间隙中插入数据的目的。</p>
<h4 id="insert-into-…-on-duplicate-key-update-…"><a href="#insert-into-…-on-duplicate-key-update-…" class="headerlink" title="insert into … on duplicate key update …"></a>insert into … on duplicate key update …</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.on duplicate key update 含义：</span><br><span class="line">  1）如果在INSERT语句末尾指定了 on duplicate key update，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则在出现重复值的行执行UPDATE；</span><br><span class="line">  2）如果不会导致唯一值列重复的问题，则插入新行。</span><br><span class="line"> </span><br><span class="line">2. values(col_name)函数只是取当前插入语句中的插入值，并没有累加功能。</span><br><span class="line">  如：count = values(count) 取前面 insert into 中的 count 值，并更新</span><br><span class="line">当有多条记录冲突，需要插入时，前面的更新值都被最后一条记录覆盖，所以呈现出取最后一条更新的现象。</span><br><span class="line">  如：count = count + values(count) 依然取前面 insert into 中的 count 值，并与原记录值相加后更新回数据库，这样，当多条记录冲突需要插入时，就实现了不断累加更新的现象。</span><br><span class="line"> </span><br><span class="line">注：</span><br><span class="line">1.insert into ... on duplicate key update ... values() 这个语句</span><br><span class="line">    尽管在冲突时执行了更新，并没有插入，但是发现依然会占用 id 序号（自增），</span><br><span class="line">2.如果要更新的字段是主键或者唯一索引，不能和表中已有的数据重复，否则插入更新都失败。</span><br></pre></td></tr></table></figure>

<h4 id="replace-into"><a href="#replace-into" class="headerlink" title="replace into"></a>replace into</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对表进行replace into操作的时候，</span><br><span class="line">如果表只包含主键：</span><br><span class="line">   当不存在冲突时,replace into 相当于insert操作。</span><br><span class="line">   当存在冲突时,replace into 相当于update操作。</span><br><span class="line">如果表包含主键和唯一性索引：</span><br><span class="line">   当不存在冲突时,replace into 相当于insert操作。 </span><br><span class="line">   当存在主键冲突的时候是先<span class="keyword">delete</span>再insert,如果主键是自增的，则自增主键会做 +<span class="number">1</span> 操作。</span><br><span class="line">   当存在唯一性索引冲突的时候是直接update。,如果主键是自增的，则自增主键会做 +<span class="number">1</span> 操作。 </span><br></pre></td></tr></table></figure>

<blockquote>
<p>REPLACE INTO <code>table</code> (<code>unique_column</code>,<code>num</code>) VALUES (‘$unique_value’,$num);</p>
<p>跟</p>
<p>INSERT INTO <code>table</code> (<code>unique_column</code>,<code>num</code>) VALUES(‘$unique_value’,$num) ON DUPLICATE UPDATE num=$num;还是有些区别的.<br>区别就是replace into的时候会删除老记录。如果表中有一个自增的主键。<br>那么就要出问题了。</p>
<p>首先，因为新纪录与老记录的主键值不同，所以其他表中所有与本表老数据主键id建立的关联全部会被破坏。</p>
<p>其次，就是，频繁的REPLACE INTO 会造成新纪录的主键的值迅速增大。<br>总有一天。达到最大值后就会因为数据太大溢出了。就没法再插入新纪录了。数据表满了，不是因为空间不够了，而是因为主键的值没法再增加了</p>
</blockquote>
<p>看上面知道，其实<code>insert into ... on duplicate key update ...</code>也是会造成主键的值迅速增大的问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">请注意，没有出现在`REPLACE`语句中的列将使用默认值插入相应的列。 如果列具有`NOT NULL`属性并且没有默认值，并且您如果没有在`REPLACE`语句中指定该值，则MySQL将引发错误。这是`REPLACE`和`INSERT`语句之间的区别。</span><br><span class="line"></span><br><span class="line">使用REPLACE语句时需要知道几个重点：</span><br><span class="line">如果您开发的应用程序不仅支持MySQL数据库，而且还支持其他关系数据库管理系统(RDBMS)，则应避免使用REPLACE语句，因为其他RDBMS可能不支持。代替的作法是在事务中使用DELETE和INSERT语句的组合。</span><br><span class="line">如果在具有触发器的表中使用了REPLACE语句，并且发生了重复键错误的删除，则触发器将按以下顺序触发：在删除前删除，删除之后，删除后，如果REPLACE语句删除当前 行并插入新行。 如果REPLACE语句更新当前行，则触发BEFORE UPDATE和AFTER UPDATE触发器。</span><br></pre></td></tr></table></figure>

<h4 id="alter"><a href="#alter" class="headerlink" title="alter"></a>alter</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1) 加索引</span><br><span class="line">  mysql&gt; alter table 表名 add index 索引名 (字段名1[，字段名2 …]);</span><br><span class="line"></span><br><span class="line">2) 加主关键字的索引</span><br><span class="line">  mysql&gt; alter table 表名 add primary key (字段名);</span><br><span class="line"></span><br><span class="line">3) 加唯一限制条件的索引</span><br><span class="line">  mysql&gt; alter table 表名 add unique 索引名 (字段名);</span><br><span class="line"></span><br><span class="line">4) 删除某个索引</span><br><span class="line">  mysql&gt; alter table 表名 drop index 索引名;</span><br><span class="line"></span><br><span class="line">5) 增加字段</span><br><span class="line">  mysql&gt; ALTER TABLE table_name ADD field_name field_type;</span><br><span class="line">  </span><br><span class="line">  如果你需要指定新增字段的位置，可以使用MySQL提供的关键字 FIRST (设定位第一列)， AFTER 字段名（设定位于某个字段之后）。</span><br><span class="line"></span><br><span class="line">6) 修改原字段名称及类型(modify或者change)</span><br><span class="line">  mysql&gt; ALTER TABLE table_name CHANGE old_field_name new_field_name field_type;</span><br><span class="line"></span><br><span class="line">7) 删除字段</span><br><span class="line">  mysql&gt; ALTER TABLE table_name DROP field_name;</span><br></pre></td></tr></table></figure>

<h4 id="WITH-ROLLUP"><a href="#WITH-ROLLUP" class="headerlink" title="WITH ROLLUP"></a>WITH ROLLUP</h4><p>在group分组字段的基础上再进行统计数据。搭配ifnull()或者COALESCE()使用对汇总值命名</p>
<h4 id="添加外键约束"><a href="#添加外键约束" class="headerlink" title="添加外键约束"></a>添加外键约束</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--sql语句创建表的同时添加外键约束 CREATE TABLE tb_UserAndRole  --用户角色表</span><br><span class="line">(</span><br><span class="line">  ID INT PRIMARY KEY IDENTITY(1,1),</span><br><span class="line">  UserID INT NOT NULL,--用户ID</span><br><span class="line">  RoleID INT NOT NULL,--角色ID</span><br><span class="line">  foreign key(UserID) references tb_Users(ID)--tb_Users表的ID作为tb_UserAndRole表的外键 ) </span><br><span class="line"></span><br><span class="line">  --2、添加外键约束(关联字段要用括号括起来)</span><br><span class="line"></span><br><span class="line">  -- ALTER TABLE 从表</span><br><span class="line"></span><br><span class="line">  -- ADD CONSTRAINT 约束名 FOREIGN KEY (关联字段) references 主表(关联字段);</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE &lt;数据表名&gt; ADD CONSTRAINT &lt;外键名&gt;<br>FOREIGN KEY(&lt;列名&gt;) REFERENCES &lt;主表名&gt; (&lt;列名&gt;);</p>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="REPLACE-str-old-string-new-string"><a href="#REPLACE-str-old-string-new-string" class="headerlink" title="REPLACE(str,old_string,new_string)"></a>REPLACE(str,old_string,new_string)</h4><p><code>REPLACE()</code>函数有三个参数，它将<code>string</code>中的<code>old_string</code>替换为<code>new_string</code>字符串。</p>
<h4 id="REGEXP-REPLACE-expression-pattern-replace-string-pos-occurrence-match-type"><a href="#REGEXP-REPLACE-expression-pattern-replace-string-pos-occurrence-match-type" class="headerlink" title="REGEXP_REPLACE (expression, pattern, replace_string[, pos[, occurrence[, match_type]]])"></a>REGEXP_REPLACE (expression, pattern, replace_string[, pos[, occurrence[, match_type]]])</h4><p><code>REGEXP_REPLACE (expression, pattern, replace_string[, pos[, occurrence[, match_type]]])</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">参数说明</span><br><span class="line">REGEXP_REPLACE() 函数参数的解释是：</span><br><span class="line">expression:它是一个输入字符串，我们将通过正则表达式参数和函数对其进行搜索。</span><br><span class="line">patterns:它表示子字符串的正则表达式模式。</span><br><span class="line">replace_string：如果找到匹配项，将被替换的子字符串。</span><br><span class="line">REGEXP_INSTR() 函数使用下面给出的各种可选参数：</span><br><span class="line">pos:它用于指定字符串中表达式中的位置以开始搜索。如果我们不指定此参数，它将从位置 1 开始。</span><br><span class="line">occurrence:它用于指定我们要搜索的匹配项。如果我们不指定这个参数，所有出现的都会被替换。</span><br><span class="line">match_type：它是一个字符串，可以让我们细化正则表达式。它使用以下可能的字符来执行匹配。</span><br><span class="line">- c:它表示区分大小写的匹配。</span><br><span class="line">- i:它表示不区分大小写的匹配。</span><br><span class="line">- m:它代表 multiple-line 模式，允许在字符串中使用行终止符。默认情况下，此函数匹配字符串开头和结尾的行终止符。</span><br><span class="line">- n:它用于修改 . (点)字符来匹配行终止符。</span><br><span class="line">- u:它代表 Unix-only 行结尾。</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">1、用’#‘替换字符串中的所有数字</span><br><span class="line">SELECT regexp_replace(&#x27;01234abcde56789&#x27;,&#x27;[0-9]&#x27;,&#x27;#&#x27;) AS new_str FROM dual;</span><br><span class="line">结果：#####abcde#####</span><br><span class="line"></span><br><span class="line">用’#‘替换字符串中的数字0、9</span><br><span class="line">SELECT regexp_replace(‘01234abcde56789’,’[09]’,’#’) AS new_str FROM dual;</span><br><span class="line">结果：#1234abcde5678#</span><br><span class="line"></span><br><span class="line">2、遇到非小写字母或者数字跳过，从匹配到的第4个值开始替换，替换为&#x27;&#x27;</span><br><span class="line">SELECT regexp_replace(&#x27;abcdefg123456ABC&#x27;,&#x27;[a-z0-9]&#x27;,&#x27;&#x27;,4)</span><br><span class="line">结果：abcefg123456ABC</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&#x27;abcDEfg123456ABC&#x27;,&#x27;[a-z0-9]&#x27;,&#x27;&#x27;,4)</span><br><span class="line">结果：abcDEg123456ABC</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&#x27;abcDEfg123456ABC&#x27;,&#x27;[a-z0-9]&#x27;,&#x27;&#x27;,7);</span><br><span class="line">结果：abcDEfg13456ABC</span><br><span class="line"></span><br><span class="line">遇到非小写字母或者数字跳过，将所有匹配到的值替换为&#x27;&#x27;</span><br><span class="line">SELECT regexp_replace(&#x27;abcDefg123456ABC&#x27;,&#x27;[a-z0-9]&#x27;,&#x27;&#x27;,0);</span><br><span class="line">结果：DABC</span><br><span class="line"></span><br><span class="line">3、格式化手机号，将+86 13811112222转换为(+86) 138-1111-2222,’+‘在正则表达式中有定义，需要转义。\\1表示引用的第一个组</span><br><span class="line">SELECT regexp_replace(&#x27;+86 13811112222&#x27;,&#x27;(\\+[0-9]&#123;2&#125;)( )([0-9]&#123;3&#125;)([0-9]&#123;4&#125;)([0-9]&#123;4&#125;)&#x27;,&#x27;(\\1)\\3-\\4-\\5&#x27;,0);</span><br><span class="line">结果：(+86)138-1111-2222</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&quot;123.456.7890&quot;,&quot;([[:digit:]]&#123;3&#125;)\\.([[:digit:]]&#123;3&#125;)\\.([[:digit:]]&#123;4&#125;)&quot;,&quot;(\\1)\\2-\\3&quot;,0) ;</span><br><span class="line">SELECT regexp_replace(&quot;123.456.7890&quot;,&quot;([0-9]&#123;3&#125;)\\.([0-9]&#123;3&#125;)\\.([0-9]&#123;4&#125;)&quot;,&quot;(\\1)\\2-\\3&quot;,0) ;</span><br><span class="line">结果：(123)456-7890</span><br><span class="line"></span><br><span class="line">4、将字符用空格分隔开，0表示替换掉所有的匹配子串。</span><br><span class="line">SELECT regexp_replace(&#x27;abcdefg123456ABC&#x27;,&#x27;(.)&#x27;,&#x27;\\1 &#x27;,0) AS new_str FROM dual;</span><br><span class="line">结果：a b c d e f g 1 2 3 4 5 6 A B C</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&#x27;abcdefg123456ABC&#x27;,&#x27;(.)&#x27;,&#x27;\\1 &#x27;,2) AS new_str FROM dual;</span><br><span class="line">结果：ab cdefg123456ABC</span><br><span class="line"></span><br><span class="line">5、</span><br><span class="line">SELECT regexp_replace(&quot;abcd&quot;,&quot;(.*)(.)$&quot;,&quot;\\1&quot;,0) ;</span><br><span class="line">结果：abc</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&quot;abcd&quot;,&quot;(.*)(.)$&quot;,&quot;\\2&quot;,0) ;</span><br><span class="line">结果：d</span><br><span class="line"></span><br><span class="line">SELECT regexp_replace(&quot;abcd&quot;,&quot;(.*)(.)$&quot;,&quot;\\1-\\2&quot;,0) ;</span><br><span class="line">结果：abc-d</span><br></pre></td></tr></table></figure>

<p>正则符号释义：</p>
<p><img src="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/hexo\k12blog\source_posts\我的mysql笔记\zzbds1.jpg" alt="img"></p>
<p><img src="/2022/03/26/%E6%88%91%E7%9A%84mysql%E7%AC%94%E8%AE%B0/hexo\k12blog\source_posts\我的mysql笔记\zzbds2.jpg" alt="img"></p>
<h4 id="RIGHT-s，n-和LEFT-s，n"><a href="#RIGHT-s，n-和LEFT-s，n" class="headerlink" title="RIGHT(s，n)和LEFT(s，n)"></a>RIGHT(s，n)和LEFT(s，n)</h4><p>RIGHT(s，n) 函数返回字符串 s 最右边的 n 个字符。</p>
<p>LEFT(s，n) 函数返回字符串 s 最左边的 n 个字符。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mysql&gt; SELECT RIGHT(&#x27;MySQL&#x27;,3);</span><br><span class="line">+------------------+</span><br><span class="line">| RIGHT(&#x27;MySQL&#x27;,3) |</span><br><span class="line">+------------------+</span><br><span class="line">| SQL              |</span><br><span class="line">+------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">mysql&gt; SELECT LEFT(&#x27;MySQL&#x27;,2);</span><br><span class="line">+-----------------+</span><br><span class="line">| LEFT(&#x27;MySQL&#x27;,2) |</span><br><span class="line">+-----------------+</span><br><span class="line">| My              |</span><br><span class="line">+-----------------+</span><br><span class="line">1 row in set (0.04 sec)</span><br></pre></td></tr></table></figure>

<h4 id="group-concat-X-Y"><a href="#group-concat-X-Y" class="headerlink" title="group_concat(X,Y)"></a>group_concat(X,Y)</h4><p>其中X是要连接的字段，Y是连接时用的符号，可省略，默认为逗号。此函数必须与 GROUP BY 配合使用。（=hive里的concat_ws(“,”,collect_set(字段))）</p>
<p>group_concat函数，实现分组查询之后的数据进行合并，并返回一个字符串结果。</p>
<p>格式：group_concat（ [distinct] 要连接的字段 [order by 排序字段 asc/desc  ] [separator ‘分隔符’] ）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GROUP_CONCAT(</span><br><span class="line">    <span class="keyword">DISTINCT</span> expression</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> expression</span><br><span class="line">    SEPARATOR sep</span><br><span class="line">);</span><br><span class="line">group_concat(<span class="keyword">distinct</span> emp_no <span class="keyword">order</span> <span class="keyword">by</span> emp_no <span class="keyword">asc</span> separator <span class="string">&#x27;,&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="last-day-curdate"><a href="#last-day-curdate" class="headerlink" title="last_day(curdate());"></a>last_day(curdate());</h4><p>获取当月最后一天</p>
<h4 id="cast"><a href="#cast" class="headerlink" title="cast()"></a>cast()</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CAST</span>(expression <span class="keyword">AS</span> TYPE);</span><br></pre></td></tr></table></figure>

<p><code>CAST()</code>函数将任何类型的值转换为具有指定类型的值。目标类型可以是以下类型之一：<code>BINARY</code>，<code>CHAR</code>，<code>DATE</code>，<code>DATETIME</code>，<code>TIME</code>，<code>DECIMAL</code>，<code>SIGNED</code>，<code>UNSIGNED</code>。</p>
<p><code>CAST()</code>函数通常用于返回具有指定类型的值，以便在WHERE、JOIN和HAVING子句中进行比较。</p>
<p>案例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">在下面的例子中，在进行计算之前，MySQL将一个字符串隐式转换成一个整数：</span><br><span class="line">mysql&gt; SELECT (1 + &#x27;1&#x27;)/2;</span><br><span class="line">+-------------+</span><br><span class="line">| (1 + &#x27;1&#x27;)/2 |</span><br><span class="line">+-------------+</span><br><span class="line">|           1 |</span><br><span class="line">+-------------+</span><br><span class="line">1 row in set</span><br><span class="line">SQL</span><br><span class="line"></span><br><span class="line">要将字符串显式转换为整数，可以使用CAST()函数，如以下语句：</span><br><span class="line">mysql&gt; SELECT (1 + CAST(&#x27;1&#x27; AS UNSIGNED))/2;</span><br><span class="line">+-------------------------------+</span><br><span class="line">| (1 + CAST(&#x27;1&#x27; AS UNSIGNED))/2 |</span><br><span class="line">+-------------------------------+</span><br><span class="line">| 1                             |</span><br><span class="line">+-------------------------------+</span><br><span class="line"></span><br><span class="line">为了安全起见，可以使用CAST()函数将字符串显式转换为TIMESTAMP值</span><br><span class="line">SELECT orderNumber,</span><br><span class="line">       requiredDate</span><br><span class="line">FROM orders</span><br><span class="line">WHERE requiredDate BETWEEN  CAST(&#x27;2013-01-01&#x27; AS DATETIME)</span><br><span class="line">                        AND CAST(&#x27;2013-01-31&#x27; AS DATETIME);//原文出</span><br></pre></td></tr></table></figure>

<h4 id="窗口函数LAG-和LEAD"><a href="#窗口函数LAG-和LEAD" class="headerlink" title="窗口函数LAG()和LEAD()"></a>窗口函数LAG()和LEAD()</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">LAG()函数是一个窗口函数，允许您回顾多行并从当前行访问行的数据。</span><br><span class="line">LAG(&lt;expression&gt;[,offset[, default_value]]) OVER (</span><br><span class="line">    PARTITION BY expr,...</span><br><span class="line">    ORDER BY expr [ASC|DESC],...</span><br><span class="line">) </span><br><span class="line">expression：LAG()函数返回expression当前行之前的行的值，其值为offset 其分区或结果集中的行数。</span><br><span class="line">offset：offset是从当前行返回的行数，以获取值。offset必须是零或文字正整数。如果offset为零，则LAG()函数计算expression当前行的值。如果未指定offset，则LAG()默认情况下函数使用一个。</span><br><span class="line">default_value：如果没有前一行，则LAG()函数返回default_value。例如，如果offset为2，则第一行的返回值为default_value。如果省略default_value，则默认LAG()返回函数NULL。</span><br><span class="line">PARTITION BY子句将结果集中的行划分LAG()为应用函数的分区。如果省略PARTITION BY子句，LAG()函数会将整个结果集视为单个分区。</span><br><span class="line">ORDER BY 子句</span><br><span class="line">ORDER BY子句指定在LAG()应用函数之前每个分区中的行的顺序。</span><br><span class="line"></span><br><span class="line">LAG()函数可用于计算当前行和上一行之间的差异。</span><br><span class="line">LEAD()函数是一个窗口函数，允许您向前看多行并从当前行访问行的数据。</span><br><span class="line">与LAG()函数类似，LEAD()函数对于计算同一结果集中当前行和后续行之间的差异非常有用。</span><br></pre></td></tr></table></figure>

<p>窗口函数指定窗口案例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sum(U_Id) over(partition by U_Pwd order by U_Id) 列1,</span><br><span class="line">sum(U_Id) over(partition by U_Pwd order by U_Id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) 列2,</span><br><span class="line">sum(U_Id) over(partition by U_Pwd order by U_Id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) 列3,</span><br><span class="line">sum(U_Id) over(partition by U_Pwd order by U_Id ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING) 列4,</span><br><span class="line">sum(U_Id) over(partition by U_Pwd order by U_Id ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) 列5</span><br></pre></td></tr></table></figure>

<h4 id="窗口函数NTH-VALUE"><a href="#窗口函数NTH-VALUE" class="headerlink" title="窗口函数NTH_VALUE()"></a>窗口函数NTH_VALUE()</h4><p><code>NTH_VALUE()</code>函数返回<code>expression</code>窗口框架第N行的值。如果第N行不存在，则函数返回<code>NULL</code>。</p>
<h4 id="MOD-N-M"><a href="#MOD-N-M" class="headerlink" title="MOD(N,M)"></a>MOD(N,M)</h4><p>该函数返回N除以M后的余数. 分请看下面的例子：                        </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;SELECT MOD(29,3);</span><br><span class="line">+---------------------------------------------------------+</span><br><span class="line">| MOD(29,3)                                               |</span><br><span class="line">+---------------------------------------------------------+</span><br><span class="line">| 2                                                       |</span><br><span class="line">+---------------------------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h4 id="取整函数（ceil、floor、round）"><a href="#取整函数（ceil、floor、round）" class="headerlink" title="取整函数（ceil、floor、round）"></a>取整函数（ceil、floor、round）</h4><p>CEIL()函数：返回大于或等于数字的最小整数值。</p>
<p>floor()函数：返回小于或等于数字的最大整数值。</p>
<p>ROUND(X,D)：此函数返回x舍入到最接近的整数。如果第二个参数，D有提供，则函数返回x四舍五入至第D位小数点。</p>
<h4 id="SUBSTRING-string-position"><a href="#SUBSTRING-string-position" class="headerlink" title="SUBSTRING(string,position)"></a>SUBSTRING(string,position)</h4><p><code>SUBSTRING</code>函数从特定位置开始的字符串返回一个给定长度的子字符串。 MySQL提供了各种形式的子串功能。</p>
<p>substring（被截取字段，从第几位开始截取）<br>substring（被截取字段，从第几位开始截取，截取长度） </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT SUBSTRING(&#x27;MYSQL SUBSTRING&#x27;, 7);</span><br><span class="line">+---------------------------------+</span><br><span class="line">| SUBSTRING(&#x27;MYSQL SUBSTRING&#x27;, 7) |</span><br><span class="line">+---------------------------------+</span><br><span class="line">| SUBSTRING                       |</span><br><span class="line">+---------------------------------+</span><br></pre></td></tr></table></figure>

<p>注意，如果<code>position</code>参数为零，则<code>SUBSTRING</code>函数返回一个空字符串</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT SUBSTRING(&#x27;MYSQL SUBSTRING&#x27;, 0);</span><br><span class="line">+---------------------------------+</span><br><span class="line">| SUBSTRING(&#x27;MYSQL SUBSTRING&#x27;, 0) |</span><br><span class="line">+---------------------------------+</span><br><span class="line">|                                 |</span><br><span class="line">+---------------------------------+</span><br><span class="line">1 row in set</span><br></pre></td></tr></table></figure>

<h4 id="substring-index-str-delim-count"><a href="#substring-index-str-delim-count" class="headerlink" title="substring_index(str,delim,count)"></a>substring_index(str,delim,count)</h4><p>substring_index（str,delim,count）<br>说明：substring_index（被截取字段，关键字，关键字出现的次数） </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT SUBSTRING_INDEX(&#x27;blog.jb51.net&#x27;, 2);</span><br><span class="line">+--------------------------------------------+</span><br><span class="line">| SELECT SUBSTRING_INDEX(&#x27;blog.jb51.net&#x27;, 2) |</span><br><span class="line">+--------------------------------------------+</span><br><span class="line">| blog.jb51                                  |</span><br><span class="line">+--------------------------------------------+</span><br></pre></td></tr></table></figure>

<h4 id="date-add-和date-sub"><a href="#date-add-和date-sub" class="headerlink" title="date_add()和date_sub()"></a>date_add()和date_sub()</h4><p>date_add()：为日期增加一个时间间隔</p>
<p>date_sub()：为日期减去一个时间间隔</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select date_add(now(), interval 1 day); - 加1天</span><br><span class="line">select date_add(now(), interval 1 hour); -加1小时</span><br><span class="line">select date_add(now(), interval 1 minute); - 加1分钟</span><br><span class="line">select date_add(now(), interval 1 second); -加1秒</span><br><span class="line">select date_add(now(), interval 1 microsecond);-加1毫秒</span><br><span class="line">select date_add(now(), interval 1 week);-加1周</span><br><span class="line">select date_add(now(), interval 1 month);-加1月</span><br><span class="line">select date_add(now(), interval 1 quarter);-加1季</span><br><span class="line">select date_add(now(), interval 1 year);-加1年</span><br><span class="line">MySQL date_sub() 日期时间函数 和date_add() 用法一致。</span><br></pre></td></tr></table></figure>

<h4 id="DATEDIFF"><a href="#DATEDIFF" class="headerlink" title="DATEDIFF()"></a>DATEDIFF()</h4><p>DATEDIFF() 函数返回两个日期之间的天数。前者减后者。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT DATEDIFF(&#x27;2008-12-30&#x27;,&#x27;2008-12-29&#x27;) AS DiffDate</span><br><span class="line">+----------+</span><br><span class="line">| DiffDate |</span><br><span class="line">+----------+</span><br><span class="line">| 1        |</span><br><span class="line">+----------+</span><br><span class="line">mysql&gt; SELECT DATEDIFF(&#x27;2008-12-29&#x27;,&#x27;2008-12-30&#x27;) AS DiffDate</span><br><span class="line">+----------+</span><br><span class="line">| DiffDate |</span><br><span class="line">+----------+</span><br><span class="line">| -1        |</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure>

<h4 id="TIMESTAMPDIFF-unit-begin-end"><a href="#TIMESTAMPDIFF-unit-begin-end" class="headerlink" title="TIMESTAMPDIFF(unit,begin,end)"></a>TIMESTAMPDIFF(unit,begin,end)</h4><p><code>TIMESTAMPDIFF</code>函数返回<code>begin-end</code>的结果，其中<code>begin</code>和<code>end</code>是<a target="_blank" rel="noopener" href="http://www.yiibai.com/mysql/date.html">DATE</a>或<a target="_blank" rel="noopener" href="http://www.yiibai.com/mysql/datetime.html">DATETIME</a>表达式。</p>
<p><code>TIMESTAMPDIFF</code>函数允许其参数具有混合类型，例如，<code>begin</code>是<code>DATE</code>值，<code>end</code>可以是<code>DATETIME</code>值。 如果使用<code>DATE</code>值，则<code>TIMESTAMPDIFF</code>函数将其视为时间部分为<code>“00:00:00”</code>的<code>DATETIME</code>值。</p>
<p><code>unit</code>参数是确定(<code>end-begin</code>)的结果的单位，表示为整数。 以下是有效单位：</p>
<p>MICROSECOND、SECOND、MINUTE、HOUR、DAY、WEEK、MONTH、QUARTER、YEAR</p>
<h4 id="字段长度char-length和length"><a href="#字段长度char-length和length" class="headerlink" title="字段长度char_length和length"></a>字段长度char_length和length</h4><p><strong>char_length(str)</strong></p>
<ol>
<li>计算单位：字符</li>
<li>不管汉字还是数字或者是字母都算是一个字符</li>
</ol>
<p><strong>length(str)</strong></p>
<ol>
<li>计算单位：字节</li>
<li>utf8编码：一个汉字三个字节，一个数字或字母一个字节。</li>
<li>gbk编码：一个汉字两个字节，一个数字或字母一个字节。</li>
</ol>
<p>MySQL5.0.3版本之后varchar类型大小的计算方式有所变化，从最早的按字节算大小varchar(length)改成了varchar(char_length)。</p>
<p>1）MySQL 5.0.3 之前：</p>
<ul>
<li>数据类型大小：0–255字节</li>
<li>详解：varchar(20)中的20表示字节数，如果存放utf-8编码的话只能放6个汉字。varchar(n)，这里的n表示字节数。</li>
</ul>
<p>2）MySQL 5.0.3之后：</p>
<ul>
<li>数据类型大小：0–65535字节，但最多占65532字节（其中需要用两个字节存放长度，小于255字节用1个字节存放长度）</li>
<li>详解：varchar(20)表示字符数，不管什么编码，不管是英文还是中文都可以存放20个。</li>
</ul>
<h4 id="COALESCE-value1-value2-…"><a href="#COALESCE-value1-value2-…" class="headerlink" title="COALESCE(value1,value2,…)"></a>COALESCE(value1,value2,…)</h4><p><code>COALESCE</code>函数需要许多参数，并返回第一个非<code>NULL</code>参数。如果所有参数都为<code>NULL</code>，则<code>COALESCE</code>函数返回<code>NULL</code>。</p>
<h4 id="REGEXP：正则表达式查询-rlike"><a href="#REGEXP：正则表达式查询-rlike" class="headerlink" title="REGEXP：正则表达式查询(rlike)"></a>REGEXP：正则表达式查询(rlike)</h4><h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><ol>
<li><p>limit分页</p>
<ol>
<li><p><code>select * from article LIMIT 3 OFFSET 1</code>等价于<code>select* from article LIMIT 1,3</code>都表示取2,3,4三条条数据</p>
</li>
<li><p><strong>select * from table limit (start-1)*pageSize,pageSize;</strong> 其中<strong>start</strong>是页码，<strong>pageSize</strong>是每页显示的条数。</p>
</li>
<li><p>页数公式：totalRecord是总记录数；pageSize是一页分多少条记录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int totalPageNum = (totalRecord +pageSize - 1) / pageSize;</span><br></pre></td></tr></table></figure></li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/youyoui/p/7851007.html">MySQL分页查询优化 </a></p>
</li>
</ol>
</li>
<li><p>根据阿里巴巴Java开发规范v1.4中，数据库规约，关键词应大写，SELECT后面不要跟着“*”，要把具体的字段写出来。</p>
</li>
<li><p>group by vs distinct，group by性能高</p>
<p>对于distinct与group by的使用: </p>
<p>1、当对系统的性能高并数据量大时使用group by </p>
<p>2、当对系统的性能不高时使用数据量少时两者皆可 </p>
<p>3、尽量使用group by</p>
<p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zox2011/archive/2012/09/12/2681797.html">MySQL中distinct和group by性能比较</a></p>
</li>
<li><p>触发器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create trigger audit_log after insert </span><br><span class="line">on employees_test </span><br><span class="line">for each row </span><br><span class="line">begin </span><br><span class="line">INSERT INTO audit (EMP_no,NAME) VALUES (new.id, new.name);</span><br><span class="line">end </span><br></pre></td></tr></table></figure></li>
<li><p>报错：SQL_ERROR_INFO: “You can’t specify target table ‘titles_test’ for update in FROM clause”</p>
<p>原因：同一张表的 UPDATE 操作和 SELECT 不能同时进行</p>
<p>解决：在子查询中重命名表格名（必须），再进行 SELECT</p>
</li>
<li><p>update操作，要注意若干列之间只能用逗号连接，切勿用AND 连接。</p>
</li>
<li><p>mysql中修改表信息的规则。</p>
<p>alter table 表名 change 原列名 新列名 类型； –修改表的列属性名</p>
<p>alter table 表名 modify 列名 类型 ； –修改表的类类型</p>
<p>alter table 表名 drop 列名； –删除表的某一列</p>
<p>alter table 表名 add 列名 类型；–添加某一列</p>
<p>alter table 表名 rename 新表名； –修改表名</p>
</li>
<li><p>表改名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE titles_test RENAME TO titles_2017;</span><br><span class="line">#另外一种写法：</span><br><span class="line">RENAME TABLE titles_test TO titles_2017;</span><br></pre></td></tr></table></figure></li>
<li><p>SQL解决同一时刻最大数量问题</p>
<p>用union把in_time和out_time放到同一个表中，并添加一个字段diff区分，intime的值为1，outtime的值为-1，然后用窗口函数在每个时间点上做sum计算，有规定先算in再算out的话，order by再添加一个diff字段。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/28/Sqoop%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/28/Sqoop%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">Sqoop：部署与使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-28 06:57:07" itemprop="dateCreated datePublished" datetime="2022-02-28T06:57:07+08:00">2022-02-28</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-03-02 01:14:56" itemprop="dateModified" datetime="2022-03-02T01:14:56+08:00">2022-03-02</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><blockquote>
<p>Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.</p>
<p><a target="_blank" rel="noopener" href="https://sqoop.apache.org/">https://sqoop.apache.org/</a></p>
</blockquote>
<p>​    传统的应用管理系统，也就是与关系型数据库的使用RDBMS应用程序的交互，是产生大数据的来源之一。这样大的数据，由关系数据库生成的，存储在关系数据库结构关系数据库服务器。</p>
<p>​    当大数据存储器和分析器，如MapReduce, Hive, HBase, Cassandra, Pig等，Hadoop的生态系统等应运而生图片，它们需要一个工具来用的导入和导出的大数据驻留在其中的关系型数据库服务器进行交互。在这里，Sqoop占据着Hadoop生态系统提供关系数据库服务器和Hadoop HDFS之间的可行的互动。</p>
<p>​    Sqoop是Hadoop和关系数据库服务器之间传送数据的一种工具。它是用来从关系数据库如MySQL，Oracle到Hadoop的HDFS从Hadoop文件系统导出数据到关系数据库。</p>
<p><img src="/2022/02/28/Sqoop%E9%83%A8%E7%BD%B2/sqoop1.jpg" alt="Sqoop的工作流程"></p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="下载tar包"><a href="#下载tar包" class="headerlink" title="下载tar包"></a>下载tar包</h3><p>地址：<a target="_blank" rel="noopener" href="http://archive.apache.org/dist/sqoop/">http://archive.apache.org/dist/sqoop/</a></p>
<p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz">sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</a></p>
<h3 id="解压到相应目录"><a href="#解压到相应目录" class="headerlink" title="解压到相应目录"></a>解压到相应目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 software]$ mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop-1.4.7</span><br><span class="line">[hadoop@hadoop001 software]$ tar -xzvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz </span><br><span class="line">[hadoop@hadoop001 app]$ ln -s /home/hadoop/software/sqoop-1.4.7 /home/hadoop/app/sqoop</span><br></pre></td></tr></table></figure>

<h3 id="修改conf-sqoop-env-sh"><a href="#修改conf-sqoop-env-sh" class="headerlink" title="修改conf/sqoop-env.sh"></a>修改conf/sqoop-env.sh</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sqoop]$ cd conf/</span><br><span class="line">[hadoop@hadoop001 conf]$ cp sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">[hadoop@hadoop001 conf]$ ll</span><br><span class="line">total 32</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 3895 Dec 18  2017 oraoop-site-template.xml</span><br><span class="line">-rwxr-xr-x. 1 hadoop hadoop 1345 Feb 27 23:04 sqoop-env.sh</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 1404 Dec 18  2017 sqoop-env-template.cmd</span><br><span class="line">-rwxr-xr-x. 1 hadoop hadoop 1345 Dec 18  2017 sqoop-env-template.sh</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6044 Dec 18  2017 sqoop-site-template.xml</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6044 Dec 18  2017 sqoop-site.xml</span><br></pre></td></tr></table></figure>

<p>配置相关变量(本机暂未部署hbase和zk)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Set Hadoop-specific environment variables here.</span><br><span class="line"></span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/app/hadoop</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/app/hadoop</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/home/hadoop/app/hive</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br></pre></td></tr></table></figure>

<h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ vi .bash_profile </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SQOOP_HOME=/home/hadoop/app/sqoop</span><br><span class="line">export PATH=$&#123;SQOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ source .bash_profile </span><br></pre></td></tr></table></figure>

<h3 id="拷贝mysql驱动包到sqoop的lib目录下"><a href="#拷贝mysql驱动包到sqoop的lib目录下" class="headerlink" title="拷贝mysql驱动包到sqoop的lib目录下"></a>拷贝mysql驱动包到sqoop的lib目录下</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cp lib/mysql-connector-java-5.1.47.jar app/sqoop/lib/</span><br></pre></td></tr></table></figure>

<h3 id="测试Sqoop是否能够成功连接数据库"><a href="#测试Sqoop是否能够成功连接数据库" class="headerlink" title="测试Sqoop是否能够成功连接数据库"></a>测试Sqoop是否能够成功连接数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://hadoop001:3306 --username root --password 123456</span><br></pre></td></tr></table></figure>

<h3 id="成功访问"><a href="#成功访问" class="headerlink" title="成功访问"></a>成功访问</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ sqoop list-databases --connect jdbc:mysql://hadoop001:3306 --username root --password &#x27;123456&#x27;</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2022-02-27 23:54:08,530 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-02-27 23:54:08,647 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-02-27 23:54:08,832 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">Sun Feb 27 23:54:09 GMT 2022 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">information_schema</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">sys</span><br></pre></td></tr></table></figure>

<h3 id="问题小结"><a href="#问题小结" class="headerlink" title="问题小结"></a>问题小结</h3><ul>
<li><p><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop list-databases --connect jdbc:mysql://hadoop001:3306 --username root --password &#x27;123456&#x27;</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2022-02-27 23:25:34,723 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-02-27 23:25:34,838 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-02-27 23:25:35,007 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils</span><br><span class="line">	at org.apache.sqoop.manager.MySQLManager.initOptionDefaults(MySQLManager.java:73)</span><br><span class="line">	at org.apache.sqoop.manager.SqlManager.&lt;init&gt;(SqlManager.java:89)</span><br><span class="line">	at com.cloudera.sqoop.manager.SqlManager.&lt;init&gt;(SqlManager.java:33)</span><br><span class="line">	at org.apache.sqoop.manager.GenericJdbcManager.&lt;init&gt;(GenericJdbcManager.java:51)</span><br><span class="line">	at com.cloudera.sqoop.manager.GenericJdbcManager.&lt;init&gt;(GenericJdbcManager.java:30)</span><br><span class="line">	at org.apache.sqoop.manager.CatalogQueryManager.&lt;init&gt;(CatalogQueryManager.java:46)</span><br><span class="line">	at com.cloudera.sqoop.manager.CatalogQueryManager.&lt;init&gt;(CatalogQueryManager.java:31)</span><br><span class="line">	at org.apache.sqoop.manager.InformationSchemaManager.&lt;init&gt;(InformationSchemaManager.java:38)</span><br><span class="line">	at com.cloudera.sqoop.manager.InformationSchemaManager.&lt;init&gt;(InformationSchemaManager.java:31)</span><br><span class="line">	at org.apache.sqoop.manager.MySQLManager.&lt;init&gt;(MySQLManager.java:65)</span><br><span class="line">	at org.apache.sqoop.manager.DefaultManagerFactory.accept(DefaultManagerFactory.java:67)</span><br><span class="line">	at org.apache.sqoop.ConnFactory.getManager(ConnFactory.java:184)</span><br><span class="line">	at org.apache.sqoop.tool.BaseSqoopTool.init(BaseSqoopTool.java:272)</span><br><span class="line">	at org.apache.sqoop.tool.ListDatabasesTool.run(ListDatabasesTool.java:44)</span><br><span class="line">	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)</span><br><span class="line">	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)</span><br><span class="line">	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.commons.lang.StringUtils</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">	... 20 more</span><br><span class="line">[hadoop@hadoop001 ~]$ </span><br></pre></td></tr></table></figure>

<p><strong>原因</strong>：Caused by: java.lang.ClassNotFoundException: org.apache.commons.lang.StringUtils</p>
<p>Sqoop1.4.7默认只加载了commons-lang3-3.4.jar的jar包，里面的StringUtils类的package为：org/apache/commons/lang3/StringUtils，所以直接使用sqoop命令时报上述错误。</p>
<p><strong>解决方法</strong>：</p>
<p>将旧版的jar包下载并导入到sqoop目录下的lib目录下即可</p>
<p>下载：<a target="_blank" rel="noopener" href="https://repo.maven.apache.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar">commons-lang-2.6.jar</a></p>
</li>
<li><p>MySQL登录验证失败</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2022-02-27 23:45:27,936 ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.RuntimeException: java.sql.SQLException: Access denied for user &#x27;root&#x27;@&#x27;hadoop001&#x27; (using password: YES)</span><br><span class="line">java.lang.RuntimeException: java.sql.SQLException: Access denied for user &#x27;root&#x27;@&#x27;hadoop001&#x27; (using password: YES)</span><br></pre></td></tr></table></figure>

<p>检查密码有没有输入出错，在<code>--password</code>选项建议添加单引号输入，如：<code>&#39;password&#39;</code></p>
</li>
</ul>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop help</span><br><span class="line">2022-03-01 16:31:57,832 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See &#x27;sqoop help COMMAND&#x27; for information on a specific command.</span><br></pre></td></tr></table></figure>



<h3 id="列出数据库list-databases"><a href="#列出数据库list-databases" class="headerlink" title="列出数据库list-databases"></a>列出数据库list-databases</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop list-databases --connect jdbc:mysql://hadoop001:3306?useSSL=false --username root --password &#x27; 123456&#x27;</span><br><span class="line">2022-02-28 10:35:17,817 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-02-28 10:35:17,972 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-02-28 10:35:18,212 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">information_schema</span><br><span class="line">azkaban</span><br><span class="line">hive</span><br><span class="line">hue</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">ruozedata</span><br><span class="line">sys</span><br></pre></td></tr></table></figure>



<h3 id="列出所有表list-databases"><a href="#列出所有表list-databases" class="headerlink" title="列出所有表list-databases"></a>列出所有表list-databases</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop list-tables --connect jdbc:mysql://hadoop001:3306/mysql?useSSL=false --username root --password &#x27; 123456&#x27;</span><br><span class="line">2022-02-28 10:38:45,712 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-02-28 10:38:45,823 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-02-28 10:38:46,000 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">columns_priv</span><br><span class="line">db</span><br><span class="line">engine_cost</span><br><span class="line">event</span><br><span class="line">func</span><br><span class="line">general_log</span><br><span class="line">gtid_executed</span><br><span class="line">help_category</span><br><span class="line">help_keyword</span><br><span class="line">help_relation</span><br><span class="line">help_topic</span><br><span class="line">innodb_index_stats</span><br><span class="line">innodb_table_stats</span><br><span class="line">ndb_binlog_index</span><br><span class="line">plugin</span><br><span class="line">proc</span><br><span class="line">procs_priv</span><br><span class="line">proxies_priv</span><br><span class="line">server_cost</span><br><span class="line">servers</span><br><span class="line">slave_master_info</span><br><span class="line">slave_relay_log_info</span><br><span class="line">slave_worker_info</span><br><span class="line">slow_log</span><br><span class="line">tables_priv</span><br><span class="line">time_zone</span><br><span class="line">time_zone_leap_second</span><br><span class="line">time_zone_name</span><br><span class="line">time_zone_transition</span><br><span class="line">time_zone_transition_type</span><br><span class="line">user</span><br></pre></td></tr></table></figure>



<h3 id="导入import"><a href="#导入import" class="headerlink" title="导入import"></a>导入import</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop help import</span><br><span class="line">usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]</span><br><span class="line"></span><br><span class="line">Common arguments:</span><br><span class="line">   --connect &lt;jdbc-uri&gt;                                       Specify JDBC connect string</span><br><span class="line">   --connection-manager &lt;class-name&gt;                          Specify connection manager class name</span><br><span class="line">   --connection-param-file &lt;properties-file&gt;                  Specify connection parameters file</span><br><span class="line">   --driver &lt;class-name&gt;                                      Manually specify JDBC driver class to use</span><br><span class="line">   --hadoop-home &lt;hdir&gt;                                       Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --hadoop-mapred-home &lt;dir&gt;                                 Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --help                                                     Print usage instructions</span><br><span class="line">   --metadata-transaction-isolation-level &lt;isolationlevel&gt;    Defines the transaction isolation level for metadata queries. </span><br><span class="line">                                                              For more details check java.sql.Connection javadoc </span><br><span class="line">                                                              or the JDBC specificaiton</span><br><span class="line">   --oracle-escaping-disabled &lt;boolean&gt;                       Disable the escaping mechanism of the Oracle/OraOop </span><br><span class="line">                                                              connection managers</span><br><span class="line">-P                                                            Read password from console</span><br><span class="line">   --password &lt;password&gt;                                      Set authentication password</span><br><span class="line">   --password-alias &lt;password-alias&gt;                          Credential provider passwor alias</span><br><span class="line">   --password-file &lt;password-file&gt;                            Set authentication password file path</span><br><span class="line">   --relaxed-isolation                                        Use read-uncommitted isolation for imports</span><br><span class="line">   --skip-dist-cache                                          Skip copying jars to distributed cache</span><br><span class="line">   --temporary-rootdir &lt;rootdir&gt;                              Defines the temporary root directory for the import</span><br><span class="line">   --throw-on-error                                           Rethrow a RuntimeException on error occurred during the job</span><br><span class="line">   --username &lt;username&gt;                                      Set authenticati on username</span><br><span class="line">   --verbose                                                  Print more information while working</span><br><span class="line"></span><br><span class="line">Import control arguments:</span><br><span class="line">   --append                                                   Imports data in append mode</span><br><span class="line">   --as-avrodatafile                                          Imports data to Avro data files</span><br><span class="line">   --as-parquetfile                                           Imports data to Parquet files</span><br><span class="line">   --as-sequencefile                                          Imports data to SequenceFiles</span><br><span class="line">   --as-textfile                                              Imports data as plain text (default)</span><br><span class="line">   --autoreset-to-one-mapper                                  Reset the number of mappers to one mapper </span><br><span class="line">                                                              if no split key available</span><br><span class="line">   --boundary-query &lt;statement&gt;                               Set boundary query for retrieving max and min value of </span><br><span class="line">                                                              the primary key</span><br><span class="line">   --columns &lt;col,col,col...&gt;                                 Columns to import from table</span><br><span class="line">   --compression-codec &lt;codec&gt;                                Compression codec to use for import</span><br><span class="line">   --delete-target-dir                                        Imports data in delete mode</span><br><span class="line">   --direct                                                   Use direct import fast path</span><br><span class="line">   --direct-split-size &lt;n&gt;                                    Split the input stream every &#x27;n&#x27; bytes when importing in                                                                         direct mode</span><br><span class="line">-e,--query &lt;statement&gt;                                        Import results of SQL &#x27;statement&#x27;</span><br><span class="line">   --fetch-size &lt;n&gt;                                           Set number &#x27;n&#x27; of rows to fetch from the database when                                                                           more rows are needed</span><br><span class="line">   --inline-lob-limit &lt;n&gt;                                     Set the maximum size for an inline LOB</span><br><span class="line">-m,--num-mappers &lt;n&gt;                                          Use &#x27;n&#x27; map tasks to import in parallel</span><br><span class="line">   --mapreduce-job-name &lt;name&gt;                                Set name for generated mapreduce job</span><br><span class="line">   --merge-key &lt;column&gt;                                       Key column to use to join results</span><br><span class="line">   --split-by &lt;column-name&gt;                                   Column of the table used to split work units</span><br><span class="line">   --split-limit &lt;size&gt;                                       Upper Limit of rows per split for split columns </span><br><span class="line">                                                              of Date/Time/Timestamp and integer types. For date or timestamp</span><br><span class="line">                                                              fields it is calculated in seconds. split-limit should be</span><br><span class="line">                                                              greater than 0</span><br><span class="line">   --table &lt;table-name&gt;                                       Table to read</span><br><span class="line">   --target-dir &lt;dir&gt;                                         HDFS plain table destination</span><br><span class="line">   --validate                                                 Validate the copy using the configured validator</span><br><span class="line">   --validation-failurehandler &lt;validation-failurehandler&gt;    Fully qualified class name for ValidationFailureHandler</span><br><span class="line">   --validation-threshold &lt;validation-threshold&gt;              Fully qualified class name for ValidationThreshold</span><br><span class="line">   --validator &lt;validator&gt;                                    Fully qualified class name for the Validator</span><br><span class="line">   --warehouse-dir &lt;dir&gt;                                      HDFS parent for table destination</span><br><span class="line">   --where &lt;where clause&gt;                                     WHERE clause to use during import</span><br><span class="line">-z,--compress                                                 Enable compression</span><br><span class="line"></span><br><span class="line">Incremental import arguments:</span><br><span class="line">   --check-column &lt;column&gt;        Source column to check for incremental change</span><br><span class="line">   --incremental &lt;import-type&gt;    Define an incremental import of type &#x27;append&#x27; or &#x27;lastmodified&#x27;</span><br><span class="line">   --last-value &lt;value&gt;           Last imported value in the incremental check column</span><br><span class="line"></span><br><span class="line">Output line formatting arguments:</span><br><span class="line">   --enclosed-by &lt;char&gt;               Sets a required field enclosing character</span><br><span class="line">   --escaped-by &lt;char&gt;                Sets the escape character</span><br><span class="line">   --fields-terminated-by &lt;char&gt;      Sets the field separator character</span><br><span class="line">   --lines-terminated-by &lt;char&gt;       Sets the end-of-line character</span><br><span class="line">   --mysql-delimiters                 Uses MySQL&#x27;s default delimiter set: fields: ,  lines: \n  escaped-by: \</span><br><span class="line">                                      optionally-enclosed-by: &#x27;</span><br><span class="line">   --optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Input parsing arguments:</span><br><span class="line">   --input-enclosed-by &lt;char&gt;               Sets a required field encloser</span><br><span class="line">   --input-escaped-by &lt;char&gt;                Sets the input escape character</span><br><span class="line">   --input-fields-terminated-by &lt;char&gt;      Sets the input field separator</span><br><span class="line">   --input-lines-terminated-by &lt;char&gt;       Sets the input end-of-line char</span><br><span class="line">   --input-optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Hive arguments:</span><br><span class="line">   --create-hive-table                         Fail if the target hive table exists</span><br><span class="line">   --external-table-dir &lt;hdfs path&gt;            Sets where the external table is in HDFS</span><br><span class="line">   --hive-database &lt;database-name&gt;             Sets the database name to use when importing to hive</span><br><span class="line">   --hive-delims-replacement &lt;arg&gt;             Replace Hive record \0x01 and row delimiters (\n\r)</span><br><span class="line">                                               from imported string fields with user-defined string</span><br><span class="line">   --hive-drop-import-delims                   Drop Hive record \0x01 and row delimiters (\n\r) from imported string fields</span><br><span class="line">   --hive-home &lt;dir&gt;                           Override $HIVE_HOME</span><br><span class="line">   --hive-import                               Import tables into Hive (Uses Hive&#x27;s default delimiters if none are set.)</span><br><span class="line">   --hive-overwrite                            Overwrite existing data in the Hive table</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;        Sets the partition key to use when importing to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;    Sets the partition value to use when importing to hive</span><br><span class="line">   --hive-table &lt;table-name&gt;                   Sets the table name to use when importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                     Override mapping for specific column to hive types.</span><br><span class="line"></span><br><span class="line">HBase arguments:</span><br><span class="line">   --column-family &lt;family&gt;    Sets the target column family for the import</span><br><span class="line">   --hbase-bulkload            Enables HBase bulk loading</span><br><span class="line">   --hbase-create-table        If specified, create missing HBase tables</span><br><span class="line">   --hbase-row-key &lt;col&gt;       Specifies which input column to use as the row key</span><br><span class="line">   --hbase-table &lt;table&gt;       Import to &lt;table&gt; in HBase</span><br><span class="line"></span><br><span class="line">HCatalog arguments:</span><br><span class="line">   --hcatalog-database &lt;arg&gt;                        HCatalog database name</span><br><span class="line">   --hcatalog-home &lt;hdir&gt;                           Override $HCAT_HOME</span><br><span class="line">   --hcatalog-partition-keys &lt;partition-key&gt;        Sets the partition keys to use when importing to hive</span><br><span class="line">   --hcatalog-partition-values &lt;partition-value&gt;    Sets the partition values to use when importing to hive</span><br><span class="line">   --hcatalog-table &lt;arg&gt;                           HCatalog table name</span><br><span class="line">   --hive-home &lt;dir&gt;                                Override $HIVE_HOME</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;             Sets the partition key to use when importing to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;         Sets the partition value to use when importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                          Override mapping for specific column to hive types.</span><br><span class="line"></span><br><span class="line">HCatalog import specific options:</span><br><span class="line">   --create-hcatalog-table             Create HCatalog before import</span><br><span class="line">   --drop-and-create-hcatalog-table    Drop and Create HCatalog before import</span><br><span class="line">   --hcatalog-storage-stanza &lt;arg&gt;     HCatalog storage stanza for table creation</span><br><span class="line"></span><br><span class="line">Accumulo arguments:</span><br><span class="line">   --accumulo-batch-size &lt;size&gt;          Batch size in bytes</span><br><span class="line">   --accumulo-column-family &lt;family&gt;     Sets the target column family for the import</span><br><span class="line">   --accumulo-create-table               If specified, create missing Accumulo tables</span><br><span class="line">   --accumulo-instance &lt;instance&gt;        Accumulo instance name.</span><br><span class="line">   --accumulo-max-latency &lt;latency&gt;      Max write latency in milliseconds</span><br><span class="line">   --accumulo-password &lt;password&gt;        Accumulo password.</span><br><span class="line">   --accumulo-row-key &lt;col&gt;              Specifies which input column to use as the row key</span><br><span class="line">   --accumulo-table &lt;table&gt;              Import to &lt;table&gt; in Accumulo</span><br><span class="line">   --accumulo-user &lt;user&gt;                Accumulo user name.</span><br><span class="line">   --accumulo-visibility &lt;vis&gt;           Visibility token to be applied to all rows imported</span><br><span class="line">   --accumulo-zookeepers &lt;zookeepers&gt;    Comma-separated list of zookeepers (host:port)</span><br><span class="line"></span><br><span class="line">Code generation arguments:</span><br><span class="line">   --bindir &lt;dir&gt;                             Output directory for compiled objects</span><br><span class="line">   --class-name &lt;name&gt;                        Sets the generated class name. This overrides --package-name.</span><br><span class="line">                                              When combined with --jar-file, sets the input class.</span><br><span class="line">   --escape-mapping-column-names &lt;boolean&gt;    Disable special characters escaping in column names</span><br><span class="line">   --input-null-non-string &lt;null-str&gt;         Input null non-string representation</span><br><span class="line">   --input-null-string &lt;null-str&gt;             Input null string representation</span><br><span class="line">   --jar-file &lt;file&gt;                          Disable code generation; use specified jar</span><br><span class="line">   --map-column-java &lt;arg&gt;                    Override mapping for specific columns to java types</span><br><span class="line">   --null-non-string &lt;null-str&gt;               Null non-string representation</span><br><span class="line">   --null-string &lt;null-str&gt;                   Null string representation</span><br><span class="line">   --outdir &lt;dir&gt;                             Output directory for generated code</span><br><span class="line">   --package-name &lt;name&gt;                      Put auto-generated classes in this package</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At minimum, you must specify --connect and --table</span><br><span class="line">Arguments to mysqldump and other subprograms may be supplied</span><br><span class="line">after a &#x27;--&#x27; on the command line.</span><br></pre></td></tr></table></figure>

<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ol>
<li><p>导入路径默认是在/user/{username}/下，我的hadoop用户名为hadoop，所以导出路径是/user/hadoop/EMP_COLUMN</p>
</li>
<li><p>默认导入是4个文件，是同时4个task在运行的</p>
</li>
<li><p>当table没有设置primary key时，需要指定<code>--split-by</code>或者设置并行度为<code>-m 1</code>,因为如果并行度不为1，导出表是需要根据指定的字段或者主键计算分割到每个task任务的记录数量。</p>
</li>
<li><p>使用<code>-e</code>或者<code>--query</code>查询数据时，要在语句里where条件上加上<code>&#39;$CONDITIONS&#39;</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Import failed: java.io.IOException: Query [SELECT * FROM emp WHERE EMPNO&gt;=7900] must contain &#x27;$CONDITIONS&#x27; in WHERE clause.`</span><br><span class="line">-e &quot;SELECT * FROM emp WHERE EMPNO&gt;=7900&quot;</span><br><span class="line">应该写成：</span><br><span class="line">-e &quot;SELECT * FROM emp WHERE EMPNO&gt;=7900 AND \$CONDITIONS&quot;</span><br></pre></td></tr></table></figure>

<p>同时，<code>-e</code>和<code>--query</code>不与<code>--where</code>、<code>--columns</code>、<code>--table</code>同时使用。</p>
</li>
</ol>
<h4 id="hadoop案例"><a href="#hadoop案例" class="headerlink" title="hadoop案例"></a>hadoop案例</h4><p>MySQL表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use ruozedata;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select * from emp;</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">| empno | ename  | job       | mgr  | hiredate            | sal     | comm    | deptno |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 00:00:00 |  800.00 |    NULL |     20 |</span><br><span class="line">|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 00:00:00 | 1600.00 |  300.00 |     30 |</span><br><span class="line">|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 00:00:00 | 1250.00 |  500.00 |     30 |</span><br><span class="line">|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 00:00:00 | 2975.00 |    NULL |     20 |</span><br><span class="line">|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 00:00:00 | 1250.00 | 1400.00 |     30 |</span><br><span class="line">|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 00:00:00 | 2850.00 |    NULL |     30 |</span><br><span class="line">|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 00:00:00 | 2450.00 |    NULL |     10 |</span><br><span class="line">|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 00:00:00 | 5000.00 |    NULL |     10 |</span><br><span class="line">|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 00:00:00 | 1500.00 |    0.00 |     30 |</span><br><span class="line">|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 00:00:00 | 1100.00 |    NULL |     20 |</span><br><span class="line">|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 00:00:00 |  950.00 |    NULL |     30 |</span><br><span class="line">|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 00:00:00 | 3000.00 |    NULL |     20 |</span><br><span class="line">|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 00:00:00 | 1300.00 |    NULL |     10 |</span><br><span class="line">+-------+--------+-----------+------+---------------------+---------+---------+--------+</span><br><span class="line">14 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure>

<p>导出ruozedata数据库emp表到hadoop的/home/{username}下</p>
<p>import语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/ruozedata \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;123456&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--columns &quot;EMPNO,ENAME,JOB,SAL,COMM&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS \</span><br><span class="line">--table emp \</span><br><span class="line">--null-string &#x27;&#x27; \</span><br><span class="line">--null-non-string 0 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--where &#x27;SAL&gt;2000&#x27; \</span><br><span class="line">--target-dir EMP_COLUMN</span><br><span class="line">-m 1 \</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>–connect：连接的数据库url</li>
<li>–username：访问用户名</li>
<li>–password：访问用户密码</li>
<li>–delete-target-dir：先删除数据目录</li>
<li>–columns：选择导出的字段，没有该选项则全部导出</li>
<li>–mapreduce-job-name：重命名MapReduce作业名称</li>
<li>–table emp：指定要导出的表</li>
<li>–null-string：表中string类型字段为null时填充的值</li>
<li>–null-non-string：表中非string类型字段为null时填充的值</li>
<li>–fields-terminated-by：输出文件中字段分隔符</li>
<li>–where：过滤条件</li>
<li>–target-dir：输出目录</li>
<li>-m：并行度，即MR的task数量</li>
</ul>
<h4 id="Hive案例"><a href="#Hive案例" class="headerlink" title="Hive案例"></a>Hive案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/ruozedata \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;123456&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database ruozedata_hive \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table emp_column \</span><br><span class="line">--columns &quot;EMPNO,ENAME,JOB,SAL,COMM&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2Hive \</span><br><span class="line">--table emp \</span><br><span class="line">--null-string &#x27;&#x27; \</span><br><span class="line">--null-non-string 0 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; </span><br><span class="line">--hive-partition-key &#x27;day&#x27; \</span><br><span class="line">--hive-partition-value &#x27;yyyyMMdd&#x27; \</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li><p>–hive-database：指定导入到的hive数据库</p>
</li>
<li><p>–hive-import：导入hive</p>
</li>
<li><p>–hive-overwrite：覆盖模式</p>
</li>
<li><p>–hive-table：导入的hive表名</p>
<p>如果是分区表，还有以下2个选项</p>
</li>
<li><p>–hive-partition-key：分区字段key</p>
</li>
<li><p>–hive-partition-value：分区字段值value</p>
</li>
</ul>
<h3 id="导出export"><a href="#导出export" class="headerlink" title="导出export"></a>导出export</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop help export</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2022-02-28 11:50:42,057 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">usage: sqoop export [GENERIC-ARGS] [TOOL-ARGS]</span><br><span class="line"></span><br><span class="line">Common arguments:</span><br><span class="line">   --connect &lt;jdbc-uri&gt;                                       Specify JDBC connect string</span><br><span class="line">   --connection-manager &lt;class-name&gt;                          Specify connection manager class name</span><br><span class="line">   --connection-param-file &lt;properties-file&gt;                  Specify connection parameters file</span><br><span class="line">   --driver &lt;class-name&gt;                                      Manually specify JDBC driver class to use</span><br><span class="line">   --hadoop-home &lt;hdir&gt;                                       Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --hadoop-mapred-home &lt;dir&gt;                                 Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --help                                                     Print usage instructions</span><br><span class="line">   --metadata-transaction-isolation-level &lt;isolationlevel&gt;    Defines the transaction isolation level for metadata queries. </span><br><span class="line">                                                              For more details check java.sql.Connection javadoc </span><br><span class="line">                                                              or the JDBC specificaiton</span><br><span class="line">   --oracle-escaping-disabled &lt;boolean&gt;                       Disable the escaping mechanism of the  Oracle/OraOop connection</span><br><span class="line">                                                              managers Read password from console</span><br><span class="line">   --password &lt;password&gt;                                      Set authenticati on password</span><br><span class="line">   --password-alias &lt;password-alias&gt;                          Credential provider password alias</span><br><span class="line">   --password-file &lt;password-file&gt;                            Set authentication password file path</span><br><span class="line">   --relaxed-isolation                                        Use read-uncommitted isolation for imports</span><br><span class="line">   --skip-dist-cache                                          Skip copying jars to distributed cache</span><br><span class="line">   --temporary-rootdir &lt;rootdir&gt;                              Defines the temporary root directory for the import</span><br><span class="line">   --throw-on-error                                           Rethrow a RuntimeException on error occurred during the job</span><br><span class="line">   --username &lt;username&gt;                                      Set authentication username</span><br><span class="line">   --verbose                                                  Print more information while working</span><br><span class="line">   </span><br><span class="line">Export control arguments:</span><br><span class="line">   --batch                                                    Indicates underlying statements to be executed in batch mode</span><br><span class="line">   --call &lt;arg&gt;                                               Populate the table using this stored procedure (one call per row)</span><br><span class="line">   --clear-staging-table                                      Indicates that any data in staging table can be deleted</span><br><span class="line">   --columns &lt;col,col,col...&gt;                                 Columns to export to table</span><br><span class="line">   --direct                                                   Use direct export fast path</span><br><span class="line">   --export-dir &lt;dir&gt;                                         HDFS source path for the export</span><br><span class="line">-m,--num-mappers &lt;n&gt;                                          Use &#x27;n&#x27; maptasks toexport in parallel</span><br><span class="line">   --mapreduce-job-name &lt;name&gt;                                Set name for generated mapreduce job</span><br><span class="line">   --staging-table &lt;table-name&gt;                               Intermediate staging table</span><br><span class="line">   --table &lt;table-name&gt;                                       Table to populate</span><br><span class="line">   --update-key &lt;key&gt;                                         Update records by specified key column</span><br><span class="line">   --update-mode &lt;mode&gt;                                       Specifies how updates are performed when new rows are</span><br><span class="line">                                                              found with non-matching keys in database</span><br><span class="line">   --validate                                                 Validate the copy using the configured validator</span><br><span class="line">   --validation-failurehandler &lt;validation-failurehandler&gt;    Fully qualified class name for ValidationFailureHandler</span><br><span class="line">   --validation-threshold &lt;validation-threshold&gt;              Fully qualified class name for ValidationThreshold</span><br><span class="line">   --validator &lt;validator&gt;                                    Fullyqualified class name for the Validator</span><br><span class="line"></span><br><span class="line">Input parsing arguments:</span><br><span class="line">   --input-enclosed-by &lt;char&gt;               Sets a required field encloser</span><br><span class="line">   --input-escaped-by &lt;char&gt;                Sets the input escape character</span><br><span class="line">   --input-fields-terminated-by &lt;char&gt;      Sets the input field separator</span><br><span class="line">   --input-lines-terminated-by &lt;char&gt;       Sets the input end-of-line char</span><br><span class="line">   --input-optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Output line formatting arguments:</span><br><span class="line">   --enclosed-by &lt;char&gt;               Sets a required field enclosing character</span><br><span class="line">   --escaped-by &lt;char&gt;                Sets the escape character</span><br><span class="line">   --fields-terminated-by &lt;char&gt;      Sets the field separator character</span><br><span class="line">   --lines-terminated-by &lt;char&gt;       Sets the end-of-line character</span><br><span class="line">   --mysql-delimiters                 Uses MySQL&#x27;s default delimiter set: fields: ,  lines: \n  escaped-by: \</span><br><span class="line">                                      optionally-enclosed-by: &#x27;</span><br><span class="line">   --optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Code generation arguments:</span><br><span class="line">   --bindir &lt;dir&gt;                             Output directory for compiled objects</span><br><span class="line">   --class-name &lt;name&gt;                        Sets the generated class name. This overrides --package-name. When</span><br><span class="line">                                              combined with --jar-file, sets the input class.</span><br><span class="line">   --escape-mapping-column-names &lt;boolean&gt;    Disable special characters escaping in column names</span><br><span class="line">   --input-null-non-string &lt;null-str&gt;         Input null non-string representation</span><br><span class="line">   --input-null-string &lt;null-str&gt;             Input null string representation</span><br><span class="line">   --jar-file &lt;file&gt;                          Disable code generation; use specified jar</span><br><span class="line">   --map-column-java &lt;arg&gt;                    Override mapping for specific columns to java types</span><br><span class="line">   --null-non-string &lt;null-str&gt;               Null non-string representation</span><br><span class="line">   --null-string &lt;null-str&gt;                   Null string representation</span><br><span class="line">   --outdir &lt;dir&gt;                             Output directory for generated code</span><br><span class="line">   --package-name &lt;name&gt;                      Put auto-generated classes in this package</span><br><span class="line"></span><br><span class="line">HCatalog arguments:</span><br><span class="line">   --hcatalog-database &lt;arg&gt;                        HCatalog database name</span><br><span class="line">   --hcatalog-home &lt;hdir&gt;                           Override $HCAT_HOME</span><br><span class="line">   --hcatalog-partition-keys &lt;partition-key&gt;        Sets the partition keys to use when importing to hive</span><br><span class="line">   --hcatalog-partition-values &lt;partition-value&gt;    Sets the partition values to use when importing to hive</span><br><span class="line">   --hcatalog-table &lt;arg&gt;                           HCatalog table name</span><br><span class="line">   --hive-home &lt;dir&gt;                                Override $HIVE_HOME</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;             Sets the partition key to use when importing to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;         Sets the partition value to use when importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                          Override mapping for specific column to hive types.</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At minimum, you must specify --connect, --export-dir, and --table</span><br></pre></td></tr></table></figure>

<p>导出emp2表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/ruozedata \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;123456&#x27; \</span><br><span class="line">--table emp2 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--export-dir /user/hadoop/emp</span><br></pre></td></tr></table></figure>



<h3 id="作业job"><a href="#作业job" class="headerlink" title="作业job"></a>作业job</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop job --help</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please set $HBASE_HOME to the root of your HBase installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">Warning: /home/hadoop/app/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">2022-03-01 15:55:19,426 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">usage: sqoop job [GENERIC-ARGS] [JOB-ARGS] [-- [&lt;tool-name&gt;] [TOOL-ARGS]]</span><br><span class="line"></span><br><span class="line">Job management arguments:</span><br><span class="line">   --create &lt;job-id&gt;            Create a new saved job</span><br><span class="line">   --delete &lt;job-id&gt;            Delete a saved job</span><br><span class="line">   --exec &lt;job-id&gt;              Run a saved job</span><br><span class="line">   --help                       Print usage instructions</span><br><span class="line">   --list                       List saved jobs</span><br><span class="line">   --meta-connect &lt;jdbc-uri&gt;    Specify JDBC connect string for the</span><br><span class="line">                                metastore</span><br><span class="line">   --show &lt;job-id&gt;              Show the parameters for a saved job</span><br><span class="line">   --verbose                    Print more information while working</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<p>sqoop 作业的语法是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job (generic-args) (job-args) [-- [subtool-name] (subtool-args)]</span><br></pre></td></tr></table></figure>

<p>注意–后面和subtool-name之间有空格</p>
<h4 id="创建job-–create"><a href="#创建job-–create" class="headerlink" title="创建job(–create)"></a>创建job(–create)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --create myjob -- \</span><br><span class="line">import \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/ruozedata \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;123456&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--columns &quot;EMPNO,ENAME,JOB,SAL,COMM&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS \</span><br><span class="line">--table emp \</span><br><span class="line">--null-string &#x27;&#x27; \</span><br><span class="line">--null-non-string 0 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--where &#x27;SAL&gt;2000&#x27; \</span><br><span class="line">--target-dir EMP_COLUMN \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>

<p>问题：</p>
<p>报错：Exception in thread “main” java.lang.NoClassDefFoundError: org/json/JSONObject</p>
<p>原因：sqoop缺少java-json.jar包.</p>
<p>解决：这是因为sqoop缺少java-json.jar包，下载<a target="_blank" rel="noopener" href="http://www.java2s.com/Code/JarDownload/java-json/java-json.jar.zip">java-json.jar.zip</a>并添加到sqoop/lib目录下</p>
<h4 id="验证作业-–list"><a href="#验证作业-–list" class="headerlink" title="验证作业 (–list)"></a>验证作业 (–list)</h4><p><code>--list</code> 参数是用来验证保存Sqoop作业的列表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop job --list</span><br><span class="line">2022-03-01 16:20:34,767 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">Available jobs:</span><br><span class="line">  myjob</span><br></pre></td></tr></table></figure>

<h4 id="检查作业-–show"><a href="#检查作业-–show" class="headerlink" title="检查作业(–show)"></a>检查作业(–show)</h4><p><code>--show</code> 参数用于检查或验证特定的工作，及其详细信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop job --show myjob</span><br><span class="line">2022-03-01 16:21:05,716 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">Enter password: </span><br><span class="line">Job: myjob</span><br><span class="line">Tool: import</span><br><span class="line">Options:</span><br><span class="line">----------------------------</span><br><span class="line">verbose = false</span><br><span class="line">hcatalog.drop.and.create.table = false</span><br><span class="line">db.connect.string = jdbc:mysql://hadoop001:3306/ruozedata</span><br><span class="line">codegen.output.delimiters.escape = 0</span><br><span class="line">codegen.output.delimiters.enclose.required = false</span><br><span class="line">codegen.input.delimiters.field = 0</span><br><span class="line">mainframe.input.dataset.type = p</span><br><span class="line">hbase.create.table = false</span><br><span class="line">split.limit = null</span><br><span class="line">null.string = </span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="执行作业-–exec"><a href="#执行作业-–exec" class="headerlink" title="执行作业 (–exec)"></a>执行作业 (–exec)</h4><p><code>--exec</code> 选项用于执行保存的作业。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --exec myjob</span><br></pre></td></tr></table></figure>

<h4 id="删除作业-–delete"><a href="#删除作业-–delete" class="headerlink" title="删除作业 (–delete)"></a>删除作业 (–delete)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --delete myjob</span><br></pre></td></tr></table></figure>



<h3 id="eval工具"><a href="#eval工具" class="headerlink" title="eval工具"></a>eval工具</h3><p>它允许用户执行用户定义的查询，对各自的数据库服务器和预览结果在控制台中。这样，用户可以期望得到的表数据来导入。使用eval我们可以评估任何类型的SQL查询可以是DDL或DML语句。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop help eval</span><br><span class="line">2022-03-01 16:38:37,514 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">usage: sqoop eval [GENERIC-ARGS] [TOOL-ARGS]</span><br><span class="line"></span><br><span class="line">Common arguments:</span><br><span class="line">   --connect &lt;jdbc-uri&gt;                                       Specify JDBC connect string</span><br><span class="line">   --connection-manager &lt;class-name&gt;                          Specify connection manager class name</span><br><span class="line">   --connection-param-file &lt;properties-file&gt;                  Specify connection parameters file</span><br><span class="line">   --driver &lt;class-name&gt;                                      Manually specify JDBC driver class to use</span><br><span class="line">   --hadoop-home &lt;hdir&gt;                                       Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --hadoop-mapred-home &lt;dir&gt;                                 Override $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --help                                                     Print usage instructions</span><br><span class="line">   --metadata-transaction-isolation-level &lt;isolationlevel&gt;    Defines the transaction isolation level for metadata queries. </span><br><span class="line">   	                                                          For more details check java.sql.Connection javadoc or the JDBC</span><br><span class="line">                                                              specificaiton</span><br><span class="line">   --oracle-escaping-disabled &lt;boolean&gt;                       Disable the escaping mechanism of the Oracle/OraOop </span><br><span class="line">                                                              connection managers</span><br><span class="line">-P                                                            Read password from console</span><br><span class="line">   --password &lt;password&gt;                                      Set authenticati on password</span><br><span class="line">   --password-alias &lt;password-alias&gt;                          Credential provider password alias</span><br><span class="line">   --password-file &lt;password-file&gt;                            Set authentication password file path</span><br><span class="line">   --relaxed-isolation                                        Use read-uncommitted isolation for imports</span><br><span class="line">   --skip-dist-cache                                          Skip copying jars to distributed cache</span><br><span class="line">   --temporary-rootdir &lt;rootdir&gt;                              Defines the temporary root directory for the import</span><br><span class="line">   --throw-on-error                                           Rethrow a RuntimeExcep tion on error occurred during the job</span><br><span class="line">   --username &lt;username&gt;                                      Set authentication username</span><br><span class="line">   --verbose                                                  Print more information while working</span><br><span class="line"></span><br><span class="line">SQL evaluation arguments:</span><br><span class="line">-e,--query &lt;statement&gt;    Execute &#x27;statement&#x27; in SQL and exit</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ sqoop eval --connect jdbc:mysql://hadoop001:3306/ruozedata --username root --password &#x27;123456&#x27; --query &quot;SELECT * FROM emp where deptno=10&quot;</span><br><span class="line">2022-03-01 16:45:12,146 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-03-01 16:45:12,256 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-03-01 16:45:12,426 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">Tue Mar 01 16:45:12 GMT 2022 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">| empno | ename      | job       | mgr   | hiredate            | sal       | comm      | deptno | </span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">| 7782  | CLARK      | MANAGER   | 7839  | 1981-06-09 00:00:00.0 | 2450.00   | (null)    | 10  | </span><br><span class="line">| 7839  | KING       | PRESIDENT | (null) | 1981-11-17 00:00:00.0 | 5000.00   | (null)    | 10  | </span><br><span class="line">| 7934  | MILLER     | CLERK     | 7782  | 1982-01-23 00:00:00.0 | 1300.00   | (null)    | 10  | </span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">[hadoop@hadoop001 ~]$ sqoop eval --connect jdbc:mysql://hadoop001:3306/ruozedata --username root --password &#x27;ruozedata001&#x27; --query &quot;INSERT INTO emp VALUES(7934,&#x27;KKK&#x27;,&#x27;CLERK&#x27;,7782,&#x27;1985-01-20 00:00:00.0&#x27;,1400,200,10)&quot;</span><br><span class="line">2022-03-01 16:49:02,886 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-03-01 16:49:03,001 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-03-01 16:49:03,169 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">Tue Mar 01 16:49:03 GMT 2022 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">2022-03-01 16:49:03,566 INFO tool.EvalSqlTool: 1 row(s) updated.</span><br><span class="line">[hadoop@hadoop001 ~]$ sqoop eval --connect jdbc:mysql://hadoop001:3306/ruozedata --username root --password &#x27;ruozedata001&#x27; --query &quot;SELECT * FROM emp where deptno=10&quot;</span><br><span class="line">2022-03-01 16:49:28,104 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">2022-03-01 16:49:28,239 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">2022-03-01 16:49:28,391 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">Tue Mar 01 16:49:28 GMT 2022 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">| empno | ename      | job       | mgr   | hiredate            | sal       | comm      | deptno | </span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">| 7782  | CLARK      | MANAGER   | 7839  | 1981-06-09 00:00:00.0 | 2450.00   | (null)    | 10  | </span><br><span class="line">| 7839  | KING       | PRESIDENT | (null) | 1981-11-17 00:00:00.0 | 5000.00   | (null)    | 10  | </span><br><span class="line">| 7934  | MILLER     | CLERK     | 7782  | 1982-01-23 00:00:00.0 | 1300.00   | (null)    | 10  | </span><br><span class="line">| 7934  | KKK        | CLERK     | 7782  | 1985-01-20 00:00:00.0 | 1400.00   | 200.00    | 10  | </span><br><span class="line">----------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/23/Spark-DF-DS-API%EF%BC%9A%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/23/Spark-DF-DS-API%EF%BC%9A%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/" class="post-title-link" itemprop="url">Spark DF/DS API：行列转换</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-23 01:36:53 / 修改时间：01:46:05" itemprop="dateCreated datePublished" datetime="2022-02-23T01:36:53+08:00">2022-02-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>前文回顾：<a href="https://k12coding.github.io/2022/02/07/Hive%EF%BC%9A%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/">Hive：行列转换</a></p>
<h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><h3 id="多行转多列"><a href="#多行转多列" class="headerlink" title="多行转多列"></a>多行转多列</h3><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">val rows1 = new util.ArrayList[Row]()</span><br><span class="line">rows1.add(Row(&quot;a&quot;, &quot;c&quot;, 1))</span><br><span class="line">rows1.add(Row(&quot;a&quot;, &quot;d&quot;, 2))</span><br><span class="line"></span><br><span class="line">rows1.add(Row(&quot;b&quot;, &quot;d&quot;, 5))</span><br><span class="line">rows1.add(Row(&quot;b&quot;, &quot;e&quot;, 6))</span><br><span class="line"></span><br><span class="line">val schema1 = StructType(</span><br><span class="line">StructField(&quot;col1&quot;, StringType)::</span><br><span class="line">StructField(&quot;col2&quot;, StringType)::</span><br><span class="line">StructField(&quot;col3&quot;, IntegerType)::Nil</span><br><span class="line">)</span><br><span class="line">val df1 = spark.createDataFrame(rows1, schema1)</span><br><span class="line"></span><br><span class="line">println(&quot;多行转多列&quot;)</span><br><span class="line">df1.show()</span><br><span class="line"></span><br><span class="line">df1.groupBy(&#x27;col1)</span><br><span class="line">.pivot(&#x27;col2)</span><br><span class="line">.max(&quot;col3&quot;).na.fill(0)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">多行转多列</span><br><span class="line">+----+----+----+</span><br><span class="line">|col1|col2|col3|</span><br><span class="line">+----+----+----+</span><br><span class="line">|   a|   c|   1|</span><br><span class="line">|   a|   d|   2|</span><br><span class="line">|   b|   d|   5|</span><br><span class="line">|   b|   e|   6|</span><br><span class="line">+----+----+----+</span><br><span class="line"></span><br><span class="line">+----+---+---+---+</span><br><span class="line">|col1|  c|  d|  e|</span><br><span class="line">+----+---+---+---+</span><br><span class="line">|   b|  0|  5|  6|</span><br><span class="line">|   a|  1|  2|  0|</span><br><span class="line">+----+---+---+---+</span><br></pre></td></tr></table></figure>

<h3 id="多行转单列"><a href="#多行转单列" class="headerlink" title="多行转单列"></a>多行转单列</h3><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val rows2 = new util.ArrayList[Row]()</span><br><span class="line">rows2.add(Row(&quot;a&quot;, &quot;b&quot;, 1))</span><br><span class="line">rows2.add(Row(&quot;a&quot;, &quot;b&quot;, 2))</span><br><span class="line">rows2.add(Row(&quot;a&quot;, &quot;b&quot;, 3))</span><br><span class="line">rows2.add(Row(&quot;b&quot;, &quot;d&quot;, 4))</span><br><span class="line">rows2.add(Row(&quot;b&quot;, &quot;d&quot;, 5))</span><br><span class="line">rows2.add(Row(&quot;b&quot;, &quot;d&quot;, 6))</span><br><span class="line"></span><br><span class="line">val schema2 = StructType(</span><br><span class="line">StructField(&quot;col1&quot;, StringType)::</span><br><span class="line">StructField(&quot;col2&quot;, StringType)::</span><br><span class="line">StructField(&quot;col3&quot;, IntegerType)::Nil</span><br><span class="line">)</span><br><span class="line">val df2 = spark.createDataFrame(rows2, schema2)</span><br><span class="line">println(&quot;多行转单列&quot;)</span><br><span class="line">df2.show()</span><br><span class="line">df2.groupBy(&#x27;col1,&#x27;col2)</span><br><span class="line">.agg(concat_ws(&quot;,&quot;,collect_set(&#x27;col3)).as(&quot;col3&quot;))</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">多行转单列</span><br><span class="line">+----+----+----+</span><br><span class="line">|col1|col2|col3|</span><br><span class="line">+----+----+----+</span><br><span class="line">|   a|   b|   1|</span><br><span class="line">|   a|   b|   2|</span><br><span class="line">|   a|   b|   3|</span><br><span class="line">|   b|   d|   4|</span><br><span class="line">|   b|   d|   5|</span><br><span class="line">|   b|   d|   6|</span><br><span class="line">+----+----+----+</span><br><span class="line"></span><br><span class="line">+----+----+-----+</span><br><span class="line">|col1|col2| col3|</span><br><span class="line">+----+----+-----+</span><br><span class="line">|   a|   b|1,2,3|</span><br><span class="line">|   b|   d|5,6,4|</span><br><span class="line">+----+----+-----+</span><br></pre></td></tr></table></figure>

<h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><h3 id="多列转多行"><a href="#多列转多行" class="headerlink" title="多列转多行"></a>多列转多行</h3><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val rows3 = new util.ArrayList[Row]()</span><br><span class="line">rows3.add(Row(&quot;a&quot;, 1, 2, 3))</span><br><span class="line">rows3.add(Row(&quot;b&quot;, 4, 5, 6))</span><br><span class="line"></span><br><span class="line">val schema3 = StructType(</span><br><span class="line">StructField(&quot;col1&quot;, StringType)::</span><br><span class="line">StructField(&quot;c&quot;, IntegerType)::</span><br><span class="line">StructField(&quot;d&quot;, IntegerType)::</span><br><span class="line">StructField(&quot;e&quot;, IntegerType)::Nil</span><br><span class="line">)</span><br><span class="line">val df3 = spark.createDataFrame(rows3, schema3)</span><br><span class="line"></span><br><span class="line">println(&quot;多列转多行&quot;)</span><br><span class="line">df3.show()</span><br><span class="line">df3.selectExpr(&quot;col1&quot;,&quot;stack(3, &#x27;c&#x27;, c, &#x27;d&#x27;, d, &#x27;e&#x27;, e) as (`col2`,`col3`)&quot;)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">多列转多行</span><br><span class="line">+----+---+---+---+</span><br><span class="line">|col1|  c|  d|  e|</span><br><span class="line">+----+---+---+---+</span><br><span class="line">|   a|  1|  2|  3|</span><br><span class="line">|   b|  4|  5|  6|</span><br><span class="line">+----+---+---+---+</span><br><span class="line"></span><br><span class="line">+----+----+----+</span><br><span class="line">|col1|col2|col3|</span><br><span class="line">+----+----+----+</span><br><span class="line">|   a|   c|   1|</span><br><span class="line">|   a|   d|   2|</span><br><span class="line">|   a|   e|   3|</span><br><span class="line">|   b|   c|   4|</span><br><span class="line">|   b|   d|   5|</span><br><span class="line">|   b|   e|   6|</span><br><span class="line">+----+----+----+</span><br></pre></td></tr></table></figure>

<h3 id="单列转多行"><a href="#单列转多行" class="headerlink" title="单列转多行"></a>单列转多行</h3><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val rows4 = new util.ArrayList[Row]()</span><br><span class="line">rows4.add(Row(&quot;a&quot;, &quot;b&quot;, &quot;1,2,3&quot;))</span><br><span class="line">rows4.add(Row(&quot;c&quot;, &quot;d&quot;, &quot;4,5,6&quot;))</span><br><span class="line"></span><br><span class="line">val schema4 = StructType(</span><br><span class="line">StructField(&quot;col1&quot;, StringType)::</span><br><span class="line">StructField(&quot;col2&quot;, StringType)::</span><br><span class="line">StructField(&quot;col3&quot;, StringType):: Nil</span><br><span class="line">)</span><br><span class="line">val df4 = spark.createDataFrame(rows4, schema4)</span><br><span class="line"></span><br><span class="line">println(&quot;单列转多行&quot;)</span><br><span class="line">df4.show()</span><br><span class="line">df4.select(&#x27;col1,&#x27;col2,explode(split(&#x27;col3,&quot;,&quot;)).as(&quot;col3&quot;)).show()</span><br></pre></td></tr></table></figure>

<h4 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">单列转多行</span><br><span class="line">+----+----+-----+</span><br><span class="line">|col1|col2| col3|</span><br><span class="line">+----+----+-----+</span><br><span class="line">|   a|   b|1,2,3|</span><br><span class="line">|   c|   d|4,5,6|</span><br><span class="line">+----+----+-----+</span><br><span class="line"></span><br><span class="line">+----+----+----+</span><br><span class="line">|col1|col2|col3|</span><br><span class="line">+----+----+----+</span><br><span class="line">|   a|   b|   1|</span><br><span class="line">|   a|   b|   2|</span><br><span class="line">|   a|   b|   3|</span><br><span class="line">|   c|   d|   4|</span><br><span class="line">|   c|   d|   5|</span><br><span class="line">|   c|   d|   6|</span><br><span class="line">+----+----+----+</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/22/Spark-%E6%AF%8F%E4%B8%AA%E7%94%A8%E6%88%B7%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E6%9C%80%E5%A4%A7%E5%A4%A9%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/22/Spark-%E6%AF%8F%E4%B8%AA%E7%94%A8%E6%88%B7%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E6%9C%80%E5%A4%A7%E5%A4%A9%E6%95%B0/" class="post-title-link" itemprop="url">Spark:每个用户连续登录最大天数</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-22 18:33:22 / 修改时间：18:47:52" itemprop="dateCreated datePublished" datetime="2022-02-22T18:33:22+08:00">2022-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>请使用RDD、DF/DS API功能实现每个用户连续登录最大天数。</p>
<p>输出格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user     times   start_date   end_date</span><br><span class="line">user_1    4       2021-08-01  2021-08-04</span><br><span class="line">user_2    3       2021-07-30  2021-08-01</span><br></pre></td></tr></table></figure>

<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">user_1,20210801</span><br><span class="line">user_1,20210802</span><br><span class="line">user_1,20210803</span><br><span class="line">user_1,20210804</span><br><span class="line">user_1,20210806</span><br><span class="line">user_1,20210807</span><br><span class="line">user_1,20210808</span><br><span class="line">user_1,20210811</span><br><span class="line">user_1,20210812</span><br><span class="line">user_2,20210730</span><br><span class="line">user_2,20210731</span><br><span class="line">user_2,20210801</span><br><span class="line">user_2,20210804</span><br><span class="line">user_2,20210806</span><br></pre></td></tr></table></figure>

<h2 id="RDD实现"><a href="#RDD实现" class="headerlink" title="RDD实现"></a>RDD实现</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line"></span><br><span class="line">object spring_job2 &#123;</span><br><span class="line">	def main(args: Array[String]): Unit = &#123;</span><br><span class="line">	</span><br><span class="line">		val input = &quot;data/login.log&quot;</span><br><span class="line"></span><br><span class="line">		val sparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(this.getClass.getCanonicalName)</span><br><span class="line">		val sc = new SparkContext(sparkConf)</span><br><span class="line">		val d = new SimpleDateFormat(&quot;yyyyMMdd&quot;)</span><br><span class="line">		val d2 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)</span><br><span class="line"></span><br><span class="line">		sc.textFile(input).map(_.split(&quot;,&quot;))</span><br><span class="line">			.map(x =&gt;(x(0), x(1)))</span><br><span class="line">			.groupByKey()</span><br><span class="line">			.mapValues(_.zipWithIndex.map(x=&gt; &#123;</span><br><span class="line">				val relative_day = d.format(d.parse(x._1).getTime - x._2 * 1000 * 60 * 60 * 24)</span><br><span class="line">				val end_day = x._1</span><br><span class="line">				(relative_day,end_day)</span><br><span class="line">			&#125;)</span><br><span class="line">				.groupBy(_._1).map(x=&gt;(x._2.size,x._2.map(_._2).min,x._2.map(_._2).max))</span><br><span class="line">			).map(x=&gt;(x._1,x._2.max._1,d2.format(d.parse(x._2.max._2)),d2.format(d.parse(x._2.max._3))))</span><br><span class="line">			.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">		sc.stop()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ol>
<li><p>group by name order by date</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,CompactBuffer(20210730, 20210731, 20210801, 20210804, 20210806))</span><br><span class="line">(user_1,CompactBuffer(20210801, 20210802, 20210803, 20210804, 20210806, 20210807, 20210808, 20210811, 20210812))</span><br></pre></td></tr></table></figure></li>
<li><p>group内zipWithIndex添加字段index:为组内每条记录增加index字段,从0开始，每条+1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,List((20210730,0), (20210731,1), (20210801,2), (20210804,3), (20210806,4)))</span><br><span class="line">(user_1,List((20210801,0), (20210802,1), (20210803,2), (20210804,3), (20210806,4), (20210807,5), (20210808,6), (20210811,7), (20210812,8)))</span><br></pre></td></tr></table></figure></li>
<li><p>添加字段relative_day:相对开始日期，它由（当前日期-index天数）所得，如果是连续登录，此日期相同。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,List((20210730,20210730), (20210730,20210731), (20210730,20210801), (20210801,20210804), (20210802,20210806)))</span><br><span class="line">(user_1,List((20210801,20210801), (20210801,20210802), (20210801,20210803), (20210801,20210804), (20210802,20210806), (20210802,20210807), (20210802,20210808), (20210804,20210811), (20210804,20210812)))</span><br></pre></td></tr></table></figure></li>
<li><p>得到relative_day后，按relative_day分组，找到连续登录的group</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,Map(20210730 -&gt; List((20210730,20210730), (20210730,20210731), (20210730,20210801)), 20210801 -&gt; List((20210801,20210804)), 20210802 -&gt; List((20210802,20210806))))</span><br><span class="line">(user_1,Map(20210801 -&gt; List((20210801,20210801), (20210801,20210802), (20210801,20210803), (20210801,20210804)), 20210802 -&gt; List((20210802,20210806), (20210802,20210807), (20210802,20210808)), 20210804 -&gt; List((20210804,20210811), (20210804,20210812))))</span><br></pre></td></tr></table></figure></li>
<li><p>遍历每一组数据，并输出当前组的连续登录天数（size），开始日期(day字段的min)，结束日期（day字段的max）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,List((3,20210730,20210801), (1,20210804,20210804), (1,20210806,20210806)))</span><br><span class="line">(user_1,List((4,20210801,20210804), (3,20210806,20210808), (2,20210811,20210812)))</span><br></pre></td></tr></table></figure></li>
<li><p>找出连续登录次数最大的一组，并格式化输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(user_2,3,2021-07-30,2021-08-01)</span><br><span class="line">(user_1,4,2021-08-01,2021-08-04)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="DF-DS-API"><a href="#DF-DS-API" class="headerlink" title="DF/DS API"></a>DF/DS API</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;DataFrame, Dataset, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line"></span><br><span class="line">object spring_job2_df &#123;</span><br><span class="line">	def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">		val input = &quot;data/login.log&quot;</span><br><span class="line"></span><br><span class="line">		val spark = SparkSession.builder()</span><br><span class="line">			.master(&quot;local[2]&quot;)</span><br><span class="line">			.appName(this.getClass.getCanonicalName)</span><br><span class="line">			.getOrCreate()</span><br><span class="line"></span><br><span class="line">		val d = new SimpleDateFormat(&quot;yyyyMMdd&quot;)</span><br><span class="line">		val d2 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)</span><br><span class="line">		import spark.implicits._</span><br><span class="line">		import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">		val df = spark.read.textFile(input)</span><br><span class="line"></span><br><span class="line">		val frame = df.map(x=&gt;&#123;</span><br><span class="line">			val arr = x.split(&quot;,&quot;)</span><br><span class="line">			val user = arr(0)</span><br><span class="line">			val date = d2.format(d.parse(arr(1)))</span><br><span class="line">			Info(user,date)</span><br><span class="line">		&#125;).toDF()</span><br><span class="line"></span><br><span class="line">		frame.createOrReplaceTempView(&quot;login&quot;)</span><br><span class="line"></span><br><span class="line">		spark.sql(</span><br><span class="line">			&quot;&quot;&quot;</span><br><span class="line">				|select</span><br><span class="line">				|	t4.user,t4.times,t4.start_date,t4.end_date</span><br><span class="line">				|from</span><br><span class="line">				|	(with</span><br><span class="line">				|		t3</span><br><span class="line">				|		as</span><br><span class="line">				|		(select</span><br><span class="line">				|			t2.user ,count(1) as times,min(t2.date) as start_date,max(t2.date) as end_date</span><br><span class="line">				|		from</span><br><span class="line">				|			(select</span><br><span class="line">				|				t1.user,t1.date, date_add(t1.date,-t1.index) as relative_day</span><br><span class="line">				|			from</span><br><span class="line">				|				(select</span><br><span class="line">				|					user,date,rank() over(partition by user order by date) as index</span><br><span class="line">				|				from login</span><br><span class="line">				|				) t1</span><br><span class="line">				|			) t2</span><br><span class="line">				|			group by t2.user,t2.relative_day</span><br><span class="line">				|		)</span><br><span class="line">				|		select t3.*,rank() over(partition by t3.user order by t3.times desc) rk from t3</span><br><span class="line">				|	)t4</span><br><span class="line">				|where rk =1</span><br><span class="line">				|&quot;&quot;&quot;.stripMargin).show(false)</span><br><span class="line"></span><br><span class="line">		spark.stop()</span><br><span class="line">	&#125;</span><br><span class="line">	case class Info(user:String,date:String)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="表"><a href="#表" class="headerlink" title="表"></a>表</h3><p>t1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------+-----+</span><br><span class="line">|name |date    |index|</span><br><span class="line">+-----+--------+-----+</span><br><span class="line">|pk   |20210801|1    |</span><br><span class="line">|pk   |20210802|2    |</span><br><span class="line">|pk   |20210803|3    |</span><br><span class="line">|pk   |20210804|4    |</span><br><span class="line">|pk   |20210806|5    |</span><br><span class="line">|pk   |20210807|6    |</span><br><span class="line">|pk   |20210808|7    |</span><br><span class="line">|pk   |20210811|8    |</span><br><span class="line">|pk   |20210812|9    |</span><br><span class="line">|ruoze|20210730|1    |</span><br><span class="line">|ruoze|20210731|2    |</span><br><span class="line">|ruoze|20210801|3    |</span><br><span class="line">|ruoze|20210804|4    |</span><br><span class="line">|ruoze|20210806|5    |</span><br><span class="line">+-----+--------+-----+</span><br></pre></td></tr></table></figure>

<p>t2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">+-----+----------+------------+</span><br><span class="line">|name |date      |relative_day|</span><br><span class="line">+-----+----------+------------+</span><br><span class="line">|pk   |2021-08-01|2021-07-31  |</span><br><span class="line">|pk   |2021-08-02|2021-07-31  |</span><br><span class="line">|pk   |2021-08-03|2021-07-31  |</span><br><span class="line">|pk   |2021-08-04|2021-07-31  |</span><br><span class="line">|pk   |2021-08-06|2021-08-01  |</span><br><span class="line">|pk   |2021-08-07|2021-08-01  |</span><br><span class="line">|pk   |2021-08-08|2021-08-01  |</span><br><span class="line">|pk   |2021-08-11|2021-08-03  |</span><br><span class="line">|pk   |2021-08-12|2021-08-03  |</span><br><span class="line">|ruoze|2021-07-30|2021-07-29  |</span><br><span class="line">|ruoze|2021-07-31|2021-07-29  |</span><br><span class="line">|ruoze|2021-08-01|2021-07-29  |</span><br><span class="line">|ruoze|2021-08-04|2021-07-31  |</span><br><span class="line">|ruoze|2021-08-06|2021-08-01  |</span><br><span class="line">+-----+----------+------------+</span><br></pre></td></tr></table></figure>

<p>t3</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------+----------+----------+</span><br><span class="line">|name |count(1)|min(date) |max(date) |</span><br><span class="line">+-----+--------+----------+----------+</span><br><span class="line">|pk   |4       |2021-08-01|2021-08-04|</span><br><span class="line">|pk   |3       |2021-08-06|2021-08-08|</span><br><span class="line">|pk   |2       |2021-08-11|2021-08-12|</span><br><span class="line">|ruoze|3       |2021-07-30|2021-08-01|</span><br><span class="line">|ruoze|1       |2021-08-04|2021-08-04|</span><br><span class="line">|ruoze|1       |2021-08-06|2021-08-06|</span><br><span class="line">+-----+--------+----------+----------+</span><br></pre></td></tr></table></figure>

<p>t4</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-----+-----+----------+----------+---+</span><br><span class="line">|pk   |4    |2021-08-01|2021-08-04|1  |</span><br><span class="line">|pk   |3    |2021-08-06|2021-08-08|2  |</span><br><span class="line">|pk   |2    |2021-08-11|2021-08-12|3  |</span><br><span class="line">|ruoze|3    |2021-07-30|2021-08-01|1  |</span><br><span class="line">|ruoze|1    |2021-08-04|2021-08-04|2  |</span><br><span class="line">|ruoze|1    |2021-08-06|2021-08-06|2  |</span><br><span class="line">+-----+-----+----------+----------+---+</span><br></pre></td></tr></table></figure>

<p>result</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+-----+-----+----------+----------+</span><br><span class="line">|user |times|start_date|end_date  |</span><br><span class="line">+-----+-----+----------+----------+</span><br><span class="line">|pk   |4    |2021-08-01|2021-08-04|</span><br><span class="line">|ruoze|3    |2021-07-30|2021-08-01|</span><br><span class="line">+-----+-----+----------+----------+</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/22/Spark-Compression-codec-com-hadoop-compression-lzo-LzoCodec-not-found/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/22/Spark-Compression-codec-com-hadoop-compression-lzo-LzoCodec-not-found/" class="post-title-link" itemprop="url">Spark:Compression codec com.hadoop.compression.lzo.LzoCodec not found</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-22 12:26:47 / 修改时间：12:30:30" itemprop="dateCreated datePublished" datetime="2022-02-22T12:26:47+08:00">2022-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在安装完Hadoop Lzo后。进入spark-sql shell 正常，但是执行查询语句时候，抛出：</p>
<p>Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>原因：在hadoop中配置了编解码器lzo，所以当使用yarn模式时，spark自身没有lzo的jar包所以无法找到。这是因为在hadoop 的core-site.xml 和mapred-site.xml 中开启了压缩，并且压缩式lzo的。这就导致写入上传到hdfs 的文件自动被压缩为lzo了。而spark没有lzo这个jar包，所以无法被找到。</p>
<p>解决方法有2个：</p>
<p>1.软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s  /home/hadoop/app/hadoop/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar /home/hadoop/app/spark/jars/hadoop-lzo-0.4.21-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>2.配置路径</p>
<p>配置spark-default.conf如下即可：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.jars=/home/hadoop/app/hadoop/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/22/Azkaban-multiple-executor%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/22/Azkaban-multiple-executor%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">Azkaban multiple-executor模式部署</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-22 10:32:00 / 修改时间：11:30:59" itemprop="dateCreated datePublished" datetime="2022-02-22T10:32:00+08:00">2022-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="以Multi-Executor-Server部署Azkaban"><a href="#以Multi-Executor-Server部署Azkaban" class="headerlink" title="以Multi Executor Server部署Azkaban"></a>以Multi Executor Server部署Azkaban</h2><h3 id="mysql准备"><a href="#mysql准备" class="headerlink" title="mysql准备"></a>mysql准备</h3><ol>
<li><p>create database for Azkaban.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure></li>
<li><p>create a mysql user for Azkaban. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE USER &#x27;azkaban&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;azkaban123&#x27;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to &#x27;azkaban&#x27;@&#x27;%&#x27; WITH GRANT OPTION;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure></li>
<li><p>修改mysql配置my.cnf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">...</span><br><span class="line">max_allowed_packet=1024M</span><br></pre></td></tr></table></figure></li>
<li><p>重启mysql</p>
</li>
</ol>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ol>
<li><p>创建安装目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ cd software/</span><br><span class="line">[hadoop@hadoop001 software]$ mkdir azkaban</span><br></pre></td></tr></table></figure></li>
<li><p>在编译成功的目录下获取以下三个需要的tar包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 azkaban-master]$ cp azkaban-web-server/build/distributions/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz ~/software/azkaban</span><br><span class="line">[hadoop@hadoop001 azkaban-master]$ cp azkaban-exec-server/build/distributions/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz ~/software/azkaban </span><br><span class="line">[hadoop@hadoop001 azkaban-master]$ cp azkaban-db/build/distributions/azkaban-db-0.1.0-SNAPSHOT.tar.gz ~/software/azkaban</span><br></pre></td></tr></table></figure></li>
<li><p>解压并重命名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 azkaban]$ ll</span><br><span class="line">total 119840</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop     8864 Feb 16 13:30 azkaban-db-0.1.0-SNAPSHOT.tar.gz</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop 64787133 Feb 16 09:24 azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop 57896671 Feb 16 09:25 azkaban-web-server-0.1.0-SNAPSHOT.tar.gz</span><br><span class="line">drwxr-xr-x.  2 hadoop hadoop     4096 Jan 25 17:53 db</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop     4096 Feb 16 17:28 exec</span><br><span class="line">drwxr-xr-x.  8 hadoop hadoop     4096 Feb 16 17:28 web</span><br><span class="line">[hadoop@hadoop001 azkaban]$ tar -xzvf azkaban-db-0.1.0-SNAPSHOT .tar.gz</span><br><span class="line">[hadoop@hadoop001 azkaban]$ mv azkaban-db-0.1.0-SNAPSHOT db</span><br><span class="line">[hadoop@hadoop001 azkaban]$ tar -xzvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz</span><br><span class="line">[hadoop@hadoop001 azkaban]$ mv azkaban-exec-server-0.1.0-SNAPSHOT exec</span><br><span class="line">[hadoop@hadoop001 azkaban]$ tar -xzvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz</span><br><span class="line">[hadoop@hadoop001 azkaban]$ mv azkaban-web-server-0.1.0-SNAPSHOT web</span><br></pre></td></tr></table></figure></li>
<li><p>创建软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 azkaban]$ ln -s /home/hadoop/software/azkaban /home/hadoop/app/azkaban</span><br></pre></td></tr></table></figure></li>
<li><p>mysql脚本导入</p>
<p>导入sql脚本,批量创建表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; source /home/hadoop/software/azkaban/db/create-all-sql-0.1.0-SNAPSHOT.sql</span><br></pre></td></tr></table></figure></li>
<li><p>Installing Azkaban Executor Server</p>
<p>修改exec目录下<code>conf/azkaban.properties</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Mysql Configs</span><br><span class="line">mysql.user=&lt;username&gt;</span><br><span class="line">mysql.password=&lt;password&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/2022/02/22/Azkaban-multiple-executor%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/2638668-45e4ac37cec07e31.png" alt="img"></p>
<p>在azkaban-web-server中的azkaban.properties添加：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#Multiple Executor</span><br><span class="line">azkaban.use.multiple.executors=true</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span><br><span class="line">azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1</span><br><span class="line">azkaban.executorselector.comparator.Memory=1</span><br><span class="line">azkaban.executorselector.comparator.LastDispatched=1</span><br><span class="line">azkaban.executorselector.comparator.CpuUsage=1</span><br></pre></td></tr></table></figure>

<p>启动Executor Server：在下面的步骤统一启动。注意，在Multi Executor Server模式下启动了Executor Server后，需要手动激活其状态。</p>
</li>
<li><p>Installing Azkaban Web Server</p>
<p>修改web目录下<code>conf/azkaban.properties</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Mysql Configs</span><br><span class="line">mysql.user=&lt;username&gt;</span><br><span class="line">mysql.password=&lt;password&gt;</span><br></pre></td></tr></table></figure>

<p>添加用户，修改<code>conf/azkaban-users.xml</code>,如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;user password=&quot;admin&quot; roles=&quot;admin&quot; username=&quot;admin&quot;/&gt;                 </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>因为配置文件下的路径是使用相对路径，所以启动需要进入到其目录下调用命令，具体如下：。</p>
<p>【注意】需要先启动并激活Executor,才能启动web成功。</p>
<p>启动Executor Server，并激活。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 exec]$ pwd</span><br><span class="line">/home/hadoop/software/azkaban/exec</span><br><span class="line">[hadoop@hadoop001 exec]$ ./bin/start-exec.sh </span><br><span class="line">[hadoop@hadoop001 exec]$ curl -G &quot;localhost:$(&lt;./executor.port)/executor?action=activate&quot;</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;[hadoop@hadoop001 exec]$ </span><br></pre></td></tr></table></figure>

<p>启动Web Server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 web]$ pwd</span><br><span class="line">/home/hadoop/software/azkaban/web</span><br><span class="line">[hadoop@hadoop001 exec]$ ./bin/start-web.sh </span><br></pre></td></tr></table></figure>

<p>查看进程，访问UI页面</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 web]$ jps</span><br><span class="line">9989 Jps</span><br><span class="line">9868 AzkabanExecutorServer</span><br><span class="line">9965 AzkabanWebServer</span><br></pre></td></tr></table></figure>

<p>Web UI:<a target="_blank" rel="noopener" href="http://hadoop001:8081/">http://hadoop001:8081/</a></p>
<h2 id="azkaban-project案例"><a href="#azkaban-project案例" class="headerlink" title="azkaban project案例"></a>azkaban project案例</h2><h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><ul>
<li><p>flow20.project</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure></li>
<li><p>spring.flow</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: hello</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo &quot;hello world&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>把以上两个文件夹放到同一个文件夹下，并压缩为zip文件，上传到project上执行即可。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><ol>
<li><p>更换日志文件目录</p>
<p>修改 azkaban-web-server/conf/log4j.properties</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 默认为INFO, Console 需要修改</span><br><span class="line">log4j.rootLogger=INFO, server</span><br><span class="line">log4j.logger.azkaban=INFO, server</span><br><span class="line">log4j.appender.server=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.server.layout=org.apache.log4j.PatternLayout</span><br><span class="line"># 修改为绝对路径</span><br><span class="line">log4j.appender.server.File=/home/hadoop/log/azkaban/azkaban-webserver.log</span><br><span class="line">log4j.appender.server.layout.ConversionPattern=%d&#123;yyyy/MM/dd HH:mm:ss.SSS Z&#125; %5p [%c&#123;1&#125;] [%t] [Azkaban] %m%n</span><br><span class="line"># 修改为1024MB，默认为102400MB，显然是不合理的</span><br><span class="line">log4j.appender.server.MaxFileSize=1024MB</span><br><span class="line">log4j.appender.server.MaxBackupIndex=2</span><br><span class="line">log4j.appender.Console=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.Console.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.Console.layout.ConversionPattern=%d&#123;yyyy/MM/dd HH:mm:ss.SSS Z&#125; %5p [%c&#123;1&#125;] [%t] [Azkaban] %m%n</span><br></pre></td></tr></table></figure></li>
<li><p>web-server和exec-server启停脚本的修改</p>
<p>优化点：</p>
<ul>
<li><p>因为启动web-server和exec-server，在哪个目录下执行启动服务，就会生成一个<code>.out</code>日志文件。由于已经更改日志文件存储目录，于是修改azkaban-web-server的<code>bin/start-web.sh</code>和azkaban-exec-server的<code>bin/start-exec.sh</code></p>
</li>
<li><p>exec-server的临时文件</p>
<p>启动exec-server，在哪个目录下执行启动服务，就会生成<code>executions</code>目录、<code>temp</code>目录和<code>executor.port</code>文件这些临时的目录或文件，然后停止服务后，也不会删除这些临时目录或文件，于是:</p>
<p>在<code>azkaban/azkaban-exec-server</code>目录下新建<code>tmp</code>目录用于存放这些临时目录或文件；</p>
</li>
<li><p>激活executor（修改后启动后自动激活）</p>
</li>
</ul>
<p>修改后脚本：</p>
<p>start-web.sh：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">script_dir=$(dirname $0)</span><br><span class="line"></span><br><span class="line">#$&#123;script_dir&#125;/internal/internal-start-web.sh &gt;webServerLog_`date +%F+%T`.out 2&gt;&amp;1 &amp;</span><br><span class="line">$&#123;script_dir&#125;/internal/internal-start-web.sh &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>start-exec.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">#script_dir=$(dirname $0)</span><br><span class="line">script_dir=/home/hadoop/app/azkaban/exec/tmp</span><br><span class="line">cd $&#123;script_dir&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># pass along command line arguments to the internal launch script.</span><br><span class="line">#$&#123;script_dir&#125;/internal/internal-start-executor.sh &quot;$@&quot; &gt;executorServerLog__`date +%F+%T`.out 2&gt;&amp;1 &amp;</span><br><span class="line">../bin/internal/internal-start-executor.sh &quot;$@&quot; &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># 这里休眠5s是为了给exec-server启动后提供一些准备时间</span><br><span class="line">sleep 5s</span><br><span class="line"># 然后再激活executor</span><br><span class="line">curl -G &quot;hadoop001:$(&lt;$&#123;script_dir&#125;/executor.port)/executor?action=activate&quot;</span><br><span class="line">~       </span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><ol>
<li><p>Azkaban 部署完成后 执行 job 一直处于 Preparing 状态</p>
<p>主要原因：没有可运行的executor</p>
<p>可能：1.没激活；2.激活了但被过滤掉不可用。</p>
<p>azkaban默认情况下在开始运行job时会检测系统的内存，其最低要求的内存是3G，若系统内存不足3G，便会出现运行的job一直卡在那不动。</p>
<p>修改 <code>azkaban-web-server/conf/azkaban.properties</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#MinimumFreeMemory 过滤器会检查 executor 主机空余内存是否会大于 3G，如果不足 3G，则 web-server 不会将任务交由该主机执行</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br></pre></td></tr></table></figure></li>
<li><p>Azkaban异常：Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job XX, sleep for 60 secs and retry, attempt 1 of 72</p>
<p>azkaban默认情况下在开始运行job时会检测系统的内存，其最低要求的内存是3G，若系统内存不足3G，便会出现运行的job一直卡在那不动。</p>
<p>解决方法：</p>
<ol>
<li><p>增加系统内存</p>
</li>
<li><p>关闭检测内存的选项。<br>具体办法为，在<code>azkaban/azkaban-exec-server/plugins/jobtypes/</code>目录下的<code>commonprivate.properties</code>的文件中添加一下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>jobtypes错误解决</p>
<p>在executor的根目录下创建<code>plugins\jobtypes\commonprivate.properties</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># set execute-as-user</span><br><span class="line">execute.as.user=false</span><br><span class="line">azkaban.native.lib=false</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/07/Hive%EF%BC%9A%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/07/Hive%EF%BC%9A%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/" class="post-title-link" itemprop="url">Hive：行列转换</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-07 01:46:26 / 修改时间：11:06:24" itemprop="dateCreated datePublished" datetime="2022-02-07T01:46:26+08:00">2022-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><h3 id="多行转多列"><a href="#多行转多列" class="headerlink" title="多行转多列"></a>多行转多列</h3><p>假设数据表row2col：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">col1   col2    col3</span><br><span class="line">a      c       1</span><br><span class="line">a      d       2</span><br><span class="line">a      e       3  </span><br><span class="line">b      c       4</span><br><span class="line">b      d       5</span><br><span class="line">b      e       6</span><br></pre></td></tr></table></figure>

<p>现在要将其转化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">col1   c      d      e</span><br><span class="line">a      1      2      3</span><br><span class="line">b      4      5      6</span><br></pre></td></tr></table></figure>

<p>此时需要使用到max(case … when … then … else 0 end)，仅限于转化的字段为数值类型，且为正值的情况。</p>
<p>HQL语句为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select col1,</span><br><span class="line">max(case col2 when &#x27;c&#x27; then col3 else 0 end) as c,</span><br><span class="line">max(case col2 when &#x27;d&#x27; then col3 else 0 end) as d,</span><br><span class="line">max(case col2 when &#x27;e&#x27; then col3 else 0 end) as e</span><br><span class="line">from row2col</span><br><span class="line">group by col1;</span><br></pre></td></tr></table></figure>

<h3 id="多行转单列"><a href="#多行转单列" class="headerlink" title="多行转单列"></a>多行转单列</h3><p>假设数据表row2col：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">col1    col2    col3</span><br><span class="line">a       b       1</span><br><span class="line">a       b       2</span><br><span class="line">a       b       3</span><br><span class="line">c       d       4</span><br><span class="line">c       d       5</span><br><span class="line">c       d       6</span><br></pre></td></tr></table></figure>

<p>现在要将其转化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">col1    col2    col3</span><br><span class="line">a       b       1,2,3</span><br><span class="line">c       d       4,5,6</span><br></pre></td></tr></table></figure>

<p>此时需要用到内置的UDF：</p>
<ol>
<li>concat_ws(separator, str1, str2, …)：把多个字符串用分隔符进行拼接</li>
<li>collect_set()：把列聚合成为数据，去重</li>
<li>collect_list()：把列聚合成为数组，不去重</li>
</ol>
<p>HQL语句为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select col1, col2, concat_ws(&#x27;,&#x27;, collect_set(col3)) as col3</span><br><span class="line">from row2col</span><br><span class="line">group by col1, col2;</span><br></pre></td></tr></table></figure>

<p>注意：由于使用concat_ws()函数，collect_set()中的字段必须为string类型，如果是其他类型可使用cast(col3 as string)将其转换为string类型。</p>
<h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><h3 id="多列转多行"><a href="#多列转多行" class="headerlink" title="多列转多行"></a>多列转多行</h3><p>假设有数据表col2row：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">col1   c      d      e</span><br><span class="line">a      1      2      3</span><br><span class="line">b      4      5      6</span><br></pre></td></tr></table></figure>

<p>现要将其转化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">col1   col2    col3</span><br><span class="line">a      c       1</span><br><span class="line">a      d       2</span><br><span class="line">a      e       3</span><br><span class="line">b      c       4</span><br><span class="line">b      d       5</span><br><span class="line">b      e       6</span><br></pre></td></tr></table></figure>

<p>这里需要使用union进行拼接。</p>
<p>HQL语句为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select col1, &#x27;c&#x27; as col2, c as col3 from col2row</span><br><span class="line">UNION ALL</span><br><span class="line">select col1, &#x27;d&#x27; as col2, d as col3 from col2row</span><br><span class="line">UNION ALL</span><br><span class="line">select col1, &#x27;e&#x27; as col2, e as col3 from col2row</span><br><span class="line">order by col1, col2;</span><br></pre></td></tr></table></figure>

<h3 id="单列转多行"><a href="#单列转多行" class="headerlink" title="单列转多行"></a>单列转多行</h3><p>假设有数据表col2row：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">col1    col2    col3</span><br><span class="line">a       b       1,2,3</span><br><span class="line">c       d       4,5,6</span><br></pre></td></tr></table></figure>

<p>现要将其转化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">col1    col2    col3</span><br><span class="line">a       b       1</span><br><span class="line">a       b       2</span><br><span class="line">a       b       3</span><br><span class="line">c       d       4</span><br><span class="line">c       d       5</span><br><span class="line">c       d       6</span><br></pre></td></tr></table></figure>

<p>这里需要使用UDTF（表生成函数）explode()，该函数接受array类型的参数，其作用恰好与collect_set相反，实现将array类型数据行转列。explode配合lateral view实现将某列数据拆分成多行。</p>
<p>HQL语句为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select col1, col2, lv.col3 as col3</span><br><span class="line">from col2row </span><br><span class="line">lateral view explode(split(col3, &#x27;,&#x27;)) lv as col3;</span><br></pre></td></tr></table></figure>



<p>下面看下行转列使用的函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lateral view explode(split表达式) tableName as columnName</span><br></pre></td></tr></table></figure>

<ul>
<li>tableName 表示虚拟表的名称。</li>
<li>columnName 表示虚拟表的虚拟字段名称，如果分裂之后有一个列，则写一个即可；如果分裂之后有多个列，按照列的顺序在括号中声明所有虚拟列名，以逗号隔开。</li>
</ul>
<p><strong>explode 函数</strong>：处理数组结构的字段，将数组转换成多行。</p>
<p><strong>Lateral View</strong>：其实explode是一个UDTF函数（一行输入多行输出），这个时候如果要select除了explode得到的字段以外的多个字段，需要创建虚拟表</p>
<blockquote>
<p>Lateral View 用于<strong>和UDTF函数【explode,split】结合来使用</strong>。<br>首先通过 UDTF 函数将数据拆分成多行，再将多行结果组合成一个支持别名的虚拟表。<br>主要解决在 select 使用UDTF做查询的过程中查询只能包含单个UDTF，不能包含其它字段以及多个UDTF的情况。<br>语法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias)</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/02/07/%E6%8B%89%E9%93%BE%E8%A1%A8%EF%BC%88%E5%8E%9F%E7%90%86%E3%80%81%E8%AE%BE%E8%AE%A1%E4%BB%A5%E5%8F%8A%E5%9C%A8Hive%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/07/%E6%8B%89%E9%93%BE%E8%A1%A8%EF%BC%88%E5%8E%9F%E7%90%86%E3%80%81%E8%AE%BE%E8%AE%A1%E4%BB%A5%E5%8F%8A%E5%9C%A8Hive%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%89/" class="post-title-link" itemprop="url">拉链表（原理、设计以及在Hive中的实现）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-07 00:28:24 / 修改时间：01:25:23" itemprop="dateCreated datePublished" datetime="2022-02-07T00:28:24+08:00">2022-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是拉链表"><a href="#什么是拉链表" class="headerlink" title="什么是拉链表"></a>什么是拉链表</h2><p>拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有变化的信息。</p>
<ul>
<li>记录一个事物<strong>从开始，一直到当前状态</strong>的所有变化的信息。</li>
<li>我们可以使用这张表拿到最新的当天的<strong>最新数据</strong>以及<strong>之前的历史数据</strong>。</li>
<li>既能满足反应数据的历史状态，又可以最大限度地节省存储空间</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><ol>
<li>数据量比较大;</li>
<li>表中的部分字段会被update,如用户的地址，产品的描述信息，订单的状态等等;</li>
<li>需要查看某一个时间点或者时间段的历史快照信息，比如，查看某一个订单在历史某一个时间点的状态，<br>比如，查看某一个用户在过去某一段时间内，更新过几次等等;</li>
<li>变化的比例和频率不是很大，比如，总共有1000万的会员，每天新增和发生变化的有10万左右;</li>
<li>如果对这边表每天都保留一份全量，那么每次全量中会保存很多不变的信息，对存储是极大的浪费;</li>
</ol>
<p>这些场景下使用拉链历史表，既可以反映数据的历史状态，又可以最大程度的节省存储。</p>
<blockquote>
<p>拉链表：记录一个事物从开始，一直到当前状态的所有变化的信息。</p>
<p>全量表：保存用户所有的数据（包括新增与历史数据）</p>
<p>增量表：只保留当前新增的数据</p>
<p>快照表：按日分区，记录截止数据日期的全量数据</p>
<p>切片表：切片表根据基础表，往往只反映某一个维度的相应数据。其表结构与基础表结构相同，但数据往往只有某一维度，或者某一个事实条件的数据</p>
</blockquote>
<h2 id="设计和实现"><a href="#设计和实现" class="headerlink" title="设计和实现"></a>设计和实现</h2><p>举例说明，用用户的拉链表来说明。</p>
<p>在2017-01-01这一天表中的数据是：</p>
<table>
<thead>
<tr>
<th align="left">注册日期</th>
<th align="left">用户编号</th>
<th align="left">手机号码</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2017-01-01</td>
<td align="left">001</td>
<td align="left">111111</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">002</td>
<td align="left">222222</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">003</td>
<td align="left">333333</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">444444</td>
</tr>
</tbody></table>
<p>在2017-01-02这一天表中的数据是， 用户002和004资料进行了修改，005是新增用户：</p>
<table>
<thead>
<tr>
<th align="left">注册日期</th>
<th align="left">用户编号</th>
<th align="left">手机号码</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2017-01-01</td>
<td align="left">001</td>
<td align="left">111111</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">002</td>
<td align="left">233333</td>
<td align="left">（由222222变成233333）</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">003</td>
<td align="left">333333</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">432432</td>
<td align="left">（由444444变成432432）</td>
</tr>
<tr>
<td align="left">2017-01-02</td>
<td align="left">005</td>
<td align="left">555555</td>
<td align="left">（2017-01-02新增）</td>
</tr>
</tbody></table>
<p>在2017-01-03这一天表中的数据是， 用户004和005资料进行了修改，006是新增用户：</p>
<table>
<thead>
<tr>
<th align="left">注册日期</th>
<th align="left">用户编号</th>
<th align="left">手机号码</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2017-01-01</td>
<td align="left">001</td>
<td align="left">111111</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">002</td>
<td align="left">233333</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">003</td>
<td align="left">333333</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">654321</td>
<td align="left">（由432432变成654321）</td>
</tr>
<tr>
<td align="left">2017-01-02</td>
<td align="left">005</td>
<td align="left">115115</td>
<td align="left">（由555555变成115115）</td>
</tr>
<tr>
<td align="left">2017-01-03</td>
<td align="left">006</td>
<td align="left">666666</td>
<td align="left">（2017-01-03新增）</td>
</tr>
</tbody></table>
<p>如果在数据仓库中设计成历史拉链表保存该表，则会有下面这样一张表，这是最新一天（即2017-01-03）的数据：</p>
<table>
<thead>
<tr>
<th align="left">注册日期</th>
<th align="left">用户编号</th>
<th align="left">手机号码</th>
<th align="left">t_start_date</th>
<th align="left">t_end_date</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2017-01-01</td>
<td align="left">001</td>
<td align="left">111111</td>
<td align="left">2017-01-01</td>
<td align="left">9999-12-31</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">002</td>
<td align="left">222222</td>
<td align="left">2017-01-01</td>
<td align="left">2017-01-01</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">002</td>
<td align="left">233333</td>
<td align="left">2017-01-02</td>
<td align="left">9999-12-31</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">003</td>
<td align="left">333333</td>
<td align="left">2017-01-01</td>
<td align="left">9999-12-31</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">444444</td>
<td align="left">2017-01-01</td>
<td align="left">2017-01-01</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">432432</td>
<td align="left">2017-01-02</td>
<td align="left">2017-01-02</td>
</tr>
<tr>
<td align="left">2017-01-01</td>
<td align="left">004</td>
<td align="left">654321</td>
<td align="left">2017-01-03</td>
<td align="left">9999-12-31</td>
</tr>
<tr>
<td align="left">2017-01-02</td>
<td align="left">005</td>
<td align="left">555555</td>
<td align="left">2017-01-02</td>
<td align="left">2017-01-02</td>
</tr>
<tr>
<td align="left">2017-01-02</td>
<td align="left">005</td>
<td align="left">115115</td>
<td align="left">2017-01-03</td>
<td align="left">9999-12-31</td>
</tr>
<tr>
<td align="left">2017-01-03</td>
<td align="left">006</td>
<td align="left">666666</td>
<td align="left">2017-01-03</td>
<td align="left">9999-12-31</td>
</tr>
</tbody></table>
<p>说明：</p>
<ul>
<li>t_start_date表示该条记录的生命周期开始时间，t_end_date表示该条记录的生命周期结束时间。</li>
<li>t_end_date = ‘9999-12-31’表示该条记录目前处于有效状态。</li>
<li>如果查询当前所有有效的记录，则<code>select * from user where t_end_date = &#39;9999-12-31&#39;</code>。</li>
<li>如果查询2017-01-02的历史快照，则<code>select * from user where t_start_date &lt;= &#39;2017-01-02&#39; and t_end_date &gt;= &#39;2017-01-02&#39;</code>。（此处要好好理解，是拉链表比较重要的一块。）</li>
</ul>
<h2 id="在Hive中实现拉链表"><a href="#在Hive中实现拉链表" class="headerlink" title="在Hive中实现拉链表"></a>在Hive中实现拉链表</h2><p>在现在的大数据场景下，大部分的公司都会选择以Hdfs和Hive为主的数据仓库架构。目前的Hdfs版本来讲，其文件系统中的文件是不能做改变的，也就是说Hive的表智能进行删除和添加操作，而不能进行update。基于这个前提，我们来实现拉链表。</p>
<p>还是以上面的用户表为例，我们要实现用户的拉链表。在实现它之前，我们需要先确定一下我们有哪些数据源可以用。</p>
<ol>
<li>我们需要一张ODS层的用户全量表。至少需要用它来初始化。</li>
<li>每日的用户更新表。</li>
</ol>
<p>而且我们要确定拉链表的时间粒度，比如说拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态，这种天粒度的表其实已经能解决大部分的问题了。<br>另外，补充一下每日的用户更新表该怎么获取，据笔者的经验，有3种方式拿到或者间接拿到每日的用户增量，因为它比较重要，所以详细说明：</p>
<ol>
<li>我们可以监听Mysql数据的变化，比如说用Canal，最后合并每日的变化，获取到最后的一个状态。</li>
<li>假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5，这样就ok了。</li>
<li>流水表！有每日的变更流水表。</li>
</ol>
<h3 id="ods层的user表"><a href="#ods层的user表" class="headerlink" title="ods层的user表"></a>ods层的user表</h3><p>现在我们来看一下我们ods层的用户资料切片表的结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE ods.user (</span><br><span class="line">  user_num STRING COMMENT &#x27;用户编号&#x27;,</span><br><span class="line">  mobile STRING COMMENT &#x27;手机号码&#x27;,</span><br><span class="line">  reg_date STRING COMMENT &#x27;注册日期&#x27;</span><br><span class="line">COMMENT &#x27;用户资料表&#x27;</span><br><span class="line">PARTITIONED BY (dt string)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; LINES TERMINATED BY &#x27;\n&#x27;</span><br><span class="line">STORED AS ORC</span><br><span class="line">LOCATION &#x27;/ods/user&#x27;;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="ods层的user-update表"><a href="#ods层的user-update表" class="headerlink" title="ods层的user_update表"></a>ods层的user_update表</h3><p>然后我们还需要一张用户每日更新表，前面已经分析过该如果得到这张表，现在我们假设它已经存在。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE ods.user_update (</span><br><span class="line">  user_num STRING COMMENT &#x27;用户编号&#x27;,</span><br><span class="line">  mobile STRING COMMENT &#x27;手机号码&#x27;,</span><br><span class="line">  reg_date STRING COMMENT &#x27;注册日期&#x27;</span><br><span class="line">COMMENT &#x27;每日用户资料更新表&#x27;</span><br><span class="line">PARTITIONED BY (dt string)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; LINES TERMINATED BY &#x27;\n&#x27;</span><br><span class="line">STORED AS ORC</span><br><span class="line">LOCATION &#x27;/ods/user_update&#x27;;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE dws.user_his (</span><br><span class="line">  user_num STRING COMMENT &#x27;用户编号&#x27;,</span><br><span class="line">  mobile STRING COMMENT &#x27;手机号码&#x27;,</span><br><span class="line">  reg_date STRING COMMENT &#x27;用户编号&#x27;,</span><br><span class="line">  t_start_date ,</span><br><span class="line">  t_end_date</span><br><span class="line">COMMENT &#x27;用户资料拉链表&#x27;</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; LINES TERMINATED BY &#x27;\n&#x27;</span><br><span class="line">STORED AS ORC</span><br><span class="line">LOCATION &#x27;/dws/user_his&#x27;;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="实现sql语句"><a href="#实现sql语句" class="headerlink" title="实现sql语句"></a>实现sql语句</h3><ul>
<li>然后初始化的sql就不写了，其实就相当于是拿一天的ods层用户表过来就行，我们写一下每日的更新语句。</li>
<li>现在我们假设我们已经已经初始化了2017-01-01的日期，然后需要更新2017-01-02那一天的数据，我们有了下面的Sql。</li>
<li>然后把两个日期设置为变量就可以了。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE dws.user_his</span><br><span class="line">SELECT * FROM</span><br><span class="line">(</span><br><span class="line">    SELECT A.user_num,</span><br><span class="line">           A.mobile,</span><br><span class="line">           A.reg_date,</span><br><span class="line">           A.t_start_time,</span><br><span class="line">           CASE</span><br><span class="line">                WHEN A.t_end_time = &#x27;9999-12-31&#x27; AND B.user_num IS NOT NULL THEN &#x27;2017-01-01&#x27;</span><br><span class="line">                ELSE A.t_end_time</span><br><span class="line">           END AS t_end_time</span><br><span class="line">    FROM dws.user_his AS A</span><br><span class="line">    LEFT JOIN ods.user_update AS B</span><br><span class="line">    ON A.user_num = B.user_num</span><br><span class="line">UNION</span><br><span class="line">    SELECT C.user_num,</span><br><span class="line">           C.mobile,</span><br><span class="line">           C.reg_date,</span><br><span class="line">           &#x27;2017-01-02&#x27; AS t_start_time,</span><br><span class="line">           &#x27;9999-12-31&#x27; AS t_end_time</span><br><span class="line">    FROM ods.user_update AS C</span><br><span class="line">) AS T</span><br></pre></td></tr></table></figure>

<ul>
<li>如感兴趣可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46893497/article/details/113965328">https://blog.csdn.net/qq_46893497/article/details/113965328</a></li>
</ul>
<h2 id="拉链表和流水表"><a href="#拉链表和流水表" class="headerlink" title="拉链表和流水表"></a>拉链表和流水表</h2><ul>
<li>流水表存放的是一个用户的变更记录，比如在一张流水表中，一天的数据中，会存放一个用户的每条修改记录，但是在拉链表中只有一条记录。</li>
<li>这是拉链表设计时需要注意的一个粒度问题。我们当然也可以设置的粒度更小一些，一般按天就足够。</li>
</ul>
<h2 id="查询性能"><a href="#查询性能" class="headerlink" title="查询性能"></a>查询性能</h2><p>拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，个人认为两个思路来解决：</p>
<ul>
<li>在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。</li>
<li>保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。</li>
</ul>
<h2 id="拉链表回滚"><a href="#拉链表回滚" class="headerlink" title="拉链表回滚"></a>拉链表回滚</h2><ul>
<li>修正拉链表回滚问题本质就是：<ul>
<li>就是找到历史的快照。</li>
</ul>
</li>
<li>历史的快照可以根据起始更新时间，那你就找endtime小于你出错的数据就行了，出错日期的数据就行了。</li>
<li>重新导入数据，将原始拉链表数据过滤到指定日期之前即可。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">举例：</span><br><span class="line">拉链表dwd_userinfo_db,目前时间是2020-12-15，想回滚到2020-11-27,那么拉链表的状态得是2020-11-26</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">userid		starttime		endtime</span><br><span class="line">1			2020-11-12		2020-11-26</span><br><span class="line">1			2020-11-27		9999-12-31</span><br><span class="line">2			2020-11-16		2020-12-13</span><br><span class="line">2			2020-12-14		9999-12-31</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">拉链表回滚：过滤starttime&lt;=2020-11-26的数据，将endtime&gt;=2020-11-26的修改为9999-12-31</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_userinfo_db</span><br><span class="line">select</span><br><span class="line">	userid,</span><br><span class="line">	starttime,</span><br><span class="line">	if(endtime&gt;=2020-11-26,&#x27;9999-12-31&#x27;,endtime)</span><br><span class="line">from dwd_userinfo_db</span><br><span class="line">where starttime&lt;=2020-11-26</span><br></pre></td></tr></table></figure>




<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>拉链表不存储冗余的数据，只有某行的数据发生变化，才需要保存下来，相比每次全量同步会节省存储空间</li>
<li>能够查询到历史快照</li>
<li>额外的增加了两列（dw_start_date dw_end_date），为数据行的生命周期</li>
<li>使用拉链表的时候可以不加t_end_date，即失效日期，但是加上之后，能优化很多查询。</li>
<li>可以加上当前行状态标识，能快速定位到当前状态。</li>
<li>在拉链表的设计中可以加一些内容，因为我们每天保存一个状态，如果我们在这个状态里面加一个字段，比如如当天修改次数，那么拉链表的作用就会更大。</li>
</ul>
<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46893497/article/details/110787881">https://blog.csdn.net/qq_46893497/article/details/110787881</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/01/30/Hive%EF%BC%9A%E5%88%86%E5%8C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/30/Hive%EF%BC%9A%E5%88%86%E5%8C%BA/" class="post-title-link" itemprop="url">Hive：分区</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-30 17:21:12" itemprop="dateCreated datePublished" datetime="2022-01-30T17:21:12+08:00">2022-01-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-02-06 21:29:28" itemprop="dateModified" datetime="2022-02-06T21:29:28+08:00">2022-02-06</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>Hive分区的概念与传统关系型数据库分区不同。</p>
<p>传统数据库的分区方式：就oracle而言，分区独立存在于段里，里面存储真实的数据，在数据进行插入的时候自动分配分区。</p>
<p>Hive的分区方式：由于Hive实际是存储在HDFS上的抽象，Hive的一个分区名对应一个目录名，子分区名就是子目录名，并不是一个实际字段。</p>
</blockquote>
<h1 id="静态分区"><a href="#静态分区" class="headerlink" title="静态分区"></a>静态分区</h1><h2 id="一级分区"><a href="#一级分区" class="headerlink" title="一级分区"></a>一级分区</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>Hive分区是在创建表的时候用Partitioned by 关键字定义的，但要注意，Partitioned by子句中定义的列是表中正式的列，但是Hive下的数据文件中并不包含这些列，因为它们是目录名。注意：分区字段不能和表中的字段重复。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">一级分区：一个目录</span><br><span class="line">多级分区：多个目录</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE `emp_partition`(</span><br><span class="line">`empno` int, </span><br><span class="line">`ename` string, </span><br><span class="line">`job` string, </span><br><span class="line">`mgr` int, </span><br><span class="line">`hiredate` string, </span><br><span class="line">`sal` double, </span><br><span class="line">`comm` double</span><br><span class="line">) partitioned by (deptno string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>通过desc查看的表结构如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; desc emp_partition;</span><br><span class="line">OK</span><br><span class="line">col_name	data_type	comment</span><br><span class="line">empno               	int</span><br><span class="line">ename               	string</span><br><span class="line">job                 	string</span><br><span class="line">mgr                 	int</span><br><span class="line">hiredate            	string</span><br><span class="line">sal                 	double</span><br><span class="line">comm                	double</span><br><span class="line">deptno              	string</span><br><span class="line">	 	 </span><br><span class="line"># Partition Information	 	 </span><br><span class="line"># col_name            	data_type           	comment             </span><br><span class="line">deptno              	string              	                    </span><br><span class="line">Time taken: 0.546 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure>

<h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式1:</span><br><span class="line">INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row …];</span><br><span class="line">格式2：</span><br><span class="line">load data local inpath &#x27;/home/hadoop/data/emp_10.txt&#x27; into table emp_partition partition (deptno=10);</span><br></pre></td></tr></table></figure>

<p>从其他表中插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; select * from emp;</span><br><span class="line">OK</span><br><span class="line">emp.empno	emp.ename	emp.job	emp.mgr	emp.hiredate	emp.sal	emp.comm	emp.deptno</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.0	NULL	20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.0	300.0	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.0	500.0	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.0	NULL	20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.0	1400.0	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.0	NULL	30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	NULL	10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.0	NULL	20</span><br><span class="line">7839	KING	PRESIDENT	NULL	1981-11-17	5000.0	NULL	10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.0	0.0	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.0	NULL	20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.0	NULL	30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.0	NULL	20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	NULL	10</span><br><span class="line">Time taken: 0.269 seconds, Fetched: 14 row(s)</span><br><span class="line">hive (hive)&gt; insert into emp_partition partition(deptno=30) select empno,ename,job,mgr,hiredate,sal,comm from emp where deptno=10;</span><br></pre></td></tr></table></figure>

<p>从文件加载数据到表中（不包含分区列字段）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ cat emp_10.txt </span><br><span class="line">88	KK	SALESMAN	8888	1998-10-14	5000	500</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; load data local inpath &#x27;/home/hadoop/data/emp_10.txt&#x27; into table emp_partition partition (deptno=10);</span><br></pre></td></tr></table></figure>

<h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><p>利用分区表查询：(一般分区表都是利用where语句查询的)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; select * from emp_partition where deptno=10;</span><br><span class="line">OK</span><br><span class="line">emp_partition.empno	emp_partition.ename	emp_partition.job	emp_partition.mgr	emp_partition.hiredate	emp_partition.sal	emp_partition.comm	emp_partition.deptno</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	NULL	10</span><br><span class="line">7839	KING	PRESIDENT	NULL	1981-11-17	5000.0	NULL	10</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	NULL	10</span><br><span class="line">88	KK	SALESMAN	8888	1998-10-14	5000.0	500.0	10</span><br><span class="line">Time taken: 0.25 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure>

<p>查看hdfs上emp_partition表目录结构，可以看到在以表名目录下，有以deptno=10（分区名）的子目录存放着真实的数据文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hadoop fs -ls -R /user/hive/warehouse/hive.db/emp_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 14:01 /user/hive/warehouse/hive.db/emp_partition/deptno=10</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        130 2021-12-28 14:01 /user/hive/warehouse/hive.db/emp_partition/deptno=10/000000_0</span><br></pre></td></tr></table></figure>

<p>同理，插入deptno为20，30的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hadoop fs -ls -R /user/hive/warehouse/hive.db/emp_partition</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 12:12 /user/hive/warehouse/hive.db/emp_partition/deptno=10</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        130 2021-12-28 14:01 /user/hive/warehouse/hive.db/emp_partition/deptno=10/000000_0</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         40 2021-12-28 12:12 /user/hive/warehouse/hive.db/emp_partition/deptno=10/emp_10.txt</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 14:01 /user/hive/warehouse/hive.db/emp_partition/deptno=20</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        214 2021-12-28 14:01 /user/hive/warehouse/hive.db/emp_partition/deptno=20/000000_0</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 12:11 /user/hive/warehouse/hive.db/emp_partition/deptno=30</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         40 2021-12-28 12:11 /user/hive/warehouse/hive.db/emp_partition/deptno=30/000000_0</span><br></pre></td></tr></table></figure>

<h3 id="查看分区"><a href="#查看分区" class="headerlink" title="查看分区"></a>查看分区</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; show partitions emp_partition;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">deptno=10</span><br><span class="line">deptno=20</span><br><span class="line">deptno=30</span><br><span class="line">Time taken: 0.152 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>

<h3 id="添加分区"><a href="#添加分区" class="headerlink" title="添加分区"></a>添加分区</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; alter table emp_partition  add partition (deptno=40);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.282 seconds</span><br><span class="line">hive (hive)&gt; show partitions emp_partition;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">deptno=10</span><br><span class="line">deptno=20</span><br><span class="line">deptno=30</span><br><span class="line">deptno=40</span><br><span class="line">Time taken: 0.127 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure>

<h3 id="删除分区-删除相应分区文件"><a href="#删除分区-删除相应分区文件" class="headerlink" title="删除分区(删除相应分区文件)"></a>删除分区(删除相应分区文件)</h3><p>注意，对于外表进行drop partition并不会删除hdfs上的文件，并且可以通过<code>msck repair table table_name</code>同步hdfs上的分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table emp_partition drop partition (deptno = 40);</span><br></pre></td></tr></table></figure>

<h3 id="修复分区"><a href="#修复分区" class="headerlink" title="修复分区"></a>修复分区</h3><p>修复分区就是重新同步hdfs上的分区信息。（外部表在hdfs目录上添加文件后使用）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck repair table table_name  [ADD/DROP/SYNC PARTITIONS];</span><br></pre></td></tr></table></figure>

<p>在hive3.0中msck命令支持删除partition信息。</p>
<h2 id="多级分区"><a href="#多级分区" class="headerlink" title="多级分区"></a>多级分区</h2><p>多分区表装载数据时，分区字段必须都要加。如果只有一个，会报错。</p>
<p>下面创建一张静态分区表par_tab_muilt，多个分区（性别+日期）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; create table par_tab_muilt (name string, nation string) partitioned by (sex string,dt string) row format delimited fields terminated by &#x27;,&#x27; ;</span><br><span class="line">hive (hive)&gt; load data local inpath &#x27;/home/hadoop/files/par_tab.txt&#x27; into table par_tab_muilt partition (sex=&#x27;man&#x27;,dt=&#x27;2021-12-28&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 files]$ hadoop fs -ls -R /user/hive/warehouse/par_tab_muilt</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 08:45 /user/hive/warehouse/par_tab_muilt/sex=man</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2021-12-28 08:45 /user/hive/warehouse/par_tab_muilt/sex=man/dt=2021-12-28</span><br><span class="line">-rwxr-xr-x   1 hadoop supergroup         71 2021-12-28 08:45 /user/hive/warehouse/par_tab_muilt/sex=man/dt=2021-12-28/par_tab.txt</span><br></pre></td></tr></table></figure>

<p>可见，新建表的时候定义的分区顺序，决定了文件目录顺序（谁是父目录谁是子目录），正因为有了这个层级关系，当我们查询所有man的时候，man以下的所有日期下的数据都会被查出来。如果只查询日期分区，但父目录sex=man和sex=woman都有该日期的数据，那么Hive会对输入路径进行修剪，从而只扫描日期分区，性别分区不作过滤（即查询结果包含了所有性别）。</p>
<h1 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h1><p>为什么要使用动态分区呢，我们举个例子，假如中国有50个省，每个省有50个市，每个市都有100个区，那我们都要使用静态分区要使用多久才能搞完。所有我们要使用动态分区。</p>
<blockquote>
<p>注意，动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区。</p>
<p>动态分区可以允许所有的分区列都是动态分区列，但是要首先设置一个参数hive.exec.dynamic.partition.mode</p>
</blockquote>
<p>动态分区默认是没有开启。开启后默认是以严格模式执行的，在这种模式下需要至少一个分区字段是静态的。这是为了防止用户有可能原意是只在子分区内进行动态建分区，但是由于疏忽忘记为主分区列指定值了，这将导致一个dml语句在短时间内创建大量的新的分区（对应大量新的文件夹），对系统性能带来影响。这有助于阻止因设计错误导致导致查询差生大量的分区。列如：用户可能错误使用时间戳作为分区表字段。然后导致每秒都对应一个分区！这样我们也可以采用相应的措施:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">关闭严格分区模式		set hive.exec.dynamic.partition.mode=nonstrict	//分区模式，默认strict（至少有一个分区列是静态分区）</span><br><span class="line">开启支持动态分区		set hive.exec.dynamic.partition=true			//开启动态分区,默认true</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">其他相关参数 ：</span><br><span class="line">set hive.exec.max.dynamic.partitions.pernode; #每一个执行mr节点上，允许创建的动态分区的最大数量(100) </span><br><span class="line">set hive.exec.max.dynamic.partitions;         #所有执行mr节点上，允许创建的所有动态分区的最大数量(1000) </span><br><span class="line">set hive.exec.max.created.files;              #所有的mr job允许创建的文件的最大数量(100000)</span><br></pre></td></tr></table></figure>

<p>利用动态分区，我们可以一次完成插入上面例子中deptno不同的数据的操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table emp_partition partition(deptno) select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp;</span><br></pre></td></tr></table></figure>



<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>hive的分区使用的表外字段，分区字段是一个伪列但是可以查询过滤。</li>
<li>分区字段不建议使用中文.</li>
<li>不太建议使用动态分区。因为动态分区将会使用mapreduce来查询数据，如果分区数量过多将导致namenode和yarn的资源瓶颈。所以建议动态分区前也尽可能之前预知分区数量。</li>
<li>分区属性的修改均可以使用手动元数据和hdfs的数据内容</li>
</ol>
<h2 id="外部分区表"><a href="#外部分区表" class="headerlink" title="外部分区表"></a>外部分区表</h2><p>外部表同样可以使用分区，事实上，用户会发现，只是管理大型生产数据集最常见的情况，这种结合给用户提供一个和其他工具共享数据的方式，同时也可以优化查询性能。这样我们就可以把数据路径改变而不影响数据的丢失，这是内部分区表远远不能做的事情:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1,(因为我们创建的是外部表)所有我们可以把表数据放到hdfs上的随便一个地方这里自动数据加载到/user/had/data/下(当然我们之前在外部表上指定了路径)</span><br><span class="line">load data local inpath &#x27;/home/had/data.txt&#x27; into table employees_ex partition (country=&quot;china&quot;,state=&quot;Asia&quot;);</span><br><span class="line">2,如果我们加载的数据要分离一些旧数据的时候就可以hadoop的distcp命令来copy数据到某个路径</span><br><span class="line">hadoop distcp /user/had/data/country=china/state=Asia /user/had/data_old/country=china/state=Asia</span><br><span class="line">3,修改表，把移走的数据的路径在hive里修改</span><br><span class="line">alter table employees partition(country=&quot;china&quot;,state=&quot;Asia&quot;) set location &#x27;/user/had/data_old/country=china/state=Asia&#x27;</span><br><span class="line">4,使用hdfs的rm命令删除之前路径的数据</span><br><span class="line">hdfs dfs -rmr /user/had/data/country=china/state=Asia</span><br><span class="line">这样我们就完成一次数据迁移</span><br><span class="line"></span><br><span class="line">如果觉得突然忘记了数据的位置使用使用下面的方式查看</span><br><span class="line">describe extend employees_ex partition (country=&quot;china&quot;,state=&quot;Asia&quot;);</span><br></pre></td></tr></table></figure>

<h2 id="众多的修改语句"><a href="#众多的修改语句" class="headerlink" title="众多的修改语句"></a>众多的修改语句</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1，把一个分区打包成一个har包</span><br><span class="line">  alter table employees archive partition (country=&quot;china&quot;,state=&quot;Asia&quot;)</span><br><span class="line">2, 把一个分区har包还原成原来的分区</span><br><span class="line">  alter table employees unarchive partition (country=&quot;china&quot;,state=&quot;Asia&quot;)</span><br><span class="line">3, 保护分区防止被删除</span><br><span class="line">   alter table employees partition (country=&quot;china&quot;,state=&quot;Asia&quot;) enable no_drop</span><br><span class="line">4,保护分区防止被查询</span><br><span class="line">    alter table employees partition (country=&quot;china&quot;,state=&quot;Asia&quot;) enable offline</span><br><span class="line">5，允许分区删除和查询</span><br><span class="line">   alter table employees partition (country=&quot;china&quot;,state=&quot;Asia&quot;) disable no_drop</span><br><span class="line">   alter table employees partition (country=&quot;china&quot;,state=&quot;Asia&quot;) disable offline</span><br></pre></td></tr></table></figure>



<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yongjian/archive/2017/03/29/6640951.html">https://www.cnblogs.com/yongjian/archive/2017/03/29/6640951.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41122339/article/details/81584110">https://blog.csdn.net/weixin_41122339/article/details/81584110</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lixinkuan328/article/details/102103237">https://blog.csdn.net/lixinkuan328/article/details/102103237</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://k12coding.github.io/2022/01/30/Hive%EF%BC%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%88%B0%E8%A1%A8%E7%9A%84%E6%96%B9%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="k12">
      <meta itemprop="description" content="2">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="k12的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/30/Hive%EF%BC%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%88%B0%E8%A1%A8%E7%9A%84%E6%96%B9%E5%BC%8F/" class="post-title-link" itemprop="url">Hive：加载数据到表的方式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-01-30 04:27:50 / 修改时间：05:30:03" itemprop="dateCreated datePublished" datetime="2022-01-30T04:27:50+08:00">2022-01-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="将文件加载到表中"><a href="#将文件加载到表中" class="headerlink" title="将文件加载到表中"></a>将文件加载到表中</h2><h3 id="加载本地文件到hive表"><a href="#加载本地文件到hive表" class="headerlink" title="加载本地文件到hive表"></a>加载本地文件到hive表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;linux_path&#x27; into table default.emp;</span><br></pre></td></tr></table></figure>

<h3 id="加载hdfs文件到hive中"><a href="#加载hdfs文件到hive中" class="headerlink" title="加载hdfs文件到hive中"></a>加载hdfs文件到hive中</h3><p>（overwrite 覆盖掉原有文件，<del>overwrite</del>在原文件中追加）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath &#x27;hdfs_path&#x27; overwrite into table default.emp;</span><br></pre></td></tr></table></figure>

<p>会将数据文件从原来的hdfs路径移动（mv）到建表时location指定目录</p>
<h3 id="创建表的时候通过location指定加载"><a href="#创建表的时候通过location指定加载" class="headerlink" title="创建表的时候通过location指定加载"></a>创建表的时候通过location指定加载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create EXTERNAL table IF NOT EXISTS default.emp_ext(</span><br><span class="line">empno int,</span><br><span class="line">ename string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t‘</span><br><span class="line">location ‘/user/hive/warehouse/emp_ext‘;</span><br></pre></td></tr></table></figure>

<p>适用于建（外部）表时，数据文件已经存在的情况</p>
<h2 id="通过查询将数据插入到-Hive-表中"><a href="#通过查询将数据插入到-Hive-表中" class="headerlink" title="通过查询将数据插入到 Hive 表中"></a>通过查询将数据插入到 Hive 表中</h2><h3 id="创建表时通过insert加载"><a href="#创建表时通过insert加载" class="headerlink" title="创建表时通过insert加载"></a>创建表时通过insert加载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Standard syntax:</span><br><span class="line">INSERT OVERWRITE [LOCAL] DIRECTORY directory1</span><br><span class="line">  [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0)</span><br><span class="line">  SELECT ... FROM ...</span><br><span class="line">  </span><br><span class="line">Hive extension (multiple inserts):</span><br><span class="line">FROM from_statement</span><br><span class="line">INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1</span><br><span class="line">[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table default.emp_ci like emp;</span><br><span class="line">insert overwrite table default.emp_ci select * from default.emp;</span><br></pre></td></tr></table></figure>

<p><strong>from table 多重插入数据方式multiple inserts</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from test1</span><br><span class="line">insert overwrite table test2 partition (age) select name,address,school,age</span><br><span class="line">insert overwrite table test3 select name,address</span><br></pre></td></tr></table></figure>

<p>Hive支持多表插入，可以在同一个查询中使用多个insett子句，这样的好处是我们只需要扫描一遍源表就可以生成多个不相交的输出！这是一个优化，可以减少表的扫描，从而减少 JOB 中 MR的 STAGE 数量，达到优化的目的。</p>
<h4 id="CREATE-TABLE-LIKE-语句"><a href="#CREATE-TABLE-LIKE-语句" class="headerlink" title="CREATE TABLE LIKE 语句"></a>CREATE TABLE LIKE 语句</h4><ul>
<li>用来复制表的结构</li>
<li>需要外部表的话，通过create external table as …指定</li>
<li>不CTAS语句会填充数据</li>
</ul>
<p>创建表并加载数据（as select）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table default.emp_ci as select * from emp;</span><br></pre></td></tr></table></figure>

<h4 id="CTAS建表语句（CREATE-TABLE-AS-SELECT）"><a href="#CTAS建表语句（CREATE-TABLE-AS-SELECT）" class="headerlink" title="CTAS建表语句（CREATE TABLE AS SELECT）"></a>CTAS建表语句（CREATE TABLE AS SELECT）</h4><ul>
<li>使用查询创建并填充表，select中选取的列名会作为新表的列名（所以通常是要取别名）</li>
<li>会改变表的属性、结构，比如只能是内部表、分区分桶也没了</li>
<li>目标表不允许使用分区分桶的，<code>FAILED: SemanticException [Error 10068]: CREATE-TABLE-AS-SELECT does not support partitioning in the target table</code></li>
<li>对于旧表中的分区字段，如果通过select * 的方式，新表会把它看作一个新的字段，这里要注意</li>
<li>目标表不允许使用外部表，如create external table … as select…报错 <code>FAILED: SemanticException [Error 10070]: CREATE-TABLE-AS-SELECT cannot create external table</code></li>
<li>CTAS创建的表存储格式会变成默认的格式TEXTFILE</li>
<li>对了，还有字段的注释comment也会丢掉，同时新表也无法加上注释</li>
<li>但可以在CTAS语句中指定表的存储格式，行和列的分隔符等</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table xxx as select ...</span><br><span class="line"></span><br><span class="line">create table xxx</span><br><span class="line">  row format delimited</span><br><span class="line">  fields terminated by &#x27; &#x27;</span><br><span class="line">  stored as parquet</span><br><span class="line">as</span><br><span class="line">select ...</span><br></pre></td></tr></table></figure>



<h2 id="从-SQL-向表中插入值"><a href="#从-SQL-向表中插入值" class="headerlink" title="从 SQL 向表中插入值"></a>从 SQL 向表中插入值</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Standard Syntax:</span><br><span class="line">INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]</span><br></pre></td></tr></table></figure>

<p>通过insert向Hive表中插入数据可以单条插入和多条插入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into emp values(1,&#x27;xiaoming&#x27;); #单条插入</span><br><span class="line">insert into emp values(2,&#x27;xiaohong&#x27;),(3,&#x27;xiaofang&#x27;); #多条插入</span><br></pre></td></tr></table></figure>



<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/LanguageManual_DML.html#LanguageManualDML-Loadingfilesintotables">https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/LanguageManual_DML.html#LanguageManualDML-Loadingfilesintotables</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lzw2016/article/details/97811799">https://blog.csdn.net/lzw2016/article/details/97811799</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k12</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





</body>
</html>
